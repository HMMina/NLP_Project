{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "282c17f5",
   "metadata": {},
   "source": [
    "# lab6_rnn_for_ner\n",
    "- Task 1: Tải và Tiền xử lý Dữ liệu\n",
    "- Task 2: Tạo PyTorch Dataset và DataLoader\n",
    "- Task 3: Xây dựng Mô hình RNN\n",
    "- Task 4: Huấn luyện Mô hình\n",
    "- Task 5: Đánh giá Mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d438ebf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\.vscode\\face-attendace-system\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from datasets import load_dataset\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Thiết lập seed để đảm bảo tính tái lập\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08872e4",
   "metadata": {},
   "source": [
    "**Task 1: Tải và Tiền xử lý Dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc24809c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\.vscode\\face-attendace-system\\.venv\\lib\\site-packages\\datasets\\load.py:1486: FutureWarning: The repository for conll2003 contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/conll2003\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 14041\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3250\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n",
      "        num_rows: 3453\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# 1. Tải dữ liệu từ Hugging Face\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "386b42ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label names: ['O', 'B-PER', 'I-PER', 'B-ORG', 'I-ORG', 'B-LOC', 'I-LOC', 'B-MISC', 'I-MISC']\n",
      "Train size: 14041\n",
      "Validation size: 3250\n",
      "Test size: 3453\n",
      "Example sentence: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Example tags: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n",
      "Train size: 14041\n",
      "Validation size: 3250\n",
      "Test size: 3453\n",
      "Example sentence: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.']\n",
      "Example tags: ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "# 2. Trích xuất câu và nhãn\n",
    "\n",
    "# Lấy danh sách tên nhãn từ features\n",
    "label_names = dataset[\"train\"].features[\"ner_tags\"].feature.names\n",
    "print(f\"Label names: {label_names}\")\n",
    "\n",
    "def extract_data(split_name):\n",
    "    tokens_list = dataset[split_name][\"tokens\"]\n",
    "    tags_list_int = dataset[split_name][\"ner_tags\"]\n",
    "    \n",
    "    # Chuyển đổi nhãn số sang string\n",
    "    tags_list_str = [[label_names[i] for i in tags] for tags in tags_list_int]\n",
    "    return tokens_list, tags_list_str\n",
    "\n",
    "train_sentences, train_tags = extract_data(\"train\")\n",
    "val_sentences, val_tags = extract_data(\"validation\")\n",
    "test_sentences, test_tags = extract_data(\"test\")\n",
    "\n",
    "print(f\"Train size: {len(train_sentences)}\")\n",
    "print(f\"Validation size: {len(val_sentences)}\")\n",
    "print(f\"Test size: {len(test_sentences)}\")\n",
    "print(f\"Example sentence: {train_sentences[0]}\")\n",
    "print(f\"Example tags: {train_tags[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f739a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word vocab size: 23625\n",
      "Tag vocab size: 10\n",
      "Tag mapping: {'<PAD>': 0, 'B-LOC': 1, 'B-MISC': 2, 'B-ORG': 3, 'B-PER': 4, 'I-LOC': 5, 'I-MISC': 6, 'I-ORG': 7, 'I-PER': 8, 'O': 9}\n"
     ]
    }
   ],
   "source": [
    "# 3. Xây dựng Từ điển (Vocabulary)\n",
    "def build_vocab(sentences, tags_list, min_freq=1):\n",
    "    word_counter = Counter()\n",
    "    tag_set = set()\n",
    "    \n",
    "    # Đếm số lần xuất hiện của từ\n",
    "    for sent in sentences:\n",
    "        word_counter.update(sent)\n",
    "    \n",
    "    # Thu thập tập hợp nhãn\n",
    "    for tags in tags_list:\n",
    "        tag_set.update(tags)\n",
    "        \n",
    "    # Word vocabulary\n",
    "    word_to_ix = {\"<PAD>\": 0, \"<UNK>\": 1}\n",
    "    for word, count in word_counter.items():\n",
    "        if count >= min_freq:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "    # Tag vocabulary\n",
    "    # Đảm bảo <PAD> tag có index riêng nếu cần, hoặc dùng index đặc biệt trong loss\n",
    "    # Ở đây ta thêm <PAD> vào tag_to_ix để tiện cho việc padding label\n",
    "    tag_to_ix = {\"<PAD>\": 0}\n",
    "    for tag in sorted(list(tag_set)):\n",
    "        tag_to_ix[tag] = len(tag_to_ix)\n",
    "        \n",
    "    return word_to_ix, tag_to_ix\n",
    "\n",
    "word_to_ix, tag_to_ix = build_vocab(train_sentences, train_tags)\n",
    "print(f\"Word vocab size: {len(word_to_ix)}\")\n",
    "print(f\"Tag vocab size: {len(tag_to_ix)}\")\n",
    "print(f\"Tag mapping: {tag_to_ix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40584e1a",
   "metadata": {},
   "source": [
    "**Task 2: Tạo PyTorch Dataset và DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0576e507",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERDataset(Dataset):\n",
    "    def __init__(self, sentences, tags, word_to_ix, tag_to_ix):\n",
    "        self.sentences = sentences\n",
    "        self.tags = tags\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sentence = self.sentences[idx]\n",
    "        tag_seq = self.tags[idx]\n",
    "        \n",
    "        # Convert to indices\n",
    "        word_indices = [self.word_to_ix.get(w, self.word_to_ix[\"<UNK>\"]) for w in sentence]\n",
    "        tag_indices = [self.tag_to_ix[t] for t in tag_seq]\n",
    "        \n",
    "        return torch.tensor(word_indices, dtype=torch.long), torch.tensor(tag_indices, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch là list các tuple (sentence_indices, tag_indices)\n",
    "    sentences, tags = zip(*batch)\n",
    "    \n",
    "    # Lấy độ dài thực tế của mỗi câu để dùng cho pack_padded_sequence\n",
    "    lengths = torch.tensor([len(s) for s in sentences])\n",
    "    \n",
    "    # Pad sentences và tags\n",
    "    # padding_value cho sentence là index của <PAD> (0)\n",
    "    padded_sentences = pad_sequence(sentences, batch_first=True, padding_value=word_to_ix[\"<PAD>\"])\n",
    "    \n",
    "    # padding_value cho tags là index của <PAD> tag (0)\n",
    "    # Lưu ý: Khi tính loss cần ignore index này\n",
    "    padded_tags = pad_sequence(tags, batch_first=True, padding_value=tag_to_ix[\"<PAD>\"])\n",
    "    \n",
    "    return padded_sentences, padded_tags, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dac84f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train_loader: 110\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = NERDataset(train_sentences, train_tags, word_to_ix, tag_to_ix)\n",
    "val_dataset = NERDataset(val_sentences, val_tags, word_to_ix, tag_to_ix)\n",
    "test_dataset = NERDataset(test_sentences, test_tags, word_to_ix, tag_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Number of batches in train_loader: {len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab279772",
   "metadata": {},
   "source": [
    "**Task 3: Xây dựng Mô hình RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f122b782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNNForNER(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_size, pad_idx):\n",
    "        super(SimpleRNNForNER, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        # Sử dụng LSTM để có kết quả tốt hơn RNN thường\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        # Vì dùng bidirectional nên input của linear là hidden_dim * 2\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        \n",
    "    def forward(self, x, lengths):\n",
    "        # x: [batch_size, seq_len]\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = self.dropout(embedded)\n",
    "        \n",
    "        # Pack sequence\n",
    "        # lengths cần phải ở trên CPU\n",
    "        packed_embedded = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        # Unpack sequence\n",
    "        output, output_lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "        \n",
    "        # output: [batch_size, seq_len, hidden_dim * 2]\n",
    "        predictions = self.fc(self.dropout(output))\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755c507c",
   "metadata": {},
   "source": [
    "**Task 4: Huấn luyện Mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e87c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleRNNForNER(\n",
      "  (embedding): Embedding(23625, 100, padding_idx=0)\n",
      "  (rnn): LSTM(100, 256, batch_first=True, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Khởi tạo mô hình\n",
    "VOCAB_SIZE = len(word_to_ix)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_SIZE = len(tag_to_ix)\n",
    "PAD_IDX = word_to_ix[\"<PAD>\"]\n",
    "\n",
    "model = SimpleRNNForNER(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_SIZE, PAD_IDX)\n",
    "model = model.to(device)\n",
    "\n",
    "# Optimizer và Loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "# ignore_index là index của <PAD> tag để không tính loss cho phần padding\n",
    "TAG_PAD_IDX = tag_to_ix[\"<PAD>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=TAG_PAD_IDX)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3da1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.819 | Train Accuracy: 82.22%\n",
      "Epoch: 02 | Train Loss: 0.521 | Train Accuracy: 86.07%\n",
      "Epoch: 02 | Train Loss: 0.521 | Train Accuracy: 86.07%\n",
      "Epoch: 03 | Train Loss: 0.399 | Train Accuracy: 88.46%\n",
      "Epoch: 03 | Train Loss: 0.399 | Train Accuracy: 88.46%\n",
      "Epoch: 04 | Train Loss: 0.316 | Train Accuracy: 90.67%\n",
      "Epoch: 04 | Train Loss: 0.316 | Train Accuracy: 90.67%\n",
      "Epoch: 05 | Train Loss: 0.260 | Train Accuracy: 92.21%\n",
      "Epoch: 05 | Train Loss: 0.260 | Train Accuracy: 92.21%\n",
      "Epoch: 06 | Train Loss: 0.219 | Train Accuracy: 93.36%\n",
      "Epoch: 06 | Train Loss: 0.219 | Train Accuracy: 93.36%\n",
      "Epoch: 07 | Train Loss: 0.186 | Train Accuracy: 94.39%\n",
      "Epoch: 07 | Train Loss: 0.186 | Train Accuracy: 94.39%\n",
      "Epoch: 08 | Train Loss: 0.161 | Train Accuracy: 95.04%\n",
      "Epoch: 08 | Train Loss: 0.161 | Train Accuracy: 95.04%\n",
      "Epoch: 09 | Train Loss: 0.140 | Train Accuracy: 95.68%\n",
      "Epoch: 09 | Train Loss: 0.140 | Train Accuracy: 95.68%\n",
      "Epoch: 10 | Train Loss: 0.121 | Train Accuracy: 96.21%\n",
      "Epoch: 10 | Train Loss: 0.121 | Train Accuracy: 96.21%\n"
     ]
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    for batch in iterator:\n",
    "        text, tags, lengths = batch\n",
    "        text = text.to(device)\n",
    "        tags = tags.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(text, lengths)\n",
    "        # predictions: [batch_size, seq_len, output_size]\n",
    "        # tags: [batch_size, seq_len]\n",
    "        \n",
    "        # Reshape để tính loss\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        \n",
    "        loss = criterion(predictions, tags)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        # Tính accuracy\n",
    "        predicted_tags = torch.argmax(predictions, dim=-1)\n",
    "        mask = tags != TAG_PAD_IDX\n",
    "        correct = (predicted_tags == tags) & mask\n",
    "        correct_preds += correct.sum().item()\n",
    "        total_preds += mask.sum().item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), correct_preds / total_preds\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Accuracy: {train_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bc3b00",
   "metadata": {},
   "source": [
    "**Task 5: Đánh giá Mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f6e7667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_preds = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            text, tags, lengths = batch\n",
    "            text = text.to(device)\n",
    "            tags = tags.to(device)\n",
    "            \n",
    "            predictions = model(text, lengths)\n",
    "            \n",
    "            # Tính loss\n",
    "            predictions_flat = predictions.view(-1, predictions.shape[-1])\n",
    "            tags_flat = tags.view(-1)\n",
    "            loss = criterion(predictions_flat, tags_flat)\n",
    "            epoch_loss += loss.item()\n",
    "            \n",
    "            # Tính accuracy\n",
    "            # Lấy nhãn dự đoán có xác suất cao nhất\n",
    "            predicted_tags = torch.argmax(predictions, dim=-1)\n",
    "            \n",
    "            # Mask để bỏ qua padding\n",
    "            mask = tags != TAG_PAD_IDX\n",
    "            \n",
    "            correct = (predicted_tags == tags) & mask\n",
    "            correct_preds += correct.sum().item()\n",
    "            total_preds += mask.sum().item()\n",
    "            \n",
    "    return epoch_loss / len(iterator), correct_preds / total_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8381695f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.242 | Validation Accuracy: 94.45%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "print(f'Validation Loss: {val_loss:.3f} | Validation Accuracy: {val_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d81234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.349 | Test Accuracy: 92.60%\n"
     ]
    }
   ],
   "source": [
    "# Test trên tập test\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Accuracy: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c0256d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hàm dự đoán cho câu mới\n",
    "def predict_sentence(model, sentence, word_to_ix, tag_to_ix):\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize đơn giản (tách theo khoảng trắng)\n",
    "    tokens = sentence.split()\n",
    "    \n",
    "    # Chuyển tokens sang indices\n",
    "    token_indices = [word_to_ix.get(w, word_to_ix[\"<UNK>\"]) for w in tokens]\n",
    "    \n",
    "    # Chuyển sang tensor và thêm dimension batch (1, seq_len)\n",
    "    token_tensor = torch.tensor([token_indices], dtype=torch.long).to(device)\n",
    "    lengths = torch.tensor([len(token_indices)], dtype=torch.long)\n",
    "    \n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        predictions = model(token_tensor, lengths)\n",
    "        # predictions: [1, seq_len, output_size]\n",
    "        \n",
    "        # Lấy nhãn có xác suất cao nhất\n",
    "        predicted_indices = torch.argmax(predictions, dim=-1).squeeze(0).tolist()\n",
    "        \n",
    "    # Map indices ngược lại thành tags\n",
    "    ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "    predicted_tags = [ix_to_tag[idx] for idx in predicted_indices]\n",
    "    \n",
    "    return list(zip(tokens, predicted_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d16f0aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: VNU University is located in Hanoi\n",
      "Predictions:\n",
      "VNU: B-ORG\n",
      "University: I-ORG\n",
      "is: O\n",
      "located: O\n",
      "in: O\n",
      "Hanoi: O\n"
     ]
    }
   ],
   "source": [
    "# Test với câu ví dụ\n",
    "sentence = \"VNU University is located in Hanoi\"\n",
    "predictions = predict_sentence(model, sentence, word_to_ix, tag_to_ix)\n",
    "\n",
    "print(\"Sentence:\", sentence)\n",
    "print(\"Predictions:\")\n",
    "for word, tag in predictions:\n",
    "    print(f\"{word}: {tag}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
