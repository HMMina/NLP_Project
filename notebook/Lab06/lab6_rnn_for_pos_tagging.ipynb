{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "377f2da5",
   "metadata": {},
   "source": [
    "# lab6_rnn_for_pos_tagging\n",
    "- Task 1: Tải và Tiền xử lý Dữ liệu\n",
    "- Task 2: Tạo PyTorch Dataset và DataLoader\n",
    "- Task 3: Xây dựng Mô hình RNN\n",
    "- Task 4: Huấn luyện Mô hình\n",
    "- Task 5: Đánh giá Mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8a34df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "from typing import List, Tuple, Dict\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# Chọn device, nếu có GPU thì dùng GPU, không thì dùng CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4fb7b3",
   "metadata": {},
   "source": [
    "**Task 1: Tải và Tiền xử lý Dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0927f697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sentences: 12544 | Dev sentences: 2001\n",
      "Ví dụ câu (train)[0]: [('Al', 'PROPN'), ('-', 'PUNCT'), ('Zaman', 'PROPN'), (':', 'PUNCT'), ('American', 'ADJ'), ('forces', 'NOUN'), ('killed', 'VERB'), ('Shaikh', 'PROPN'), ('Abdullah', 'PROPN'), ('al', 'PROPN')]\n"
     ]
    }
   ],
   "source": [
    "# Task 1.1: Hàm đọc file .conllu -> danh sách các câu [(word, upos), ...]\n",
    "def load_conllu(file_path: str) -> List[List[Tuple[str, str]]]:\n",
    "    sentences = []\n",
    "    current = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            # Loại bỏ khoảng trắng thừa \n",
    "            line = line.strip()\n",
    "\n",
    "            # Nếu gặp dòng trống, kết thúc câu hiện tại\n",
    "            if not line:\n",
    "                if current:\n",
    "                    sentences.append(current)\n",
    "                    current = []\n",
    "                continue\n",
    "\n",
    "            # Bỏ qua các dòng chú thích\n",
    "            if line.startswith('#'):\n",
    "                continue\n",
    "\n",
    "            # Phân tách cột theo tab\n",
    "            cols = line.split('\\t')\n",
    "\n",
    "            # Bỏ qua các dòng không đủ cột\n",
    "            if len(cols) < 4:\n",
    "                continue\n",
    "\n",
    "            # Lấy ID\n",
    "            tok_id = cols[0]\n",
    "\n",
    "            # Bỏ qua multi-word tokens (vd: 3-4) hoặc empty nodes (3.1)\n",
    "            if '-' in tok_id or '.' in tok_id:\n",
    "                continue\n",
    "\n",
    "            # Lấy FORM và UPOS\n",
    "            form = cols[1]\n",
    "            upos = cols[3]\n",
    "\n",
    "            # Thêm (form, upos) vào câu hiện tại\n",
    "            current.append((form, upos))\n",
    "    \n",
    "    # Thêm câu cuối cùng nếu có\n",
    "    if current:\n",
    "        sentences.append(current)\n",
    "    return sentences\n",
    "\n",
    "# Đường dẫn dữ liệu UD English-EWT trong workspace\n",
    "DATA_DIR = os.path.join('..', 'UD_English-EWT') if os.path.isdir('UD_English-EWT') == False else 'UD_English-EWT'\n",
    "train_path = os.path.join(DATA_DIR, 'en_ewt-ud-train.conllu')\n",
    "dev_path   = os.path.join(DATA_DIR, 'en_ewt-ud-dev.conllu')\n",
    "\n",
    "train_sents = load_conllu(train_path)\n",
    "dev_sents   = load_conllu(dev_path)\n",
    "\n",
    "print(f\"Train sentences: {len(train_sents)} | Dev sentences: {len(dev_sents)}\")\n",
    "print('Ví dụ câu (train)[0]:', train_sents[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ced17460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kích thước word_to_ix: 6733\n",
      "Kích thước tag_to_ix : 18\n",
      "Ví dụ 10 từ đầu: [('<PAD>', 0), ('<UNK>', 1), ('Al', 2), ('-', 3), ('Zaman', 4), (':', 5), ('American', 6), ('forces', 7), ('killed', 8), ('Shaikh', 9)]\n",
      "Ví dụ 10 tag đầu: [('<PAD>', 0), ('ADJ', 1), ('ADP', 2), ('ADV', 3), ('AUX', 4), ('CCONJ', 5), ('DET', 6), ('INTJ', 7), ('NOUN', 8), ('NUM', 9)]\n"
     ]
    }
   ],
   "source": [
    "# Task 1.2: Xây dựng vocabulary từ train\n",
    "PAD_TOKEN = '<PAD>'\n",
    "UNK_TOKEN = '<UNK>'\n",
    "PAD_TAG   = '<PAD>'\n",
    "\n",
    "def build_vocabs(sentences: List[List[Tuple[str,str]]], min_freq=3):\n",
    "    word_counter = Counter()\n",
    "    tag_set = set()\n",
    "\n",
    "    # Đếm tần suất từ\n",
    "    for sent in sentences:\n",
    "        for w, t in sent:\n",
    "            word_counter[w] += 1\n",
    "            tag_set.add(t)\n",
    "\n",
    "    # Vocab từ\n",
    "    word_to_ix = {PAD_TOKEN: 0, UNK_TOKEN: 1}\n",
    "    for w, freq in word_counter.items():\n",
    "        if freq >= min_freq:        # giữ từ có tần suất >= 3\n",
    "            word_to_ix[w] = len(word_to_ix)\n",
    "\n",
    "    # Vocab tag\n",
    "    tag_to_ix = {PAD_TAG: 0}\n",
    "    for t in sorted(tag_set):\n",
    "        tag_to_ix[t] = len(tag_to_ix)\n",
    "\n",
    "    return word_to_ix, tag_to_ix\n",
    "\n",
    "\n",
    "word_to_ix, tag_to_ix = build_vocabs(train_sents)\n",
    "\n",
    "# Inverse mapping từ ix -> tag\n",
    "ix_to_tag = {v: k for k, v in tag_to_ix.items()}\n",
    "\n",
    "print('Kích thước word_to_ix:', len(word_to_ix))\n",
    "print('Kích thước tag_to_ix :', len(tag_to_ix))\n",
    "print('Ví dụ 10 từ đầu:', list(word_to_ix.items())[:10])\n",
    "print('Ví dụ 10 tag đầu:', list(tag_to_ix.items())[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258a6f8",
   "metadata": {},
   "source": [
    "**Task 2: Tạo PyTorch Dataset và DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53e46278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1: POSDataset và collate_fn\n",
    "class POSDataset(Dataset):\n",
    "    def __init__(self, sentences: List[List[Tuple[str,str]]], word_to_ix: Dict[str,int], tag_to_ix: Dict[str,int]):\n",
    "        self.sentences = sentences\n",
    "        self.word_to_ix = word_to_ix\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "\n",
    "    # Trả về số lượng câu trong dataset\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "    \n",
    "    # Trả về tensor word_ids và tag_ids cho câu thứ idx\n",
    "    def __getitem__(self, idx):\n",
    "        sent = self.sentences[idx]\n",
    "\n",
    "        # Lấy words và tags\n",
    "        words = [w for w, _ in sent]\n",
    "        tags  = [t for _, t in sent]\n",
    "\n",
    "        # Chuyển words và tags thành IDs\n",
    "        word_ids = [self.word_to_ix.get(w, self.word_to_ix[UNK_TOKEN]) for w in words]\n",
    "        tag_ids  = [self.tag_to_ix[t] for t in tags]\n",
    "        return torch.tensor(word_ids, dtype=torch.long), torch.tensor(tag_ids, dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: list of (word_ids_tensor, tag_ids_tensor)\n",
    "    word_seqs, tag_seqs = zip(*batch)\n",
    "\n",
    "    # Tính độ dài mỗi câu\n",
    "    lengths = torch.tensor([len(x) for x in word_seqs], dtype=torch.long)\n",
    "\n",
    "    # Pad sequences về cùng độ dài\n",
    "    padded_words = pad_sequence(word_seqs, batch_first=True, padding_value=word_to_ix[PAD_TOKEN])\n",
    "\n",
    "    # Pad tag sequences về cùng độ dài\n",
    "    padded_tags  = pad_sequence(tag_seqs,  batch_first=True, padding_value=tag_to_ix[PAD_TAG])\n",
    "    return padded_words, padded_tags, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20d453c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12544, 2001)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 2.2: Tạo DataLoader cho train và dev\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Tạo dataset và dataloader\n",
    "train_ds = POSDataset(train_sents, word_to_ix, tag_to_ix)\n",
    "dev_ds   = POSDataset(dev_sents,   word_to_ix, tag_to_ix)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "dev_loader   = DataLoader(dev_ds,   batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "len(train_ds), len(dev_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb13142e",
   "metadata": {},
   "source": [
    "**Task 3: Xây dựng Mô hình RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e1870c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3: Mô hình RNN đơn giản cho token classification\n",
    "class SimpleRNNForTokenClassification(nn.Module):\n",
    "    def __init__(self, vocab_size: int, tag_size: int, emb_dim: int = 100, hidden_dim: int = 128, num_layers: int = 1, bidirectional: bool = False, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Các lớp của mô hình\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim, padding_idx=word_to_ix[PAD_TOKEN])\n",
    "        self.rnn = nn.RNN(input_size=emb_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
    "        out_dim = hidden_dim * (2 if bidirectional else 1)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(out_dim, tag_size)\n",
    "    \n",
    "    # Định nghĩa hàm forward\n",
    "    def forward(self, input_ids: torch.Tensor, lengths: torch.Tensor):\n",
    "        # input_ids: (B, T)\n",
    "        emb = self.embedding(input_ids)  # (B, T, E)\n",
    "        # Sử dụng pack để RNN bỏ qua padding\n",
    "        packed = pack_padded_sequence(emb, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        packed_out, _ = self.rnn(packed)\n",
    "        out, _ = pad_packed_sequence(packed_out, batch_first=True)  # (B, T, H) B là batch size, T là độ dài câu, H là hidden size\n",
    "        out = self.dropout(out)\n",
    "        logits = self.classifier(out)  # (B, T, C) B là batch size, T là độ dài câu, C là số tags\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09639547",
   "metadata": {},
   "source": [
    "**Task 4: Huấn luyện Mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51327ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleRNNForTokenClassification(\n",
       "  (embedding): Embedding(6733, 100, padding_idx=0)\n",
       "  (rnn): RNN(100, 128, batch_first=True, bidirectional=True)\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=256, out_features=18, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Task 4: Khởi tạo mô hình, optimizer, loss\n",
    "\n",
    "# Thiết lập siêu tham số\n",
    "vocab_size = len(word_to_ix)\n",
    "tag_size   = len(tag_to_ix)\n",
    "emb_dim    = 100 # kích thước embedding\n",
    "hidden_dim = 128 # kích thước hidden state\n",
    "bidirectional = True # sử dụng RNN hai chiều\n",
    "\n",
    "model = SimpleRNNForTokenClassification(vocab_size, tag_size, emb_dim=emb_dim, hidden_dim=hidden_dim, bidirectional=bidirectional).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tag_to_ix[PAD_TAG])\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ef21df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4: Hàm train và evaluate\n",
    "\n",
    "# Hàm tính accuracy với mask\n",
    "def masked_accuracy(logits: torch.Tensor, gold: torch.Tensor, pad_idx: int) -> float:\n",
    "    # logits: (B,T,C), gold: (B,T) \n",
    "    preds = logits.argmax(dim=-1)  # (B,T) lấy nhãn dự đoán\n",
    "    mask = gold.ne(pad_idx)  # tạo mask cho các vị trí không phải padding\n",
    "    correct = (preds.eq(gold) & mask).sum().item() # số dự đoán đúng có mask\n",
    "    total = mask.sum().item()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    return correct / total\n",
    "\n",
    "# Hàm train một epoch\n",
    "def train_one_epoch(model, loader, optimizer, criterion, pad_idx):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    # Vòng lặp qua từng batch\n",
    "    for batch in loader:\n",
    "        # Lấy dữ liệu từ batch\n",
    "        words, tags, lengths = batch\n",
    "        words = words.to(device)\n",
    "        tags  = tags.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "\n",
    "        # Forward + Backward + Optimize\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(words, lengths)\n",
    "        # Flatten để tính CE: (B*T, C) vs (B*T)\n",
    "        loss = criterion(logits.view(-1, logits.size(-1)), tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # stats\n",
    "        n_tokens = tags.ne(pad_idx).sum().item()\n",
    "        total_tokens += n_tokens\n",
    "        total_loss += loss.item() * tags.numel()  # scale theo số phần tử\n",
    "    avg_loss = total_loss / (len(loader.dataset) if len(loader.dataset) > 0 else 1)\n",
    "    return avg_loss\n",
    "\n",
    "# Hàm evaluate mô hình\n",
    "def evaluate(model, loader, criterion, pad_idx):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    total_correct = 0\n",
    "\n",
    "    # Đánh giá không cần tính gradient\n",
    "    with torch.no_grad():\n",
    "\n",
    "        # Vòng lặp qua từng batch\n",
    "        for batch in loader:\n",
    "            # Lấy dữ liệu từ batch\n",
    "            words, tags, lengths = batch\n",
    "            words = words.to(device)\n",
    "            tags  = tags.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(words, lengths)\n",
    "            loss = criterion(logits.view(-1, logits.size(-1)), tags.view(-1))\n",
    "            total_loss += loss.item() * tags.numel()\n",
    "            # accuracy\n",
    "            acc = masked_accuracy(logits, tags, pad_idx)\n",
    "            mask = tags.ne(pad_idx)\n",
    "            total_correct += (logits.argmax(-1).eq(tags) & mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "    avg_loss = total_loss / (len(loader.dataset) if len(loader.dataset) > 0 else 1)\n",
    "    avg_acc = (total_correct / total_tokens) if total_tokens > 0 else 0.0\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4256868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=67.9458 | dev_loss=33.3044 | dev_acc=0.7603\n",
      "Epoch 02 | train_loss=36.7162 | dev_loss=25.0620 | dev_acc=0.8155\n",
      "Epoch 02 | train_loss=36.7162 | dev_loss=25.0620 | dev_acc=0.8155\n",
      "Epoch 03 | train_loss=27.3685 | dev_loss=20.9647 | dev_acc=0.8466\n",
      "Epoch 03 | train_loss=27.3685 | dev_loss=20.9647 | dev_acc=0.8466\n",
      "Epoch 04 | train_loss=21.4835 | dev_loss=18.4810 | dev_acc=0.8632\n",
      "Epoch 04 | train_loss=21.4835 | dev_loss=18.4810 | dev_acc=0.8632\n",
      "Epoch 05 | train_loss=17.7275 | dev_loss=16.6868 | dev_acc=0.8767\n",
      "Epoch 05 | train_loss=17.7275 | dev_loss=16.6868 | dev_acc=0.8767\n",
      "Epoch 06 | train_loss=15.0122 | dev_loss=15.5063 | dev_acc=0.8876\n",
      "Epoch 06 | train_loss=15.0122 | dev_loss=15.5063 | dev_acc=0.8876\n",
      "Epoch 07 | train_loss=12.7403 | dev_loss=14.9340 | dev_acc=0.8937\n",
      "Epoch 07 | train_loss=12.7403 | dev_loss=14.9340 | dev_acc=0.8937\n",
      "Epoch 08 | train_loss=11.3818 | dev_loss=14.6995 | dev_acc=0.8957\n",
      "Epoch 08 | train_loss=11.3818 | dev_loss=14.6995 | dev_acc=0.8957\n",
      "Epoch 09 | train_loss=10.1980 | dev_loss=14.7442 | dev_acc=0.8952\n",
      "Epoch 09 | train_loss=10.1980 | dev_loss=14.7442 | dev_acc=0.8952\n",
      "Epoch 10 | train_loss=9.1291 | dev_loss=15.0488 | dev_acc=0.8945\n",
      "Epoch 10 | train_loss=9.1291 | dev_loss=15.0488 | dev_acc=0.8945\n",
      "Epoch 11 | train_loss=8.2330 | dev_loss=14.5638 | dev_acc=0.8999\n",
      "Epoch 11 | train_loss=8.2330 | dev_loss=14.5638 | dev_acc=0.8999\n",
      "Epoch 12 | train_loss=7.4461 | dev_loss=14.9955 | dev_acc=0.8995\n",
      "Epoch 12 | train_loss=7.4461 | dev_loss=14.9955 | dev_acc=0.8995\n",
      "Epoch 13 | train_loss=6.8169 | dev_loss=14.9773 | dev_acc=0.9035\n",
      "Epoch 13 | train_loss=6.8169 | dev_loss=14.9773 | dev_acc=0.9035\n",
      "Epoch 14 | train_loss=6.2771 | dev_loss=15.7560 | dev_acc=0.8972\n",
      "Epoch 14 | train_loss=6.2771 | dev_loss=15.7560 | dev_acc=0.8972\n",
      "Epoch 15 | train_loss=5.5244 | dev_loss=16.7355 | dev_acc=0.8951\n",
      "Epoch 15 | train_loss=5.5244 | dev_loss=16.7355 | dev_acc=0.8951\n",
      "Epoch 16 | train_loss=5.0551 | dev_loss=16.2119 | dev_acc=0.9021\n",
      "Epoch 16 | train_loss=5.0551 | dev_loss=16.2119 | dev_acc=0.9021\n",
      "Epoch 17 | train_loss=4.5238 | dev_loss=16.8760 | dev_acc=0.9006\n",
      "Epoch 17 | train_loss=4.5238 | dev_loss=16.8760 | dev_acc=0.9006\n",
      "Epoch 18 | train_loss=3.9932 | dev_loss=17.4442 | dev_acc=0.9014\n",
      "Epoch 18 | train_loss=3.9932 | dev_loss=17.4442 | dev_acc=0.9014\n",
      "Epoch 19 | train_loss=3.6125 | dev_loss=18.6773 | dev_acc=0.8960\n",
      "Epoch 19 | train_loss=3.6125 | dev_loss=18.6773 | dev_acc=0.8960\n",
      "Epoch 20 | train_loss=3.2173 | dev_loss=18.6980 | dev_acc=0.9000\n",
      "Best dev accuracy: 0.9035\n",
      "Epoch 20 | train_loss=3.2173 | dev_loss=18.6980 | dev_acc=0.9000\n",
      "Best dev accuracy: 0.9035\n"
     ]
    }
   ],
   "source": [
    "# Task 4: Vòng lặp huấn luyện + báo cáo accuracy train/dev\n",
    "EPOCHS = 20\n",
    "best_dev_acc = 0.0\n",
    "best_state = None\n",
    "\n",
    "# Vòng lặp huấn luyện\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    # Huấn luyện một epoch\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, pad_idx=tag_to_ix[PAD_TAG])\n",
    "    dev_loss, dev_acc = evaluate(model, dev_loader, criterion, pad_idx=tag_to_ix[PAD_TAG])\n",
    "    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | dev_loss={dev_loss:.4f} | dev_acc={dev_acc:.4f}\")\n",
    "    \n",
    "    # Lưu mô hình tốt nhất theo dev accuracy\n",
    "    if dev_acc > best_dev_acc:\n",
    "        best_dev_acc = dev_acc\n",
    "        best_state = {k: v.cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "print(f\"Best dev accuracy: {best_dev_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9087f6",
   "metadata": {},
   "source": [
    "**Task 5: Đánh giá Mô hình**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13ca2384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5: Hàm dự đoán câu mới\n",
    "\n",
    "# Hàm mã hóa câu thành tensor word IDs\n",
    "def encode_sentence(words: List[str], word_to_ix: Dict[str,int]):\n",
    "    return torch.tensor([word_to_ix.get(w, word_to_ix[UNK_TOKEN]) for w in words], dtype=torch.long)\n",
    "\n",
    "# Hàm dự đoán nhãn cho câu mới\n",
    "def predict_sentence(model, sentence: str, word_to_ix, ix_to_tag):\n",
    "    model.eval()\n",
    "\n",
    "    # Tách câu thành từ và mã hóa thành tensor word IDs\n",
    "    words = sentence.strip().split()\n",
    "    x = encode_sentence(words, word_to_ix).unsqueeze(0)  # (1, T)\n",
    "    lengths = torch.tensor([x.size(1)], dtype=torch.long)\n",
    "    x = x.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "\n",
    "    # Dự đoán nhãn\n",
    "    with torch.no_grad():\n",
    "        logits = model(x, lengths)  # (1, T, C)\n",
    "        pred_ids = logits.argmax(dim=-1).squeeze(0).tolist()\n",
    "    pred_tags = [ix_to_tag[i] for i in pred_ids]\n",
    "    return list(zip(words, pred_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9cd2deb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRON'), ('love', 'VERB'), ('NLP', 'NOUN'), ('.', 'PUNCT')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Demo nhanh\n",
    "predict_sentence(model, \"I love NLP .\", word_to_ix, ix_to_tag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
