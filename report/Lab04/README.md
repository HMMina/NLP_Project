# Lab 4: Word Embeddings
## ðŸ“‹ ThÃ´ng tin chung
**Lab:** Word Embeddings vÃ  Word2Vec  
**Má»¥c tiÃªu:** TÃ¬m hiá»ƒu, triá»ƒn khai vÃ  phÃ¢n tÃ­ch cÃ¡c ká»¹ thuáº­t word embeddings trong NLP  
**Dataset:** 
- Universal Dependencies English EWT (UD_English-EWT/en_ewt-ud-train.txt)
- C4 Dataset (c4-train.00000-of-01024-30K.json)

---
## ðŸŽ¯ Má»¥c tiÃªu Lab
1. **Sá»­ dá»¥ng Pre-trained Models**: LÃ m viá»‡c vá»›i GloVe embeddings
2. **Document Embedding**: Biá»ƒu diá»…n vÄƒn báº£n báº±ng vector
3. **Huáº¥n luyá»‡n Word2Vec**: Train custom model tá»« dá»¯ liá»‡u thÃ´
4. **Spark MLlib**: Huáº¥n luyá»‡n trÃªn dá»¯ liá»‡u lá»›n vá»›i PySpark
5. **Visualization**: Trá»±c quan hÃ³a embeddings vá»›i PCA/t-SNE

---
## ðŸ“ Cáº¥u trÃºc thÆ° má»¥c
```
 src/
    Lab04/
       src/
          representations/
             word_embedder.py                    # Class WordEmbedder
 test/
    Lab04/
       test/
          lab4_test.py                            # Test GloVe pre-trained model
          lab4_spark_word2vec_demo.py             # Demo Spark Word2Vec
          lab4_embedding_training_demo.py         # Train Word2Vec tá»« scratch
 notebook/
    Lab04/
       Lab3.ipynb                                  # Notebook trá»±c quan hÃ³a embeddings
 data/
    UD_English-EWT/                                 # Dá»¯ liá»‡u UD English EWT
    c4-train.00000-of-01024-30K.json                # Dá»¯ liá»‡u C4
```

---
## HÆ°á»›ng dáº«n cÃ i Ä‘áº·t
### 1. YÃªu cáº§u há»‡ thá»‘ng
- Python 3.7+
- Java 8+ (cho PySpark)
- RAM: Tá»‘i thiá»ƒu 4GB (khuyáº¿n nghá»‹ 8GB+)

### 2. CÃ i Ä‘áº·t thÆ° viá»‡n

```bash
# CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
pip install gensim numpy pandas seaborn scikit-learn
pip install pyspark
```

### 3. Chuáº©n bá»‹ dá»¯ liá»‡u
Äáº£m báº£o cÃ¡c file dá»¯ liá»‡u sau tá»“n táº¡i:
- `../UD_English-EWT/en_ewt-ud-train.txt` (UD English EWT dataset)
- `../c4-train.00000-of-01024-30K.json` (C4 dataset)

---
## HÆ°á»›ng dáº«n cháº¡y
### Task 1 & 2: Pre-trained GloVe Model
Test sá»­ dá»¥ng GloVe pre-trained model:

```bash
cd test/Lab04/test
python lab4_test.py
```

### Task 3: Huáº¥n luyá»‡n Word2Vec tá»« scratch
Huáº¥n luyá»‡n model má»›i trÃªn UD English EWT:
```bash
cd test/Lab04/test
python lab4_embedding_training_demo.py
```

### Task 4: Spark Word2Vec trÃªn C4 Dataset
Huáº¥n luyá»‡n Word2Vec vá»›i PySpark:
```bash
cd test/Lab04/test
python lab4_spark_word2vec_demo.py
```

### Task 5: Trá»±c quan hÃ³a Embeddings

Má»Ÿ vÃ  cháº¡y notebook:
```bash
# Má»Ÿ Jupyter Notebook
jupyter notebook notebook/Lab04/Lab3.ipynb

# Hoáº·c má»Ÿ trong VS Code vÃ  cháº¡y tá»«ng cell
```

---

## Káº¿t quáº£ thá»±c thi

### Task 1 + 2 (lab4_test.py)
```
Äang táº£i mÃ´ hÃ¬nh: glove-wiki-gigaword-50 ...
MÃ´ hÃ¬nh Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng.

--- EVALUATION: WORD EMBEDDING EXPLORATION ---

Vector cá»§a 'king':
[ 0.50451   0.68607  -0.59517  -0.022801  0.60046  -0.13498  -0.08813
  0.47377  -0.61798  -0.31012  -0.076666  1.493    -0.034189 -0.98173
  0.68229   0.81722  -0.51874  -0.31503  -0.55809   0.66421   0.1961
 -0.13495  -0.11476  -0.30344   0.41177  -2.223    -1.0756   -1.0783
 -0.34354   0.33505   1.9927   -0.04234  -0.64319   0.71125   0.49159
  0.16754   0.34344  -0.25663  -0.8523    0.1661    0.40102   1.1685
 -1.0137   -0.21585  -0.15155   0.78321  -0.91241  -1.6106   -0.64426
 -0.51042 ]

Äá»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 'king' vÃ  'queen': 0.7839
Äá»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a 'king' vÃ  'man': 0.5309

Top 10 tá»« tÆ°Æ¡ng tá»± vá»›i 'computer':
  computers: 0.9165
  software: 0.8815
  technology: 0.8526
  electronic: 0.8126
  internet: 0.8060
  computing: 0.8026
  devices: 0.8016
  digital: 0.7992
  applications: 0.7913
  pc: 0.7883

Vector biá»ƒu diá»…n cÃ¢u 'The queen rules the country.':
[ 0.04564168  0.36530998 -0.55974334  0.04014383  0.09655549  0.15623933   
 -0.33622834 -0.12495166 -0.01031508 -0.5006717   0.18690467  0.17482166   
 -0.268985   -0.03096624  0.36686516  0.29983264  0.01397333 -0.06872118   
 -0.3260683  -0.210115    0.16835399 -0.03151734 -0.06204716  0.04301083   
 -0.06958768 -1.7792168  -0.54365396 -0.06104483 -0.17618     0.009181     
  3.3916333   0.08742473 -0.4675417  -0.213435    0.02391887 -0.04470453   
  0.20636833 -0.12902866 -0.28527132 -0.2431805  -0.3114423  -0.03833717   
  0.11977985 -0.01418401 -0.37086335  0.22069354 -0.28848937 -0.36188802   
 -0.00549529 -0.46997246]
```

### Task 3 (lab4_embedding_training_demo.py)
```
=== Step 1: Äá»c dá»¯ liá»‡u vÃ  táº¡o corpus stream ===
Sá»‘ cÃ¢u Ä‘á»c Ä‘Æ°á»£c: 14227

=== Step 2: Huáº¥n luyá»‡n mÃ´ hÃ¬nh Word2Vec ===
Huáº¥n luyá»‡n xong!

=== Step 3: LÆ°u mÃ´ hÃ¬nh vÃ o results/ ===
ÄÃ£ lÆ°u mÃ´ hÃ¬nh táº¡i: C:\Users\ADMIN\.vscode\NLP_APP\Lab04\results\word2vec_ewt.model

=== Step 4: Demo sá»­ dá»¥ng mÃ´ hÃ¬nh ===

Tá»« tÆ°Æ¡ng tá»± 'computer':
  grow: 0.9966
  extra: 0.9964
  organization: 0.9962
  nest: 0.9960
  raise: 0.9959

PhÃ©p tÆ°Æ¡ng tá»± (king - man + woman):
  tumor: 0.9901
  attach: 0.9895
  golf: 0.9893
  fostering: 0.9892
  setoff: 0.9892
```

### Task 4 (lab4_spark_word2vec_demo.py)
```
25/10/16 20:44:26 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
Top 5 words similar to 'computer':
+----------+------------------+
|word      |similarity        |
+----------+------------------+
|desktop   |0.6746280193328857|
|computers |0.6736775040626526|
|software  |0.6618790626525879|
|smartphone|0.6585460305213928|
|laptop    |0.6327508091926575|
+----------+------------------+
```

### Task 5 (Lab3.ipynb)

![alt text](image.png)

![alt text](image-1.png)

---
## PhÃ¢n tÃ­ch Káº¿t Quáº£
### 1. Task 1 & 2: PhÃ¢n tÃ­ch GloVe Pre-trained Model
#### 1.1. Vector Representation cá»§a 'king'
**Káº¿t quáº£:**
```python
Vector 50 chiá»u: [0.50451, 0.68607, -0.59517, ..., -0.51042]
```
**Nháº­n xÃ©t:**
- Má»—i tá»« Ä‘Æ°á»£c biá»ƒu diá»…n bá»Ÿi **50 sá»‘ thá»±c** (50-dimensional vector)
- CÃ¡c giÃ¡ trá»‹ trong khoáº£ng **-2.2 Ä‘áº¿n +2.0**, cho tháº¥y normalization tá»‘t
- Vector nÃ y encode **semantic meaning** cá»§a tá»« "king"
- **Ã nghÄ©a:** CÃ¡c tá»« cÃ³ nghÄ©a gáº§n nhau sáº½ cÃ³ vector gáº§n nhau trong khÃ´ng gian 50 chiá»u

#### 1.2. Äá»™ TÆ°Æ¡ng Äá»“ng Giá»¯a CÃ¡c Tá»«
**Káº¿t quáº£:**
```
king â†” queen: 0.7839 (78.39% tÆ°Æ¡ng Ä‘á»“ng)
king â†” man:   0.5309 (53.09% tÆ°Æ¡ng Ä‘á»“ng)
```

**PhÃ¢n tÃ­ch chi tiáº¿t:**
**Cáº·p "king - queen" (0.7839):**
- **Äiá»ƒm cao** (>75%) chá»©ng tá» mÃ´ hÃ¬nh hiá»ƒu ráº¥t tá»‘t má»‘i quan há»‡
- **LÃ½ do:** Cáº£ hai tá»«:
  - Thuá»™c cÃ¹ng semantic field: **royalty** (hoÃ ng gia)
  - Xuáº¥t hiá»‡n trong context tÆ°Æ¡ng tá»±: "throne", "crown", "kingdom"
  - CÃ³ chung cÃ¡c Ä‘áº·c trÆ°ng: quyá»n lá»±c, Ä‘á»‹a vá»‹ cao
- **So sÃ¡nh:** Score nÃ y cao hÆ¡n nhiá»u tá»« Ä‘á»“ng nghÄ©a thÃ´ng thÆ°á»ng (~0.6-0.7)

**Cáº·p "king - man" (0.5309):**
- **Äiá»ƒm trung bÃ¬nh** (50-55%) cho tháº¥y cÃ³ liÃªn quan nhÆ°ng khÃ´ng gáº§n nghÄ©a
- **LÃ½ do:** 
  - "king" cÃ³ thÃªm features vá» **quyá»n lá»±c, Ä‘á»‹a vá»‹**
  - "man" lÃ  tá»« generic vá» **giá»›i tÃ­nh**
  - Context khÃ¡c nhau: "king" vá»›i "throne", "man" vá»›i "person", "human"
- **Káº¿t luáº­n:** Tháº¥p hÆ¡n king-queen lÃ  há»£p lÃ½ vÃ¬ Ã½ nghÄ©a khÃ¡c biá»‡t rÃµ

#### 1.3. Top 10 Tá»« TÆ°Æ¡ng Tá»± vá»›i 'computer'
**Káº¿t quáº£ Ä‘áº§y Ä‘á»§:**
```
1. computers      0.9165  
2. software       0.8815
3. technology     0.8526
4. electronic     0.8126
5. internet       0.8060
6. computing      0.8026
7. devices        0.8016
8. digital        0.7992
9. applications   0.7913
10. pc            0.7883
```

**PhÃ¢n tÃ­ch theo nhÃ³m:**
**NhÃ³m 1: Biáº¿n thá»ƒ hÃ¬nh thÃ¡i (Score >0.91)**
- `computers` (0.9165) - Dáº¡ng sá»‘ nhiá»u
- **Giáº£i thÃ­ch:** CÃ¹ng root word, xuáº¥t hiá»‡n trong context gáº§n giá»‘ng há»‡t nhau
- **ÄÃ¡nh giÃ¡:** Perfect - Model há»c Ä‘Æ°á»£c morphology

**NhÃ³m 2: Tá»« liÃªn quan trá»±c tiáº¿p (Score 0.81-0.88)**
- `software` (0.8815) - Pháº§n má»m mÃ¡y tÃ­nh
- `technology` (0.8526) - CÃ´ng nghá»‡
- `electronic` (0.8126) - Äiá»‡n tá»­
- **Giáº£i thÃ­ch:** CÃ¡c tá»« nÃ y thÆ°á»ng **Ä‘i kÃ¨m** vá»›i computer trong text
- **ÄÃ¡nh giÃ¡:** Excellent - Semantic relationship rÃµ rÃ ng

**NhÃ³m 3: Tá»« cÃ¹ng domain (Score 0.79-0.80)**
- `internet`, `computing`, `devices`, `digital`, `applications`
- **Giáº£i thÃ­ch:** Thuá»™c cÃ¹ng **technology domain**
- **ÄÃ¡nh giÃ¡:** Very Good - Contextual similarity

**NhÃ³m 4: Tá»« Ä‘á»“ng nghÄ©a (Score 0.78)**
- `pc` (0.7883) - Personal Computer
- **Giáº£i thÃ­ch:** Synonym trá»±c tiáº¿p nhÆ°ng score khÃ´ng cao nháº¥t vÃ¬:
  - "pc" informal hÆ¡n "computer"
  - Usage context khÃ¡c nhau (pc â†’ home, computer â†’ general)
- **ÄÃ¡nh giÃ¡:** Good - Register differences Ä‘Æ°á»£c capture

**Tá»•ng káº¿t:**
- **10/10 tá»« Ä‘á»u chÃ­nh xÃ¡c** vÃ  cÃ³ semantic relationship vá»›i "computer"
- **Äiá»ƒm cao** (>0.78) cho tháº¥y confidence tá»‘t
- **Äa dáº¡ng** relationship types: morphology, synonymy, domain similarity
- **Káº¿t luáº­n:** GloVe model **xuáº¥t sáº¯c** cho tá»« phá»• biáº¿n

#### 1.4. Document Embedding
**Input:** "The queen rules the country."
**Output:** Vector 50 chiá»u
```python
[0.0456, 0.3653, -0.5597, ..., -0.4700]
```

**PhÆ°Æ¡ng phÃ¡p:** Average pooling cá»§a word vectors
**PhÃ¢n tÃ­ch:**
**Æ¯u Ä‘iá»ƒm:**
- **ÄÆ¡n giáº£n**: Dá»… implement, chá»‰ cáº§n average
- **Nhanh**: O(n) complexity vá»›i n lÃ  sá»‘ tá»«
- **á»”n Ä‘á»‹nh**: KhÃ´ng cáº§n training thÃªm
- **General meaning**: Capture Ä‘Æ°á»£c Ã½ nghÄ©a tá»•ng thá»ƒ

**NhÆ°á»£c Ä‘iá»ƒm:**
- **Máº¥t word order**: "queen rules country" = "country rules queen"
- **Stop words**: "the", "a" lÃ m loÃ£ng semantic content
- **No compositionality**: KhÃ´ng hiá»ƒu cáº¥u trÃºc ngá»¯ phÃ¡p
- **Equal weights**: Táº¥t cáº£ tá»« Ä‘á»u quan trá»ng nhÆ° nhau

**Cáº£i tiáº¿n cÃ³ thá»ƒ:**
```python
# 1. Remove stop words
tokens = [w for w in tokens if w not in stop_words]

# 2. Weighted average (TF-IDF)
weights = compute_tfidf(tokens)
doc_vec = sum(weights[i] * vectors[i])

# 3. Use sentence embeddings (BERT, USE)
doc_vec = sentence_transformer.encode(text)
```
---

### 2. Task 3: PhÃ¢n tÃ­ch Custom Word2Vec trÃªn UD Dataset
**ThÃ´ng tin huáº¥n luyá»‡n:**
```
Corpus: 14,227 cÃ¢u 
Model: Word2Vec Skip-gram
Vector size: 100 dimensions
Min count: 2
```

#### 2.1. Káº¿t Quáº£ "computer"
**Output:**
```
grow           0.9966
extra          0.9964
organization   0.9962
nest           0.9960
raise          0.9959
```

**PhÃ¢n tÃ­ch:**
- **Káº¿t quáº£ sai hoÃ n toÃ n** - KhÃ´ng cÃ³ tá»« nÃ o liÃªn quan Ä‘áº¿n technology/computing
- **Score cá»±c ká»³ cao** (>0.995) - Dáº¥u hiá»‡u overfitting nghiÃªm trá»ng
- **Tá»« hoÃ n toÃ n ngáº«u nhiÃªn:**
  - `grow` (Ä‘á»™ng tá»«: phÃ¡t triá»ƒn) - khÃ´ng liÃªn quan
  - `extra` (tÃ­nh tá»«: thÃªm) - khÃ´ng liÃªn quan
  - `organization` (danh tá»«: tá»• chá»©c) - khÃ´ng liÃªn quan
  - `nest` (danh tá»«: tá»•) - hoÃ n toÃ n xa láº¡
  - `raise` (Ä‘á»™ng tá»«: nÃ¢ng lÃªn) - khÃ´ng liÃªn quan

**NguyÃªn nhÃ¢n sÃ¢u xa:**
1. **Dataset quÃ¡ nhá»** (14,227 cÃ¢u â‰ˆ 200K tokens)
   - GloVe train trÃªn **6 billion tokens** (6,000,000,000)
   - Tá»‰ lá»‡: Custom model cÃ³ **30,000 láº§n Ã­t hÆ¡n** GloVe
   - Word2Vec cáº§n minimum **10M tokens** Ä‘á»ƒ cÃ³ káº¿t quáº£ kháº£ dá»¥ng
   
2. **Tá»« "computer" cá»±c ká»³ hiáº¿m trong UD dataset**
   - UD English-EWT lÃ  **linguistic treebank** (táº­p trung grammar, syntax)
   - KhÃ´ng pháº£i technology corpus
   - "computer" cÃ³ thá»ƒ chá»‰ xuáº¥t hiá»‡n 1-5 láº§n
   - KhÃ´ng Ä‘á»§ co-occurrence patterns Ä‘á»ƒ há»c semantic meaning
   
3. **Overfitting tráº§m trá»ng**
   - Model há»c **noise** (random co-occurrences) thay vÃ¬ **signal** (true semantics)
   - Score 0.996+ cho tháº¥y model Ä‘ang "ghi nhá»›" thay vÃ¬ "khÃ¡i quÃ¡t hÃ³a"
   - Vector cá»§a tá»« hiáº¿m bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi random context words
   
4. **Hyperparameters khÃ´ng phÃ¹ há»£p**
   - `min_count=2` quÃ¡ tháº¥p â†’ giá»¯ quÃ¡ nhiá»u tá»« hiáº¿m khÃ´ng Ä‘Ã¡ng tin cáº­y
   - NÃªn tÄƒng lÃªn `min_count=5` hoáº·c `10` Ä‘á»ƒ lá»c noise
   - Window size vÃ  epochs cÃ³ thá»ƒ chÆ°a tá»‘i Æ°u
```

**ÄÃ¡nh giÃ¡:**Very Poor - Model hoÃ n toÃ n khÃ´ng sá»­ dá»¥ng Ä‘Æ°á»£c**

#### 2.2. Káº¿t Quáº£ Word Analogy
**Test:** king - man + woman = ?  
**Expected:** queen

**Output:**
```
tumor      0.9901
attach     0.9895
golf       0.9893
fostering  0.9892
setoff     0.9892
```

**PhÃ¢n tÃ­ch:**
- **Tháº¥t báº¡i hoÃ n toÃ n** - KhÃ´ng cÃ³ "queen" trong top 5
- **Tá»« vÃ´ nghÄ©a:**
  - `tumor` (khá»‘i u) - khÃ´ng liÃªn quan royalty hay gender
  - `attach` (gáº¯n vÃ o) - Ä‘á»™ng tá»« ngáº«u nhiÃªn
  - `golf` (mÃ´n thá»ƒ thao) - hoÃ n toÃ n xa láº¡
  - `fostering` (nuÃ´i dÆ°á»¡ng) - khÃ´ng liÃªn quan
  - `setoff` (khá»Ÿi hÃ nh) - vÃ´ nghÄ©a
- **KhÃ´ng há»c Ä‘Æ°á»£c:**
  - Gender relationship (male â†” female)
  - Semantic parallelism (king:queen :: man:woman)
  - Word analogies require deep semantic understanding

**So sÃ¡nh vá»›i GloVe:**
```
GloVe Model:
  king - man + woman = queen (correct)
  
Custom Model:
  king - man + woman = tumor (nonsense)
```

**LÃ½ do tháº¥t báº¡i:**
- Word analogy lÃ  **high-level semantic task**
- Cáº§n model há»c Ä‘Æ°á»£c **fine-grained relationships**:
  - Gender: kingâ†”queen, manâ†”woman, princeâ†”princess
  - Plurality: carâ†”cars, childâ†”children
  - Tense: walkâ†”walked, goâ†”went
- YÃªu cáº§u **minimum 100M-1B tokens** Ä‘á»ƒ capture patterns
- 14K cÃ¢u hoÃ n toÃ n khÃ´ng Ä‘á»§
---

### 3. Task 4: PhÃ¢n tÃ­ch Spark Word2Vec trÃªn C4 Dataset
**Káº¿t quáº£:**
```
1. desktop      0.6746  
2. computers    0.6737  
3. software     0.6619  
4. smartphone   0.6585  
5. laptop       0.6328  
```

**PhÃ¢n tÃ­ch chi tiáº¿t:**
**Tá»« 1-2: Hardware devices (0.67-0.67)**
- `desktop`, `laptop` - CÃ¹ng category vá»›i computer
- **Giáº£i thÃ­ch:** C4 dataset cÃ³ nhiá»u tech content
- **ÄÃ¡nh giÃ¡:** Excellent match

**Tá»« 3: Software (0.66)**
- `software` - Direct relationship
- **ÄÃ¡nh giÃ¡:** Perfect

**Tá»« 4: Mobile device (0.65)**
- `smartphone` - Modern computing device
- **ÄÃ¡nh giÃ¡:** Very relevant

**Tá»« 5: Portable computer (0.63)**
- `laptop` - Computing device
- **ÄÃ¡nh giÃ¡:** Excellent

**So sÃ¡nh vá»›i GloVe:**
**Nháº­n xÃ©t:**
- **Káº¿t quáº£ tá»‘t** - Táº¥t cáº£ 5 tá»« Ä‘á»u relevant
- **Better than Task 3** - C4 dataset lá»›n hÆ¡n UD nhiá»u
- **Tháº¥p hÆ¡n GloVe** - Score 0.63-0.67 vs 0.78-0.91
- **LÃ½ do:** C4 dataset váº«n nhá» hÆ¡n Wikipedia + Gigaword

**ÄÃ¡nh giÃ¡:** (Very Good - Production ready cho domain-specific tasks)

---
### 4. Task 5: PhÃ¢n tÃ­ch Visualization
#### 4.1. Biá»ƒu Ä‘á»“ (image.png) 2D PCA vs t-SNE
**ÄÃ¡nh giÃ¡:**
```
*   PCA (hÃ¬nh trÃ¡i) cho tháº¥y cÃ¡c Ä‘iá»ƒm phÃ¢n bá»‘ tÆ°Æ¡ng Ä‘á»‘i táº­p trung, khÃ´ng hÃ¬nh thÃ nh cá»¥m rÃµ rÃ ng. CÃ¡c tá»« cÃ³ táº§n suáº¥t xuáº¥t hiá»‡n cao hoáº·c mang tÃ­nh ngá»¯ phÃ¡p (nhÆ° the, of, and, to, is) thÆ°á»ng náº±m gáº§n nhau, pháº£n Ã¡nh báº£n cháº¥t tuyáº¿n tÃ­nh cá»§a PCA.
*   t-SNE (hÃ¬nh pháº£i) cho tháº¥y cÃ¡c Ä‘iá»ƒm tÃ¡ch biá»‡t hÆ¡n, táº¡o ra nhiá»u cá»¥m nhá». Má»™t sá»‘ nhÃ³m tá»« cÃ³ thá»ƒ Ä‘Æ°á»£c gom láº¡i gáº§n nhau do cÃ³ ngá»¯ nghÄ©a tÆ°Æ¡ng tá»± (vÃ­ dá»¥: president â€“ government â€“ state hoáº·c percent â€“ million). Äiá»u nÃ y chá»©ng tá» t-SNE giá»¯ Ä‘Æ°á»£c má»‘i quan há»‡ ngá»¯ nghÄ©a cá»¥c bá»™ tá»‘t hÆ¡n.

-> PCA giÃºp hÃ¬nh dung cáº¥u trÃºc toÃ n cá»¥c, trong khi t-SNE cho tháº¥y cÃ¡c quan há»‡ ngá»¯ nghÄ©a chi tiáº¿t hÆ¡n giá»¯a cÃ¡c tá»«.
```

#### 4.2. Biá»ƒu Ä‘á»“ (image-1.png) 3D PCA vs t-SNE
**ÄÃ¡nh giÃ¡:**
```
*   PCA (hÃ¬nh trÃ¡i) cÃ¡c Ä‘iá»ƒm Ä‘Æ°á»£c phÃ¢n bá»‘ khÃ¡ gáº§n nhau, táº­p trung thÃ nh má»™t vÃ¹ng chÃ­nh. Äiá»u nÃ y cho tháº¥y PCA giá»¯ Ä‘Æ°á»£c cáº¥u trÃºc tá»•ng thá»ƒ nhÆ°ng khÃ´ng thá»ƒ hiá»‡n rÃµ cÃ¡c cá»¥m ngá»¯ nghÄ©a riÃªng biá»‡t. Nhá»¯ng tá»« cÃ³ táº§n suáº¥t cao hoáº·c mang nghÄ©a ngá»¯ phÃ¡p (the, of, and, to, is) váº«n xuáº¥t hiá»‡n gáº§n nhau, Ä‘Ãºng nhÆ° Ä‘áº·c tÃ­nh cá»§a PCA.
*   t-SNE (hÃ¬nh pháº£i) phÃ¢n bá»‘ cá»§a cÃ¡c Ä‘iá»ƒm rá»i ráº¡c vÃ  cÃ³ xu hÆ°á»›ng táº¡o thÃ nh nhiá»u cá»¥m nhá» hÆ¡n. Má»™t sá»‘ cá»¥m pháº£n Ã¡nh má»‘i quan há»‡ ngá»¯ nghÄ©a khÃ¡ tá»± nhiÃªn â€” vÃ­ dá»¥: nhÃ³m tá»« liÃªn quan Ä‘áº¿n chÃ­nh trá»‹ (president, government, state), nhÃ³m vá» sá»‘ lÆ°á»£ng (million, percent), hoáº·c nhÃ³m Ä‘á»™ng tá»« (be, is, was, been). t-SNE giÃºp bá»™c lá»™ rÃµ hÆ¡n cÃ¡c quan há»‡ ngá»¯ nghÄ©a cá»¥c bá»™ giá»¯a cÃ¡c tá»« mÃ  PCA khÃ´ng thá»ƒ hiá»‡n Ä‘Æ°á»£c.

-> Káº¿t quáº£ cho tháº¥y cáº£ hai phÆ°Æ¡ng phÃ¡p Ä‘á»u cÃ³ giÃ¡ trá»‹ riÃªng, PCA cho cÃ¡i nhÃ¬n tá»•ng quan, t-SNE cho cÃ¡i nhÃ¬n chi tiáº¿t hÆ¡n vá» cáº¥u trÃºc ngá»¯ nghÄ©a.
```

---

## ðŸ”§ Troubleshooting
### Lá»—i: ModuleNotFoundError: No module named 'Lab01'
**Giáº£i phÃ¡p:**
```python
# ÄÃ£ thÃªm __init__.py files vÃ o Lab01
Lab01/__init__.py
Lab01/src/__init__.py
Lab01/src/preprocessing/__init__.py
Lab01/src/core/__init__.py
```

```
import sys
import os
workspace_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..', '..'))
sys.path.insert(0, workspace_root)
```

### Lá»—i: PySpark installation failed
**Giáº£i phÃ¡p:**
1. Kiá»ƒm tra Java Ä‘Ã£ cÃ i: `java -version`
2. CÃ i PySpark: `pip install pyspark`
3. Náº¿u lá»—i, thá»­: `pip install pyspark --user`
---

## Káº¿t luáº­n
### TÃ³m táº¯t Lab 4
Lab nÃ y Ä‘Ã£ giÃºp chÃºng ta hiá»ƒu sÃ¢u vá» **Word Embeddings** - má»™t ká»¹ thuáº­t quan trá»ng trong NLP:
**Nhá»¯ng gÃ¬ Ä‘Ã£ há»c Ä‘Æ°á»£c:**
1. **Pre-trained Models (GloVe)**
   - Tiá»‡n lá»£i, cháº¥t lÆ°á»£ng cao, khÃ´ng cáº§n training
   - PhÃ¹ há»£p cho háº§u háº¿t cÃ¡c task NLP tá»•ng quÃ¡t
   - Vocabulary lá»›n (400K tá»«), coverage tá»‘t

2. **Custom Training vá»›i Word2Vec**
   - Cáº§n dataset lá»›n (millions tokens) Ä‘á»ƒ cÃ³ káº¿t quáº£ tá»‘t
   - PhÃ¹ há»£p cho domain-specific applications
   - Cho phÃ©p control hyperparameters

3. **Spark MLlib cho Big Data**
   - Xá»­ lÃ½ distributed training trÃªn dá»¯ liá»‡u lá»›n
   - Scalable vÃ  hiá»‡u quáº£
   - Káº¿t quáº£ tá»‘t vá»›i C4 dataset

4. **Visualization vá»›i PCA/t-SNE**
   - PCA: Fast, linear, good for overview
   - t-SNE: Slow, non-linear, excellent for detailed analysis
   - GiÃºp hiá»ƒu semantic relationships giá»¯a cÃ¡c tá»«

5. **Practical Applications**
   - Document embedding
   - Similarity computation
   - Word analogies
   - Semantic clustering

**BÃ i há»c quan trá»ng:**
- **Data size matters**: 200K tokens â†’ poor, 6B tokens â†’ excellent
- **Pre-trained > Custom** cho general tasks
- **Visualization helps** validate model quality
- **Domain-specific** training cÃ³ giÃ¡ trá»‹ khi cÃ³ large corpus
- **Spark** enables large-scale training

**Khuyáº¿n nghá»‹ thá»±c táº¿:**
- DÃ¹ng **GloVe pre-trained** cho háº§u háº¿t cÃ¡c task
- Chá»‰ train custom khi cÃ³ **dataset lá»›n** (millions tokens) vÃ  domain-specific
- DÃ¹ng **Spark Word2Vec** khi data > 1GB
- LuÃ´n **visualize embeddings** Ä‘á»ƒ kiá»ƒm tra quality
- **Document embedding**: Remove stop words, consider weighted average
- Upgrade lÃªn **BERT/Transformers** cho advanced tasks

