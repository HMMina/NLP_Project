# BÃ¡o CÃ¡o Lab 07: Giá»›i Thiá»‡u vá» Transformers (lab7_intro_transformers.ipynb)

## ğŸ“‹ Má»¥c Lá»¥c
1. [Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai](#1-giáº£i-thÃ­ch-cÃ¡c-bÆ°á»›c-triá»ƒn-khai)
2. [HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£](#2-hÆ°á»›ng-dáº«n-thá»±c-thi-mÃ£)
3. [PhÃ¢n TÃ­ch Káº¿t Quáº£](#3-phÃ¢n-tÃ­ch-káº¿t-quáº£)
4. [ThÃ¡ch Thá»©c vÃ  Giáº£i PhÃ¡p](#4-thÃ¡ch-thá»©c-vÃ -giáº£i-phÃ¡p)
5. [HÆ°á»›ng PhÃ¡t Triá»ƒn](#5-hÆ°á»›ng-phÃ¡t-triá»ƒn)

---

## 1. Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai

### Task 1: Masked Language Modeling (Fill-Mask)
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒu cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a mÃ´ hÃ¬nh Encoder-only (nhÆ° BERT) thÃ´ng qua bÃ i toÃ¡n Ä‘iá»n tá»« vÃ o chá»— trá»‘ng.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Sá»­ dá»¥ng `pipeline("fill-mask")` tá»« thÆ° viá»‡n `transformers`.
  - ÄÆ°a vÃ o cÃ¢u cÃ³ chá»©a token Ä‘áº·c biá»‡t `<mask>` (vÃ­ dá»¥: "Hanoi is the \<mask> of Vietnam.").
  - MÃ´ hÃ¬nh sáº½ dá»± Ä‘oÃ¡n cÃ¡c tá»« cÃ³ kháº£ nÄƒng Ä‘iá»n vÃ o vá»‹ trÃ­ `<mask>` dá»±a trÃªn ngá»¯ cáº£nh hai chiá»u (trÆ°á»›c vÃ  sau).
  - **LÃ½ thuyáº¿t**: MÃ´ hÃ¬nh Encoder-only (BERT) Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ hiá»ƒu ngá»¯ cáº£nh toÃ n cá»¥c, do Ä‘Ã³ ráº¥t giá»i trong viá»‡c Ä‘iá»n tá»« bá»‹ thiáº¿u.

### Task 2: Causal Language Modeling (Text Generation)
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒu cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a mÃ´ hÃ¬nh Decoder-only (nhÆ° GPT) thÃ´ng qua bÃ i toÃ¡n sinh vÄƒn báº£n.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Sá»­ dá»¥ng `pipeline("text-generation")` tá»« thÆ° viá»‡n `transformers`.
  - Cung cáº¥p má»™t Ä‘oáº¡n vÄƒn báº£n má»“i (prompt), vÃ­ dá»¥: "The best thing about learning NLP is".
  - MÃ´ hÃ¬nh sáº½ tá»± Ä‘á»™ng sinh tiáº¿p cÃ¡c tá»« tiáº¿p theo dá»±a trÃªn xÃ¡c suáº¥t.
  - **LÃ½ thuyáº¿t**: MÃ´ hÃ¬nh Decoder-only (GPT) Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ dá»± Ä‘oÃ¡n tá»« tiáº¿p theo (next token prediction) dá»±a trÃªn cÃ¡c tá»« Ä‘Ã£ xuáº¥t hiá»‡n trÆ°á»›c Ä‘Ã³ (unidirectional), phÃ¹ há»£p cho cÃ¡c tÃ¡c vá»¥ sÃ¡ng táº¡o ná»™i dung.

### Task 3: Sentence Embeddings vá»›i BERT
- **Má»¥c Ä‘Ã­ch**: TrÃ­ch xuáº¥t vector biá»ƒu diá»…n ngá»¯ nghÄ©a cá»§a cÃ¢u (Sentence Embedding) tá»« mÃ´ hÃ¬nh BERT.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Táº£i mÃ´ hÃ¬nh `bert-base-uncased` vÃ  tokenizer tÆ°Æ¡ng á»©ng.
  - Tokenize cÃ¢u Ä‘áº§u vÃ o, thÃªm padding vÃ  truncation Ä‘á»ƒ cÃ³ Ä‘á»™ dÃ i cá»‘ Ä‘á»‹nh.
  - ÄÆ°a qua mÃ´ hÃ¬nh BERT Ä‘á»ƒ láº¥y `last_hidden_state` (vector biá»ƒu diá»…n cá»§a tá»«ng token).
  - Thá»±c hiá»‡n **Mean Pooling**: TÃ­nh trung bÃ¬nh cá»™ng cÃ¡c vector cá»§a cÃ¡c token trong cÃ¢u (lÆ°u Ã½ sá»­ dá»¥ng `attention_mask` Ä‘á»ƒ loáº¡i bá» cÃ¡c token padding).
  - **Káº¿t quáº£**: Thu Ä‘Æ°á»£c má»™t vector cá»‘ Ä‘á»‹nh (kÃ­ch thÆ°á»›c 768) Ä‘áº¡i diá»‡n cho Ã½ nghÄ©a cá»§a toÃ n bá»™ cÃ¢u.

---

## 2. HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£

### 2.1 YÃªu Cáº§u Há»‡ Thá»‘ng
```bash
pip install transformers torch
```

### 2.2 Cáº¥u TrÃºc ThÆ° Má»¥c
```
 notebook/
    Lab07/
       lab7_intro_transformers.ipynb
```

### 2.3 CÃ¡ch Cháº¡y
1. **Má»Ÿ Jupyter Notebook**:
   ```bash
   jupyter notebook notebook/Lab07/lab7_intro_transformers.ipynb
   ```
2. **Cháº¡y tuáº§n tá»± cÃ¡c cell**:
   - Cell 1: Task 1 - Fill-Mask vá»›i BERT.
   - Cell 2: Tráº£ lá»i cÃ¢u há»i Task 1.
   - Cell 3: Task 2 - Text Generation vá»›i GPT-2.
   - Cell 4: Tráº£ lá»i cÃ¢u há»i Task 2.
   - Cell 5: Task 3 - Sentence Embedding vá»›i BERT.
   - Cell 6: Tráº£ lá»i cÃ¢u há»i Task 3.

---

## 3. PhÃ¢n TÃ­ch Káº¿t Quáº£

### 3.1 Task 1: Fill-Mask
- **Input**: "Hanoi is the \<mask> of Vietnam."
- **Dá»± Ä‘oÃ¡n**:
  - `capital` (score cao nháº¥t): ChÃ­nh xÃ¡c vá» máº·t thá»±c táº¿ vÃ  ngá»¯ nghÄ©a.
  - CÃ¡c tá»« khÃ¡c cÃ³ thá»ƒ lÃ  `heart`, `center`, `city`... tÃ¹y thuá»™c vÃ o ngá»¯ cáº£nh mÃ  mÃ´ hÃ¬nh Ä‘Ã£ há»c.
- **Nháº­n xÃ©t**: MÃ´ hÃ¬nh BERT hiá»ƒu ráº¥t tá»‘t ngá»¯ cáº£nh hai chiá»u. Tá»« "Hanoi" (Ä‘á»©ng trÆ°á»›c) vÃ  "Vietnam" (Ä‘á»©ng sau) giÃºp mÃ´ hÃ¬nh xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c tá»« cáº§n Ä‘iá»n lÃ  "capital".

### 3.2 Task 2: Text Generation
- **Input**: "The best thing about learning NLP is"
- **Output**: Má»™t Ä‘oáº¡n vÄƒn báº£n tiáº¿p diá»…n há»£p lÃ½, vÃ­ dá»¥: "...that it allows computers to understand human language..."
- **Nháº­n xÃ©t**: MÃ´ hÃ¬nh GPT sinh vÄƒn báº£n trÃ´i cháº£y, ngá»¯ phÃ¡p Ä‘Ãºng. Tuy nhiÃªn, ná»™i dung cÃ³ thá»ƒ thay Ä‘á»•i má»—i láº§n cháº¡y do tÃ­nh cháº¥t ngáº«u nhiÃªn (sampling) trong quÃ¡ trÃ¬nh sinh.

### 3.3 Task 3: Sentence Embedding
- **Output**: Má»™t vector cÃ³ kÃ­ch thÆ°á»›c `(1, 768)`.
- **Ã nghÄ©a**:
  - Con sá»‘ 768 tÆ°Æ¡ng á»©ng vá»›i `hidden_size` cá»§a mÃ´ hÃ¬nh `bert-base-uncased`.
  - Vector nÃ y chá»©a thÃ´ng tin ngá»¯ nghÄ©a cá»§a cÃ¢u "This is a sample sentence.".
- **Vai trÃ² cá»§a Attention Mask**:
  - Náº¿u khÃ´ng dÃ¹ng `attention_mask` khi tÃ­nh trung bÃ¬nh (Mean Pooling), cÃ¡c giÃ¡ trá»‹ 0 cá»§a padding token sáº½ bá»‹ tÃ­nh vÃ o, lÃ m "loÃ£ng" vector biá»ƒu diá»…n cá»§a cÃ¢u, dáº«n Ä‘áº¿n sai lá»‡ch ngá»¯ nghÄ©a.
  - Code Ä‘Ã£ xá»­ lÃ½ Ä‘Ãºng báº±ng cÃ¡ch nhÃ¢n `last_hidden_state` vá»›i `mask_expanded` trÆ°á»›c khi tÃ­nh tá»•ng vÃ  chia cho tá»•ng mask.

---

## 4. ThÃ¡ch Thá»©c vÃ  Giáº£i PhÃ¡p

### 4.1 ThÃ¡ch Thá»©c
- **KÃ­ch thÆ°á»›c mÃ´ hÃ¬nh lá»›n**: CÃ¡c mÃ´ hÃ¬nh Transformer (BERT, GPT) thÆ°á»ng ráº¥t náº·ng, tá»‘n nhiá»u RAM vÃ  thá»i gian táº£i.
- **Giá»›i háº¡n Ä‘á»™ dÃ i (Max Sequence Length)**: BERT thÆ°á»ng giá»›i háº¡n 512 tokens. Náº¿u cÃ¢u quÃ¡ dÃ i sáº½ bá»‹ cáº¯t (truncation), máº¥t thÃ´ng tin.
- **Padding**: Viá»‡c xá»­ lÃ½ padding thá»§ cÃ´ng khi tÃ­nh pooling khÃ¡ phá»©c táº¡p vÃ  dá»… sai sÃ³t.

### 4.2 Giáº£i PhÃ¡p
- **Sá»­ dá»¥ng Pipeline**: ThÆ° viá»‡n `transformers` cung cáº¥p `pipeline` giÃºp áº©n Ä‘i cÃ¡c bÆ°á»›c tiá»n xá»­ lÃ½ phá»©c táº¡p, dá»… dÃ ng sá»­ dá»¥ng cho ngÆ°á»i má»›i.
- **Mean Pooling cáº©n tháº­n**: LuÃ´n nhá»› sá»­ dá»¥ng `attention_mask` Ä‘á»ƒ loáº¡i bá» padding khi tÃ­nh toÃ¡n thá»§ cÃ´ng trÃªn output cá»§a BERT.
- **DistilBERT**: Sá»­ dá»¥ng cÃ¡c phiÃªn báº£n nhá» gá»n hÆ¡n (nhÆ° DistilBERT) náº¿u tÃ i nguyÃªn pháº§n cá»©ng háº¡n cháº¿.

---

## 5. HÆ°á»›ng PhÃ¡t Triá»ƒn
- **Fine-tuning**: Thay vÃ¬ chá»‰ dÃ¹ng pre-trained model, cÃ³ thá»ƒ fine-tune BERT trÃªn bá»™ dá»¯ liá»‡u cá»¥ thá»ƒ (vÃ­ dá»¥: phÃ¢n loáº¡i vÄƒn báº£n y táº¿, phÃ¡p luáº­t) Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c.
- **Sentence-BERT (SBERT)**: Sá»­ dá»¥ng thÆ° viá»‡n `sentence-transformers` (Ä‘Æ°á»£c xÃ¢y dá»±ng dá»±a trÃªn BERT nhÆ°ng tá»‘i Æ°u cho sentence embedding) Ä‘á»ƒ táº¡o vector cÃ¢u tá»‘t hÆ¡n vÃ  so sÃ¡nh Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng (cosine similarity).
- **á»¨ng dá»¥ng thá»±c táº¿**:
  - DÃ¹ng Sentence Embedding Ä‘á»ƒ xÃ¢y dá»±ng há»‡ thá»‘ng tÃ¬m kiáº¿m ngá»¯ nghÄ©a (Semantic Search).
  - DÃ¹ng Text Generation Ä‘á»ƒ xÃ¢y dá»±ng Chatbot hoáº·c cÃ´ng cá»¥ há»— trá»£ viáº¿t lÃ¡ch.
