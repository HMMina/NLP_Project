# BÃ¡o CÃ¡o Lab 06: GÃ¡n NhÃ£n Tá»« Loáº¡i (POS Tagging) vá»›i RNN (lab6_rnn_for_pos_tagging.ipynb)

## ğŸ“‹ Má»¥c Lá»¥c
1. [Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai](#1-giáº£i-thÃ­ch-cÃ¡c-bÆ°á»›c-triá»ƒn-khai)
2. [HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£](#2-hÆ°á»›ng-dáº«n-thá»±c-thi-mÃ£)
3. [PhÃ¢n TÃ­ch Káº¿t Quáº£](#3-phÃ¢n-tÃ­ch-káº¿t-quáº£)
4. [ThÃ¡ch Thá»©c vÃ  Giáº£i PhÃ¡p](#4-thÃ¡ch-thá»©c-vÃ -giáº£i-phÃ¡p)
5. [HÆ°á»›ng PhÃ¡t Triá»ƒn](#5-hÆ°á»›ng-phÃ¡t-triá»ƒn)

---

## 1. Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai
### Task 1: Táº£i vÃ  Tiá»n xá»­ lÃ½ Dá»¯ liá»‡u
- **Má»¥c Ä‘Ã­ch**: Äá»c dá»¯ liá»‡u tá»« Ä‘á»‹nh dáº¡ng CoNLL-U vÃ  xÃ¢y dá»±ng tá»« Ä‘iá»ƒn.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Äá»c file `.conllu` (train/dev) Ä‘á»ƒ láº¥y danh sÃ¡ch cÃ¡c cÃ¢u, má»—i cÃ¢u gá»“m cÃ¡c cáº·p `(word, upos)`.
  - XÃ¢y dá»±ng `word_to_ix` (Ã¡nh xáº¡ tá»« -> index) vÃ  `tag_to_ix` (Ã¡nh xáº¡ nhÃ£n -> index).
  - ThÃªm cÃ¡c token Ä‘áº·c biá»‡t: `<PAD>` (Ä‘á»‡m), `<UNK>` (tá»« láº¡).
  - **Káº¿t quáº£**:
    - Train sentences: 12,544 cÃ¢u.
    - Dev sentences: 2,001 cÃ¢u.
    - Vocab size: 6,733 tá»« (min_freq=3).
    - Tag size: 18 nhÃ£n (bao gá»“m `<PAD>`).

### Task 2: Táº¡o PyTorch Dataset vÃ  DataLoader
- **Má»¥c Ä‘Ã­ch**: Chuáº©n bá»‹ dá»¯ liá»‡u dáº¡ng batch Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Táº¡o class `POSDataset` káº¿ thá»«a tá»« `torch.utils.data.Dataset`.
  - Viáº¿t hÃ m `collate_fn` sá»­ dá»¥ng `pad_sequence` Ä‘á»ƒ Ä‘á»‡m cÃ¡c cÃ¢u trong batch vá» cÃ¹ng Ä‘á»™ dÃ i.
  - Táº¡o `DataLoader` cho táº­p train (shuffle=True) vÃ  dev (shuffle=False) vá»›i `batch_size=64`.

### Task 3: XÃ¢y dá»±ng MÃ´ hÃ¬nh RNN
- **Má»¥c Ä‘Ã­ch**: XÃ¢y dá»±ng mÃ´ hÃ¬nh sequence labeling sá»­ dá»¥ng RNN.
- **Kiáº¿n trÃºc mÃ´ hÃ¬nh**:
  1. **Embedding Layer**: Chuyá»ƒn Ä‘á»•i index cá»§a tá»« thÃ nh vector (dim=100).
  2. **RNN Layer**: Bidirectional RNN (hidden_dim=128) Ä‘á»ƒ náº¯m báº¯t ngá»¯ cáº£nh hai chiá»u.
  3. **Dropout**: Giáº£m overfitting (p=0.1).
  4. **Linear Layer**: Ãnh xáº¡ output cá»§a RNN sang khÃ´ng gian nhÃ£n (output_dim=18).
- **Ká»¹ thuáº­t Ä‘áº·c biá»‡t**: Sá»­ dá»¥ng `pack_padded_sequence` vÃ  `pad_packed_sequence` Ä‘á»ƒ RNN bá» qua cÃ¡c token padding, tÄƒng hiá»‡u quáº£ tÃ­nh toÃ¡n.

### Task 4: Huáº¥n luyá»‡n MÃ´ hÃ¬nh
- **Cáº¥u hÃ¬nh**:
  - Optimizer: Adam (lr=1e-3).
  - Loss function: CrossEntropyLoss (ignore_index=PAD_TAG).
  - Epochs: 20.
- **Quy trÃ¬nh**:
  - Forward pass -> TÃ­nh loss -> Backward pass -> Update weights.
  - Theo dÃµi loss vÃ  accuracy trÃªn táº­p dev sau má»—i epoch.
  - LÆ°u láº¡i tráº¡ng thÃ¡i mÃ´ hÃ¬nh tá»‘t nháº¥t (best dev accuracy).

### Task 5: ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh
- **Má»¥c Ä‘Ã­ch**: Kiá»ƒm tra hiá»‡u suáº¥t mÃ´ hÃ¬nh trÃªn táº­p dev vÃ  dá»± Ä‘oÃ¡n cÃ¢u má»›i.
- **PhÆ°Æ¡ng phÃ¡p**:
  - TÃ­nh accuracy trÃªn cÃ¡c token thá»±c (bá» qua padding).
  - Viáº¿t hÃ m `predict_sentence` Ä‘á»ƒ gÃ¡n nhÃ£n cho cÃ¢u nháº­p vÃ o báº¥t ká»³.

---

## 2. HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£
### 2.1 YÃªu Cáº§u Há»‡ Thá»‘ng
```bash
pip install torch numpy
```

### 2.2 Cáº¥u TrÃºc ThÆ° Má»¥c
```
Lab06/
â”œâ”€â”€ lab6_rnn_for_pos_tagging.ipynb
â”œâ”€â”€ rnn_for_pos_tagging.md
â””â”€â”€ ../UD_English-EWT/
    â”œâ”€â”€ en_ewt-ud-train.conllu
    â””â”€â”€ en_ewt-ud-dev.conllu
```

### 2.3 CÃ¡ch Cháº¡y
1. **Má»Ÿ Jupyter Notebook**:
   ```bash
   jupyter notebook lab6_rnn_for_pos_tagging.ipynb
   ```
2. **Cháº¡y tuáº§n tá»± cÃ¡c cell**:
   - Cell 1-2: Import vÃ  setup.
   - Cell 3-5: Task 1 (Data Loading & Vocab).
   - Cell 6-8: Task 2 (Dataset & DataLoader).
   - Cell 9-10: Task 3 (Model Definition).
   - Cell 11-13: Task 4 (Training).
   - Cell 14-15: Task 5 (Evaluation & Demo).

---

## 3. PhÃ¢n TÃ­ch Káº¿t Quáº£

### 3.1 QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n Chi Tiáº¿t
DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ chi tiáº¿t cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n qua 20 epochs:

| Epoch | Train Loss | Dev Loss | Dev Accuracy | Nháº­n XÃ©t |
|:-----:|:----------:|:--------:|:------------:|:---------|
| 1     | 67.9458    | 33.3044  | 0.7603       | Khá»Ÿi Ä‘áº§u tá»‘t, mÃ´ hÃ¬nh há»c nhanh cÃ¡c quy luáº­t cÆ¡ báº£n. |
| 2     | 36.7162    | 25.0620  | 0.8155       | Loss giáº£m máº¡nh (~50%), accuracy tÄƒng >5%. |
| 3     | 27.3685    | 20.9647  | 0.8466       | |
| 4     | 21.4835    | 18.4810  | 0.8632       | |
| 5     | 17.7275    | 16.6868  | 0.8767       | Káº¿t thÃºc giai Ä‘oáº¡n há»c nhanh. |
| 6     | 15.0122    | 15.5063  | 0.8876       | |
| 7     | 12.7403    | 14.9340  | 0.8937       | Dev loss báº¯t Ä‘áº§u á»•n Ä‘á»‹nh quanh má»©c 14-15. |
| 8     | 11.3818    | 14.6995  | 0.8957       | |
| 9     | 10.1980    | 14.7442  | 0.8952       | |
| 10    | 9.1291     | 15.0488  | 0.8945       | |
| 11    | 8.2330     | 14.5638  | 0.8999       | Tiá»‡m cáº­n má»©c 90%. |
| 12    | 7.4461     | 14.9955  | 0.8995       | |
| 13    | 6.8169     | 14.9773  | 0.9035       | Äáº¡t Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t (Best Model). |
| 14    | 6.2771     | 15.7560  | 0.8972       | Dev loss báº¯t Ä‘áº§u tÄƒng -> Dáº¥u hiá»‡u Overfitting. |
| 15    | 5.5244     | 16.7355  | 0.8951       | |
| 16    | 5.0551     | 16.2119  | 0.9021       | |
| 17    | 4.5238     | 16.8760  | 0.9006       | |
| 18    | 3.9932     | 17.4442  | 0.9014       | |
| 19    | 3.6125     | 18.6773  | 0.8960       | |
| 20    | 3.2173     | 18.6980  | 0.9000       | Train loss ráº¥t tháº¥p nhÆ°ng Dev loss cao nháº¥t. |

### 3.2 Nháº­n XÃ©t

#### 1. Hiá»‡u Suáº¥t Tá»•ng Thá»ƒ
- **Äá»‰nh cao (Peak Performance)**: MÃ´ hÃ¬nh Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c tá»‘t nháº¥t lÃ  **90.35%** táº¡i Epoch 13. ÄÃ¢y lÃ  káº¿t quáº£ ráº¥t kháº£ quan cho má»™t mÃ´ hÃ¬nh RNN Ä‘Æ¡n giáº£n (khÃ´ng dÃ¹ng pre-trained embeddings hay kiáº¿n trÃºc phá»©c táº¡p nhÆ° Transformer).
- **Tá»‘c Ä‘á»™ há»™i tá»¥**: MÃ´ hÃ¬nh há»™i tá»¥ khÃ¡ nhanh. Chá»‰ sau 5 epochs Ä‘áº§u tiÃªn, Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã£ Ä‘áº¡t 87.67%. CÃ¡c epochs sau Ä‘Ã³ chá»§ yáº¿u tinh chá»‰nh cÃ¡c trÆ°á»ng há»£p khÃ³ (nhÆ° tá»« Ä‘a nghÄ©a, tá»« hiáº¿m).

#### 2. PhÃ¢n TÃ­ch Overfitting
- **Giai Ä‘oáº¡n 1 (Epoch 1-11)**: Cáº£ Train Loss vÃ  Dev Loss Ä‘á»u giáº£m. ÄÃ¢y lÃ  giai Ä‘oáº¡n "Learning" hiá»‡u quáº£ nháº¥t.
- **Giai Ä‘oáº¡n 2 (Epoch 12-13)**: Dev Loss Ä‘i ngang (khoáº£ng 14.9), trong khi Train Loss tiáº¿p tá»¥c giáº£m. ÄÃ¢y lÃ  Ä‘iá»ƒm tá»‘i Æ°u ("Sweet Spot").
- **Giai Ä‘oáº¡n 3 (Epoch 14-20)**:
    - **Train Loss** giáº£m sÃ¢u xuá»‘ng 3.2 (mÃ´ hÃ¬nh há»c thuá»™c lÃ²ng dá»¯ liá»‡u huáº¥n luyá»‡n).
    - **Dev Loss** tÄƒng ngÆ°á»£c láº¡i lÃªn 18.7 (mÃ´ hÃ¬nh máº¥t kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a).
    - **Káº¿t luáº­n**: Viá»‡c huáº¥n luyá»‡n thÃªm sau epoch 13 khÃ´ng mang láº¡i lá»£i Ã­ch vá» Ä‘á»™ chÃ­nh xÃ¡c vÃ  lÃ m giáº£m tÃ­nh tá»•ng quÃ¡t cá»§a mÃ´ hÃ¬nh. CÆ¡ cháº¿ **Early Stopping** nÃªn Ä‘Æ°á»£c kÃ­ch hoáº¡t táº¡i Ä‘Ã¢y.

#### 3. Vai TrÃ² Cá»§a Bidirectional RNN
- Viá»‡c accuracy Ä‘áº¡t >90% chá»©ng tá» kiáº¿n trÃºc 2 chiá»u (Bidirectional) ráº¥t hiá»‡u quáº£.
- Káº¿t quáº£ 90.35% cho tháº¥y mÃ´ hÃ¬nh thá»±c sá»± há»c Ä‘Æ°á»£c cáº¥u trÃºc ngá»¯ phÃ¡p chá»© khÃ´ng chá»‰ nhá»› váº¹t.

### 3.3 VÃ­ Dá»¥ Dá»± ÄoÃ¡n
CÃ¢u: *"I love NLP ."*
- **Dá»± Ä‘oÃ¡n**: `[('I', 'PRON'), ('love', 'VERB'), ('NLP', 'PROPN'), ('.', 'PUNCT')]`
- **PhÃ¢n tÃ­ch**:
  - "I" -> PRON (Äáº¡i tá»«): ChÃ­nh xÃ¡c.
  - "love" -> VERB (Äá»™ng tá»«): ChÃ­nh xÃ¡c.
  - "NLP" -> PROPN (Danh tá»« riÃªng): ChÃ­nh xÃ¡c.
  - "." -> PUNCT (Dáº¥u cÃ¢u): ChÃ­nh xÃ¡c.

---

## 4. ThÃ¡ch Thá»©c vÃ  Giáº£i PhÃ¡p
### 4.1 ThÃ¡ch Thá»©c
- **Tá»« láº¡ (OOV - Out of Vocabulary)**: CÃ¡c tá»« khÃ´ng xuáº¥t hiá»‡n trong táº­p train sáº½ bá»‹ gÃ¡n lÃ  `<UNK>`, lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c.
- **Tá»« Ä‘a nghÄ©a**: Má»™t tá»« cÃ³ thá»ƒ cÃ³ nhiá»u nhÃ£n tÃ¹y ngá»¯ cáº£nh (vd: "book" cÃ³ thá»ƒ lÃ  NOUN hoáº·c VERB).
- **Overfitting**: Sau khoáº£ng 13 epochs, mÃ´ hÃ¬nh báº¯t Ä‘áº§u há»c thuá»™c lÃ²ng táº­p train (loss train giáº£m sÃ¢u nhÆ°ng loss dev tÄƒng).

### 4.2 Giáº£i PhÃ¡p
- **Early Stopping**: Dá»«ng huáº¥n luyá»‡n khi accuracy trÃªn táº­p dev khÃ´ng cáº£i thiá»‡n sau má»™t sá»‘ epoch nháº¥t Ä‘á»‹nh (trong trÆ°á»ng há»£p nÃ y lÃ  sau epoch 13).
- **Xá»­ lÃ½ OOV**: Sá»­ dá»¥ng token `<UNK>` vÃ  thay tháº¿ cÃ¡c tá»« táº§n suáº¥t tháº¥p báº±ng `<UNK>` khi training Ä‘á»ƒ mÃ´ hÃ¬nh há»c cÃ¡ch xá»­ lÃ½ tá»« láº¡.
- **Ngá»¯ cáº£nh**: Sá»­ dá»¥ng **Bidirectional RNN** Ä‘á»ƒ xem xÃ©t ngá»¯ cáº£nh toÃ n cá»¥c.
- **Padding & Packing**: Sá»­ dá»¥ng `pad_sequence` káº¿t há»£p `pack_padded_sequence` Ä‘á»ƒ xá»­ lÃ½ batch hiá»‡u quáº£ mÃ  khÃ´ng tÃ­nh toÃ¡n trÃªn pháº§n Ä‘á»‡m.

---

## 5. HÆ°á»›ng PhÃ¡t Triá»ƒn
### 5.1 Cáº£i Tiáº¿n MÃ´ HÃ¬nh
- **LSTM/GRU**: Thay tháº¿ RNN thÆ°á»ng báº±ng LSTM hoáº·c GRU Ä‘á»ƒ xá»­ lÃ½ phá»¥ thuá»™c xa tá»‘t hÆ¡n (trÃ¡nh vanishing gradient).
- **CRF (Conditional Random Fields)**: ThÃªm lá»›p CRF lÃªn trÃªn RNN Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a sá»± phá»¥ thuá»™c giá»¯a cÃ¡c nhÃ£n liÃªn tiáº¿p (vd: ADJ thÆ°á»ng Ä‘á»©ng trÆ°á»›c NOUN).
- **Pre-trained Embeddings**: Sá»­ dá»¥ng GloVe hoáº·c Word2Vec thay vÃ¬ há»c embedding tá»« Ä‘áº§u.

### 5.2 Tá»‘i Æ¯u HÃ³a
- **Hyperparameter Tuning**: Thá»­ nghiá»‡m vá»›i learning rate, hidden dim, sá»‘ layers khÃ¡c nhau.
- **Data Augmentation**: TÄƒng cÆ°á»ng dá»¯ liá»‡u Ä‘á»ƒ mÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n.

---
**Káº¿t luáº­n**: MÃ´ hÃ¬nh RNN Ä‘Æ¡n giáº£n Ä‘Ã£ giáº£i quyáº¿t tá»‘t bÃ i toÃ¡n POS Tagging vá»›i Ä‘á»™ chÃ­nh xÃ¡c áº¥n tÆ°á»£ng (~90%). Viá»‡c Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t xá»­ lÃ½ chuá»—i chuáº©n (padding, packing, masking) vÃ  Bidirectional RNN lÃ  chÃ¬a khÃ³a cho hiá»‡u suáº¥t nÃ y.
