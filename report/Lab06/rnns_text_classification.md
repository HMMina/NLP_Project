# BÃ¡o CÃ¡o Lab 06: PhÃ¢n Loáº¡i VÄƒn Báº£n vá»›i RNNs (lab6_rnns_text_classification.ipynb)

## ğŸ“‹ Má»¥c Lá»¥c
1. [Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai](#1-giáº£i-thÃ­ch-cÃ¡c-bÆ°á»›c-triá»ƒn-khai)
2. [HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£](#2-hÆ°á»›ng-dáº«n-thá»±c-thi-mÃ£)
3. [PhÃ¢n TÃ­ch Káº¿t Quáº£](#3-phÃ¢n-tÃ­ch-káº¿t-quáº£)
4. [ThÃ¡ch Thá»©c vÃ  Giáº£i PhÃ¡p](#4-thÃ¡ch-thá»©c-vÃ -giáº£i-phÃ¡p)
5. [HÆ°á»›ng PhÃ¡t Triá»ƒn](#5-hÆ°á»›ng-phÃ¡t-triá»ƒn)

---
## 1. Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai
### Task 0: Data Loading & Label Encoding
- **Má»¥c Ä‘Ã­ch**: Chuáº©n bá»‹ dá»¯ liá»‡u cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Äá»c dá»¯ liá»‡u tá»« cÃ¡c file CSV (train.csv, val.csv, test.csv) trong folder hwu.
  - Sá»­ dá»¥ng `LabelEncoder` Ä‘á»ƒ chuyá»ƒn Ä‘á»•i nhÃ£n "category" tá»« dáº¡ng text sang sá»‘.
  - Kiá»ƒm tra kÃ­ch thÆ°á»›c vÃ  cáº¥u trÃºc dá»¯ liá»‡u.

### Task 1: TF-IDF + Logistic Regression
- **Má»¥c Ä‘Ã­ch**: XÃ¢y dá»±ng baseline model sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Sá»­ dá»¥ng `TfidfVectorizer` Ä‘á»ƒ chuyá»ƒn vÄƒn báº£n thÃ nh vector.
  - Káº¿t há»£p vá»›i `LogisticRegression` trong má»™t pipeline.
  - Huáº¥n luyá»‡n trÃªn táº­p train vÃ  Ä‘Ã¡nh giÃ¡ trÃªn táº­p test.

### Task 2: Word2Vec (Average) + Dense Layer
- **Má»¥c Ä‘Ã­ch**: Sá»­ dá»¥ng word embeddings nhÆ°ng chÆ°a xá»­ lÃ½ Ä‘Æ°á»£c tÃ­nh tuáº§n tá»± cá»§a vÄƒn báº£n.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Huáº¥n luyá»‡n mÃ´ hÃ¬nh Word2Vec trÃªn dá»¯ liá»‡u training.
  - Chuyá»ƒn Ä‘á»•i má»—i cÃ¢u thÃ nh vector trung bÃ¬nh cá»§a cÃ¡c tá»«.
  - XÃ¢y dá»±ng máº¡ng neural vá»›i Dense layers Ä‘á»ƒ phÃ¢n loáº¡i.

### Task 3: Pre-trained Embedding + LSTM
- **Má»¥c Ä‘Ã­ch**: Sá»­ dá»¥ng LSTM vá»›i embedding Ä‘Ã£ Ä‘Æ°á»£c pre-train tá»« Word2Vec.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Tokenize vÄƒn báº£n vÃ  padding sequences.
  - Táº¡o embedding matrix tá»« Word2Vec Ä‘Ã£ huáº¥n luyá»‡n.
  - XÃ¢y dá»±ng mÃ´ hÃ¬nh Sequential: Embedding (frozen) â†’ LSTM â†’ Dense.
  - Sá»­ dá»¥ng EarlyStopping Ä‘á»ƒ trÃ¡nh overfitting.

### Task 4: Scratch Embedding + LSTM
- **Má»¥c Ä‘Ã­ch**: So sÃ¡nh hiá»‡u quáº£ khi embedding layer Ä‘Æ°á»£c há»c tá»« Ä‘áº§u.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Sá»­ dá»¥ng cÃ¹ng architecture nhÆ° Task 3 nhÆ°ng embedding layer trainable.
  - Huáº¥n luyá»‡n end-to-end vá»›i EarlyStopping.

### Task 5: Evaluation & Analysis
- **Má»¥c Ä‘Ã­ch**: So sÃ¡nh Ä‘á»‹nh lÆ°á»£ng vÃ  Ä‘á»‹nh tÃ­nh giá»¯a cÃ¡c mÃ´ hÃ¬nh.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - TÃ­nh macro F1-score vÃ  test loss cho cáº£ 4 mÃ´ hÃ¬nh.
  - PhÃ¢n tÃ­ch qualitative trÃªn cÃ¡c cÃ¢u khÃ³ cÃ³ cáº¥u trÃºc phá»§ Ä‘á»‹nh/phá»©c táº¡p.
  - Táº¡o báº£ng so sÃ¡nh vÃ  nháº­n xÃ©t.

## 2. HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£
### 2.1 YÃªu Cáº§u Há»‡ Thá»‘ng
```bash
pip install pandas numpy scikit-learn gensim tensorflow jupyter
```

### 2.2 Cáº¥u TrÃºc ThÆ° Má»¥c
```
Lab06/
â”œâ”€â”€ lab6_rnns_text_classification.ipynb
â”œâ”€â”€ hwu/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ val.csv
â”‚   â””â”€â”€ test.csv
â””â”€â”€ rnns_text_classification.md
```

### 2.3 CÃ¡ch Cháº¡y
1. **Má»Ÿ Jupyter Notebook**:
   ```bash
   jupyter notebook lab6_rnns_text_classification.ipynb
   ```

2. **Cháº¡y tá»«ng cell theo thá»© tá»±**:
   - Cell 1: Import libraries vÃ  setup random seed.
   - Cell 2: Load dá»¯ liá»‡u tá»« CSV files.
   - Cell 3: Label encoding.
   - Cell 4-5: Task 1 (TF-IDF + LR).
   - Cell 6-7: Task 2 (Word2Vec + Dense).
   - Cell 8-9: Task 3 (Pre-trained Embedding + LSTM).
   - Cell 10-11: Task 4 (Scratch Embedding + LSTM).
   - Cell 12-13: Task 5 (Evaluation & Analysis).

## 3. PhÃ¢n TÃ­ch Káº¿t Quáº£
### 3.1 Káº¿t quáº£ Task 1
```
                accuracy                           0.84      1076
               macro avg       0.85      0.83      0.84      1076
            weighted avg       0.84      0.84      0.84      1076
```
- Accuracy 0.84 : ÄÃ¢y lÃ  má»©c tá»‘t cho baseline TF-IDF + LR, nháº¥t lÃ  náº¿u dá»¯ liá»‡u cÃ³ nhiá»u nhÃ£n hoáº·c cÃ¢u ngáº¯n.
- Macro avg ~ Weighted avg : Cho tháº¥y cÃ¡c lá»›p Ä‘Æ°á»£c cÃ¢n báº±ng tá»‘t, khÃ´ng cÃ³ lá»›p nÃ o bá»‹ bá» quÃªn.

### 3.2 Káº¿t quáº£ Task 2
```
                accuracy                           0.35      1076
               macro avg       0.34      0.33      0.30      1076
            weighted avg       0.35      0.35      0.32      1076
```
- Accuracy 0.35 : Tháº¥p hÆ¡n nhiá»u so vá»›i TF-IDF + LR, cho tháº¥y viá»‡c máº¥t thÃ´ng tin vá» thá»© tá»± tá»« áº£nh hÆ°á»Ÿng lá»›n.
- Táº­p train nhá» cÃ³ thá»ƒ khÃ´ng Ä‘á»§ Ä‘á»ƒ há»c embeddings tá»‘t.
- Vector trung bÃ¬nh lÃ m máº¥t ngá»¯ cáº£nh.
- Máº¡ng nÃ´ng khÃ´ng Ä‘á»§ máº¡nh Ä‘á»ƒ bÃ¹ Ä‘áº¯p.

### 3.3 Káº¿t quáº£ Task 3
```
                accuracy                           0.40      1076
               macro avg       0.39      0.39      0.38      1076
            weighted avg       0.40      0.40      0.39      1076
```
- Accuracy 0.40 : Cáº£i thiá»‡n so vá»›i Task 2, cho tháº¥y LSTM giÃºp náº¯m báº¯t thÃ´ng tin tuáº§n tá»±.
- Pre-trained embeddings giÃºp mÃ´ hÃ¬nh há»c nhanh hÆ¡n vÃ  hiá»‡u quáº£ hÆ¡n.
- Tuy nhiÃªn, váº«n chÆ°a vÆ°á»£t trá»™i so vá»›i TF-IDF + LR cÃ³ thá»ƒ do táº­p dá»¯ liá»‡u nhá» vÃ  cáº¥u trÃºc cÃ¢u Ä‘Æ¡n giáº£n.

### 3.4 Káº¿t quáº£ Task 4
```
                accuracy                           0.27      1076
               macro avg       0.16      0.25      0.18      1076
            weighted avg       0.17      0.27      0.20      1076
```
- Accuracy 0.27 : Tháº¥p hÆ¡n so vá»›i Task 3, cho tháº¥y viá»‡c há»c embedding tá»« Ä‘áº§u gáº·p khÃ³ khÄƒn vá»›i táº­p dá»¯ liá»‡u nhá».
- Sá»­ dá»¥ng early stopping giÃºp trÃ¡nh overfitting nhÆ°ng mÃ´ hÃ¬nh váº«n chÆ°a há»c Ä‘Æ°á»£c biá»ƒu diá»…n tá»‘t.
- Cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n Ä‘á»ƒ embedding layer há»c hiá»‡u quáº£.

### 3.5 Káº¿t quáº£ Task 5
- **Báº£ng Tá»•ng Há»£p Káº¿t Quáº£**
```
| Pipeline | F1-score (Macro) | Test Loss | Nháº­n XÃ©t |
|----------|------------------|-----------|----------|
| TF-IDF + Logistic Regression | 0.835298 | 1.050197 | Hiá»‡u quáº£ báº¥t ngá», Ä‘Æ¡n giáº£n vÃ  nhanh. |
| Word2Vec (Avg) + Dense | 0.304154 | 2.452722 | Máº¥t thÃ´ng tin thá»© tá»±, máº¡ng nÃ´ng. |
| Pre-trained Embedding + LSTM | 0.376478 | 2.108491 | Xá»­ lÃ½ tuáº§n tá»± á»•n, tuy nhiÃªn cáº§n tuning thÃªm. |
| Scratch Embedding + LSTM | 0.178246 | 2.868297 | Flexible nhÆ°ng cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n. |
```

- Nháº­n XÃ©t Chung:
  - TF-IDF + LR váº«n lÃ  lá»±a chá»n tá»‘t cho táº­p dá»¯ liá»‡u nhá» vÃ  cÃ¢u Ä‘Æ¡n giáº£n.
  - MÃ´ hÃ¬nh dá»±a trÃªn embeddings vÃ  LSTM cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n Ä‘á»ƒ phÃ¡t huy hiá»‡u quáº£.
  - Pre-trained embeddings giÃºp cáº£i thiá»‡n so vá»›i há»c tá»« Ä‘áº§u, nhÆ°ng váº«n chÆ°a Ä‘á»§ Ä‘á»ƒ vÆ°á»£t qua phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng.

- **PhÃ¢n TÃ­ch Äá»‹nh TÃ­nh CÃ¡c CÃ¢u KhÃ³**
```
| Sentence | True Intent | TF-IDF + LR | W2V Avg + Dense | Pretrained LSTM | Scratch LSTM |
|-----------|--------------|--------------|------------------|------------------|---------------|
| can you remind me to not call my mom | reminder_create | calendar_set | general_quirky | takeaway_query | email_sendemail |
| is it going to be sunny or rainy tomorrow | weather_query | weather_query | qa_maths | qa_maths | takeaway_order |
| find a flight from new york to london but not ... | flight_search | general_negate | transport_query | email_sendemail | calendar_set |
```

- **CÃ¢u 1:** "can you remind me to not call my mom" vá»›i phá»§ Ä‘á»‹nh "not call"
  - TF-IDF khÃ´ng hiá»ƒu phá»§ Ä‘á»‹nh dá»… nháº§m â€œremindâ€ vá»›i â€œcalendar_setâ€
  - W2V Avg máº¥t ngá»¯ cáº£nh, chá»n intent chung chung.
  - Pre-trained LSTM Náº¯m báº¯t phá»§ Ä‘á»‹nh tá»‘t hÆ¡n, nhÆ°ng váº«n nháº§m vá»›i â€œtakeaway_queryâ€.
  - Scratch LSTM KhÃ´ng há»c Ä‘Æ°á»£c biá»ƒu diá»…n ngá»¯ nghÄ©a, chá»§ yáº¿u dá»±a vÃ o tá»« khÃ³a.

- **CÃ¢u 2:** "is it going to be sunny or rainy tomorrow"
  - TF-IDF + LR Ä‘Ãºng intent nhá» tá»« khÃ³a â€œsunnyâ€, â€œrainyâ€.
  - W2V Avg + Dense khÃ´ng hiá»ƒu ngá»¯ cáº£nh, chá»n intent khÃ´ng liÃªn quan.
  - Pre-trained LSTM vÃ  Scratch LSTM chÆ°a náº¯m báº¯t Ä‘Æ°á»£c Ã½ Ä‘á»‹nh chÃ­nh xÃ¡c, dá»… nháº§m láº«n giá»¯a cÃ¡c lá»›p há»i Ä‘Ã¡p.

- **CÃ¢u 3:** "find a flight from new york to london but not ..."
  - TF-IDF + LR khÃ´ng hiá»ƒu ngá»¯ cáº£nh â€œfind a flightâ€, nháº§m sang intent â€œgeneral_negateâ€.
  - W2V Avg + Dense khÃ´ng náº¯m báº¯t Ä‘Æ°á»£c Ã½ Ä‘á»‹nh chÃ­nh xÃ¡c.
  - Pre-trained LSTM vÃ  Scratch LSTM Ä‘á»u khÃ´ng hiá»ƒu phá»§ Ä‘á»‹nh â€œbut notâ€, dáº«n Ä‘áº¿n nháº§m láº«n.

## 4. ThÃ¡ch Thá»©c vÃ  Giáº£i PhÃ¡p
### 4.1 ThÃ¡ch Thá»©c
- Dá»¯ liá»‡u nhá» háº¡n cháº¿ kháº£ nÄƒng há»c biá»ƒu diá»…n tá»‘t.
- CÃ¡c cÃ¢u phá»©c táº¡p vá»›i phá»§ Ä‘á»‹nh, má»‡nh Ä‘á» Ä‘a nghÄ©a.
- Overfitting do mÃ´ hÃ¬nh phá»©c táº¡p vá»›i dá»¯ liá»‡u háº¡n cháº¿.

### 4.2 Giáº£i PhÃ¡p
- Sá»­ dá»¥ng pre-trained embeddings Ä‘á»ƒ táº­n dá»¥ng kiáº¿n thá»©c ngÃ´n ngá»¯ cÃ³ sáºµn.
- Fine tuning mÃ´ hÃ¬nh ngá»¯ cáº£nh.
- Káº¿t há»£p nhiá»u Ä‘áº·c trÆ°ng (TF-IDF + embeddings) hoáº·c (TF-IDF + LSTM) Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cáº£ hai.
- Data augmentation Ä‘á»ƒ tÄƒng kÃ­ch thÆ°á»›c táº­p huáº¥n luyá»‡n.
- Regularization techniques nhÆ° Dropout, Early Stopping Ä‘á»ƒ giáº£m overfitting.
- Hyperparameter tuning Ä‘á»ƒ tÃ¬m cáº¥u hÃ¬nh tá»‘i Æ°u.


## 5 HÆ°á»›ng PhÃ¡t Triá»ƒn
- Thá»­ nghiá»‡m vá»›i Transformer models (BERT, DistilBERT).
- Ensemble methods káº¿t há»£p multiple approaches.
- Advanced preprocessing (spelling correction, normalization).
- Hyperparameter optimization vá»›i tools nhÆ° Optuna.
---
