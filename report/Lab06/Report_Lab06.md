# BÃ¡o CÃ¡o Tá»•ng Há»£p Lab 06: Sequence Modeling vá»›i RNNs

## ğŸ“‹ Má»¥c Lá»¥c
1. [Part 1: TÃ¬m hiá»ƒu vá» RNNs vÃ  PhÃ¢n loáº¡i Token](#part-1-tÃ¬m-hiá»ƒu-vá»-rnns-vÃ -phÃ¢n-loáº¡i-token-lab6ipynb)
2. [Part 2: RNNs cho PhÃ¢n loáº¡i VÄƒn báº£n](#part-2-rnns-cho-phÃ¢n-loáº¡i-vÄƒn-báº£n-lab6_rnns_text_classificationipynb)
3. [Part 3: Part-of-Speech Tagging vá»›i RNN](#part-3-part-of-speech-tagging-vá»›i-rnn-lab6_rnn_for_pos_taggingipynb)
4. [Part 4: Named Entity Recognition vá»›i RNN](#part-4-named-entity-recognition-vá»›i-rnn-lab6_rnn_for_neripynb)

---

## Part 1: TÃ¬m hiá»ƒu vá» RNNs vÃ  PhÃ¢n loáº¡i Token (lab6.ipynb)
### 1. Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai
#### Task 1: LÃ m quen vá»›i Tensor
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒu cÃ¡ch táº¡o vÃ  thao tÃ¡c vá»›i cáº¥u trÃºc dá»¯ liá»‡u cÆ¡ báº£n cá»§a PyTorch.
- **Thá»±c hiá»‡n**:
  - Táº¡o tensor tá»« list, numpy array.
  - CÃ¡c phÃ©p toÃ¡n cÆ¡ báº£n: cá»™ng, nhÃ¢n vÃ´ hÆ°á»›ng, nhÃ¢n ma tráº­n (`@`).
  - Indexing, Slicing vÃ  Reshaping (`view`).

#### Task 2: CÆ¡ cháº¿ Autograd
- **Má»¥c Ä‘Ã­ch**: Hiá»ƒu cÃ¡ch PyTorch tá»± Ä‘á»™ng tÃ­nh Ä‘áº¡o hÃ m cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n (Backpropagation).
- **Thá»±c hiá»‡n**:
  - Táº¡o tensor vá»›i `requires_grad=True`.
  - Thá»±c hiá»‡n chuá»—i tÃ­nh toÃ¡n vÃ  gá»i `.backward()` Ä‘á»ƒ tÃ­nh gradient.
  - **LÆ°u Ã½**: Äá»“ thá»‹ tÃ­nh toÃ¡n Ä‘Æ°á»£c giáº£i phÃ³ng sau khi gá»i backward, nÃªn khÃ´ng thá»ƒ gá»i láº§n 2 náº¿u khÃ´ng giá»¯ láº¡i graph.

#### Task 3: CÃ¡c lá»›p Neural Network cÆ¡ báº£n
- **nn.Linear**: Lá»›p káº¿t ná»‘i Ä‘áº§y Ä‘á»§ (Fully Connected), thá»±c hiá»‡n phÃ©p biáº¿n Ä‘á»•i tuyáº¿n tÃ­nh $y = xA^T + b$.
- **nn.Embedding**: Lá»›p quan trá»ng nháº¥t cho NLP, chuyá»ƒn Ä‘á»•i index cá»§a tá»« (sá»‘ nguyÃªn) thÃ nh vector dÃ y (dense vector).
- **XÃ¢y dá»±ng nn.Module**: Táº¡o class `MyFirstModel` káº¿t há»£p Embedding -> Linear -> Activation -> Linear.

### 2. HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£
1. **CÃ i Ä‘áº·t**: `pip install torch numpy`
2. **Cháº¡y**: Má»Ÿ `lab6.ipynb` vÃ  cháº¡y tuáº§n tá»± cÃ¡c cell.

### 3. PhÃ¢n TÃ­ch Káº¿t Quáº£
- ÄÃ£ thá»±c hiá»‡n thÃ nh cÃ´ng cÃ¡c thao tÃ¡c tensor vÃ  tÃ­nh toÃ¡n Ä‘áº¡o hÃ m.
- MÃ´ hÃ¬nh `MyFirstModel` cháº¡y thÃ nh cÃ´ng forward pass, chuyá»ƒn Ä‘á»•i input index thÃ nh output vector.
- ÄÃ¢y lÃ  bÆ°á»›c Ä‘á»‡m quan trá»ng Ä‘á»ƒ hiá»ƒu cÃ¡ch dá»¯ liá»‡u (cÃ¢u chá»¯) Ä‘Æ°á»£c Ä‘Æ°a vÃ o mÃ´ hÃ¬nh Deep Learning.

### 4. KhÃ³ KhÄƒn vÃ  Giáº£i PhÃ¡p
- **KhÃ¡i niá»‡m Autograd**: Ban Ä‘áº§u khÃ³ hiá»ƒu vá» viá»‡c táº¡i sao khÃ´ng thá»ƒ gá»i backward nhiá»u láº§n. -> **Giáº£i phÃ¡p**: Äá»c tÃ i liá»‡u vá» Dynamic Computation Graph cá»§a PyTorch.
- **Shape cá»§a Tensor**: Dá»… nháº§m láº«n khi nhÃ¢n ma tráº­n. -> **Giáº£i phÃ¡p**: LuÃ´n in `.shape` Ä‘á»ƒ kiá»ƒm tra.

---

## Part 2: RNNs cho PhÃ¢n loáº¡i VÄƒn báº£n (lab6_rnns_text_classification.ipynb)

### 1. Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai
#### Task 0: Data Loading & Label Encoding
- **Má»¥c Ä‘Ã­ch**: Chuáº©n bá»‹ dá»¯ liá»‡u cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Äá»c dá»¯ liá»‡u tá»« cÃ¡c file CSV (train.csv, val.csv, test.csv) trong folder hwu.
  - Sá»­ dá»¥ng `LabelEncoder` Ä‘á»ƒ chuyá»ƒn Ä‘á»•i nhÃ£n "category" tá»« dáº¡ng text sang sá»‘.
  - Kiá»ƒm tra kÃ­ch thÆ°á»›c vÃ  cáº¥u trÃºc dá»¯ liá»‡u.

#### Task 1: TF-IDF + Logistic Regression
- **Má»¥c Ä‘Ã­ch**: XÃ¢y dá»±ng baseline model sá»­ dá»¥ng phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Sá»­ dá»¥ng `TfidfVectorizer` Ä‘á»ƒ chuyá»ƒn vÄƒn báº£n thÃ nh vector.
  - Káº¿t há»£p vá»›i `LogisticRegression` trong má»™t pipeline.
  - Huáº¥n luyá»‡n trÃªn táº­p train vÃ  Ä‘Ã¡nh giÃ¡ trÃªn táº­p test.

#### Task 2: Word2Vec (Average) + Dense Layer
- **Má»¥c Ä‘Ã­ch**: Sá»­ dá»¥ng word embeddings nhÆ°ng chÆ°a xá»­ lÃ½ Ä‘Æ°á»£c tÃ­nh tuáº§n tá»± cá»§a vÄƒn báº£n.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Huáº¥n luyá»‡n mÃ´ hÃ¬nh Word2Vec trÃªn dá»¯ liá»‡u training.
  - Chuyá»ƒn Ä‘á»•i má»—i cÃ¢u thÃ nh vector trung bÃ¬nh cá»§a cÃ¡c tá»«.
  - XÃ¢y dá»±ng máº¡ng neural vá»›i Dense layers Ä‘á»ƒ phÃ¢n loáº¡i.

#### Task 3: Pre-trained Embedding + LSTM
- **Má»¥c Ä‘Ã­ch**: Sá»­ dá»¥ng LSTM vá»›i embedding Ä‘Ã£ Ä‘Æ°á»£c pre-train tá»« Word2Vec.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Tokenize vÄƒn báº£n vÃ  padding sequences.
  - Táº¡o embedding matrix tá»« Word2Vec Ä‘Ã£ huáº¥n luyá»‡n.
  - XÃ¢y dá»±ng mÃ´ hÃ¬nh Sequential: Embedding (frozen) â†’ LSTM â†’ Dense.
  - Sá»­ dá»¥ng EarlyStopping Ä‘á»ƒ trÃ¡nh overfitting.

#### Task 4: Scratch Embedding + LSTM
- **Má»¥c Ä‘Ã­ch**: So sÃ¡nh hiá»‡u quáº£ khi embedding layer Ä‘Æ°á»£c há»c tá»« Ä‘áº§u.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Sá»­ dá»¥ng cÃ¹ng architecture nhÆ° Task 3 nhÆ°ng embedding layer trainable.
  - Huáº¥n luyá»‡n end-to-end vá»›i EarlyStopping.

#### Task 5: Evaluation & Analysis
- **Má»¥c Ä‘Ã­ch**: So sÃ¡nh Ä‘á»‹nh lÆ°á»£ng vÃ  Ä‘á»‹nh tÃ­nh giá»¯a cÃ¡c mÃ´ hÃ¬nh.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - TÃ­nh macro F1-score vÃ  test loss cho cáº£ 4 mÃ´ hÃ¬nh.
  - PhÃ¢n tÃ­ch qualitative trÃªn cÃ¡c cÃ¢u khÃ³ cÃ³ cáº¥u trÃºc phá»§ Ä‘á»‹nh/phá»©c táº¡p.
  - Táº¡o báº£ng so sÃ¡nh vÃ  nháº­n xÃ©t.

### 2. HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£
#### 2.1 YÃªu Cáº§u Há»‡ Thá»‘ng
```bash
pip install pandas numpy scikit-learn gensim tensorflow jupyter
```

#### 2.2 Cáº¥u TrÃºc ThÆ° Má»¥c
```
Lab06/
â”œâ”€â”€ lab6_rnns_text_classification.ipynb
â”œâ”€â”€ hwu/
â”‚   â”œâ”€â”€ train.csv
â”‚   â”œâ”€â”€ val.csv
â”‚   â””â”€â”€ test.csv
â””â”€â”€ rnns_text_classification.md
```

#### 2.3 CÃ¡ch Cháº¡y
1. **Má»Ÿ Jupyter Notebook**:
   ```bash
   jupyter notebook lab6_rnns_text_classification.ipynb
   ```

2. **Cháº¡y tá»«ng cell theo thá»© tá»±**:
   - Cell 1: Import libraries vÃ  setup random seed.
   - Cell 2: Load dá»¯ liá»‡u tá»« CSV files.
   - Cell 3: Label encoding.
   - Cell 4-5: Task 1 (TF-IDF + LR).
   - Cell 6-7: Task 2 (Word2Vec + Dense).
   - Cell 8-9: Task 3 (Pre-trained Embedding + LSTM).
   - Cell 10-11: Task 4 (Scratch Embedding + LSTM).
   - Cell 12-13: Task 5 (Evaluation & Analysis).

### 3. PhÃ¢n TÃ­ch Káº¿t Quáº£
#### 3.1 Káº¿t quáº£ Task 1
```
                accuracy                           0.84      1076
               macro avg       0.85      0.83      0.84      1076
            weighted avg       0.84      0.84      0.84      1076
```
- Accuracy 0.84 : ÄÃ¢y lÃ  má»©c tá»‘t cho baseline TF-IDF + LR, nháº¥t lÃ  náº¿u dá»¯ liá»‡u cÃ³ nhiá»u nhÃ£n hoáº·c cÃ¢u ngáº¯n.
- Macro avg ~ Weighted avg : Cho tháº¥y cÃ¡c lá»›p Ä‘Æ°á»£c cÃ¢n báº±ng tá»‘t, khÃ´ng cÃ³ lá»›p nÃ o bá»‹ bá» quÃªn.

#### 3.2 Káº¿t quáº£ Task 2
```
                accuracy                           0.35      1076
               macro avg       0.34      0.33      0.30      1076
            weighted avg       0.35      0.35      0.32      1076
```
- Accuracy 0.35 : Tháº¥p hÆ¡n nhiá»u so vá»›i TF-IDF + LR, cho tháº¥y viá»‡c máº¥t thÃ´ng tin vá» thá»© tá»± tá»« áº£nh hÆ°á»Ÿng lá»›n.
- Táº­p train nhá» cÃ³ thá»ƒ khÃ´ng Ä‘á»§ Ä‘á»ƒ há»c embeddings tá»‘t.
- Vector trung bÃ¬nh lÃ m máº¥t ngá»¯ cáº£nh.
- Máº¡ng nÃ´ng khÃ´ng Ä‘á»§ máº¡nh Ä‘á»ƒ bÃ¹ Ä‘áº¯p.

#### 3.3 Káº¿t quáº£ Task 3
```
                accuracy                           0.40      1076
               macro avg       0.39      0.39      0.38      1076
            weighted avg       0.40      0.40      0.39      1076
```
- Accuracy 0.40 : Cáº£i thiá»‡n so vá»›i Task 2, cho tháº¥y LSTM giÃºp náº¯m báº¯t thÃ´ng tin tuáº§n tá»±.
- Pre-trained embeddings giÃºp mÃ´ hÃ¬nh há»c nhanh hÆ¡n vÃ  hiá»‡u quáº£ hÆ¡n.
- Tuy nhiÃªn, váº«n chÆ°a vÆ°á»£t trá»™i so vá»›i TF-IDF + LR cÃ³ thá»ƒ do táº­p dá»¯ liá»‡u nhá» vÃ  cáº¥u trÃºc cÃ¢u Ä‘Æ¡n giáº£n.

#### 3.4 Káº¿t quáº£ Task 4
```
                accuracy                           0.27      1076
               macro avg       0.16      0.25      0.18      1076
            weighted avg       0.17      0.27      0.20      1076
```
- Accuracy 0.27 : Tháº¥p hÆ¡n so vá»›i Task 3, cho tháº¥y viá»‡c há»c embedding tá»« Ä‘áº§u gáº·p khÃ³ khÄƒn vá»›i táº­p dá»¯ liá»‡u nhá».
- Sá»­ dá»¥ng early stopping giÃºp trÃ¡nh overfitting nhÆ°ng mÃ´ hÃ¬nh váº«n chÆ°a há»c Ä‘Æ°á»£c biá»ƒu diá»…n tá»‘t.
- Cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n Ä‘á»ƒ embedding layer há»c hiá»‡u quáº£.

#### 3.5 Káº¿t quáº£ Task 5
- **Báº£ng Tá»•ng Há»£p Káº¿t Quáº£**
```
| Pipeline | F1-score (Macro) | Test Loss | Nháº­n XÃ©t |
|----------|------------------|-----------|----------|
| TF-IDF + Logistic Regression | 0.835298 | 1.050197 | Hiá»‡u quáº£ báº¥t ngá», Ä‘Æ¡n giáº£n vÃ  nhanh. |
| Word2Vec (Avg) + Dense | 0.304154 | 2.452722 | Máº¥t thÃ´ng tin thá»© tá»±, máº¡ng nÃ´ng. |
| Pre-trained Embedding + LSTM | 0.376478 | 2.108491 | Xá»­ lÃ½ tuáº§n tá»± á»•n, tuy nhiÃªn cáº§n tuning thÃªm. |
| Scratch Embedding + LSTM | 0.178246 | 2.868297 | Flexible nhÆ°ng cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n. |
```

- Nháº­n XÃ©t Chung:
  - TF-IDF + LR váº«n lÃ  lá»±a chá»n tá»‘t cho táº­p dá»¯ liá»‡u nhá» vÃ  cÃ¢u Ä‘Æ¡n giáº£n.
  - MÃ´ hÃ¬nh dá»±a trÃªn embeddings vÃ  LSTM cáº§n nhiá»u dá»¯ liá»‡u hÆ¡n Ä‘á»ƒ phÃ¡t huy hiá»‡u quáº£.
  - Pre-trained embeddings giÃºp cáº£i thiá»‡n so vá»›i há»c tá»« Ä‘áº§u, nhÆ°ng váº«n chÆ°a Ä‘á»§ Ä‘á»ƒ vÆ°á»£t qua phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng.

- **PhÃ¢n TÃ­ch Äá»‹nh TÃ­nh CÃ¡c CÃ¢u KhÃ³**
```
| Sentence | True Intent | TF-IDF + LR | W2V Avg + Dense | Pretrained LSTM | Scratch LSTM |
|-----------|--------------|--------------|------------------|------------------|---------------|
| can you remind me to not call my mom | reminder_create | calendar_set | general_quirky | takeaway_query | email_sendemail |
| is it going to be sunny or rainy tomorrow | weather_query | weather_query | qa_maths | qa_maths | takeaway_order |
| find a flight from new york to london but not ... | flight_search | general_negate | transport_query | email_sendemail | calendar_set |
```

- **CÃ¢u 1:** "can you remind me to not call my mom" vá»›i phá»§ Ä‘á»‹nh "not call"
  - TF-IDF khÃ´ng hiá»ƒu phá»§ Ä‘á»‹nh dá»… nháº§m â€œremindâ€ vá»›i â€œcalendar_setâ€
  - W2V Avg máº¥t ngá»¯ cáº£nh, chá»n intent chung chung.
  - Pre-trained LSTM Náº¯m báº¯t phá»§ Ä‘á»‹nh tá»‘t hÆ¡n, nhÆ°ng váº«n nháº§m vá»›i â€œtakeaway_queryâ€.
  - Scratch LSTM KhÃ´ng há»c Ä‘Æ°á»£c biá»ƒu diá»…n ngá»¯ nghÄ©a, chá»§ yáº¿u dá»±a vÃ o tá»« khÃ³a.

- **CÃ¢u 2:** "is it going to be sunny or rainy tomorrow"
  - TF-IDF + LR Ä‘Ãºng intent nhá» tá»« khÃ³a â€œsunnyâ€, â€œrainyâ€.
  - W2V Avg + Dense khÃ´ng hiá»ƒu ngá»¯ cáº£nh, chá»n intent khÃ´ng liÃªn quan.
  - Pre-trained LSTM vÃ  Scratch LSTM chÆ°a náº¯m báº¯t Ä‘Æ°á»£c Ã½ Ä‘á»‹nh chÃ­nh xÃ¡c, dá»… nháº§m láº«n giá»¯a cÃ¡c lá»›p há»i Ä‘Ã¡p.

- **CÃ¢u 3:** "find a flight from new york to london but not ..."
  - TF-IDF + LR khÃ´ng hiá»ƒu ngá»¯ cáº£nh â€œfind a flightâ€, nháº§m sang intent â€œgeneral_negateâ€.
  - W2V Avg + Dense khÃ´ng náº¯m báº¯t Ä‘Æ°á»£c Ã½ Ä‘á»‹nh chÃ­nh xÃ¡c.
  - Pre-trained LSTM vÃ  Scratch LSTM Ä‘á»u khÃ´ng hiá»ƒu phá»§ Ä‘á»‹nh â€œbut notâ€, dáº«n Ä‘áº¿n nháº§m láº«n.

### 4. ThÃ¡ch Thá»©c vÃ  Giáº£i PhÃ¡p
#### 4.1 ThÃ¡ch Thá»©c
- Dá»¯ liá»‡u nhá» háº¡n cháº¿ kháº£ nÄƒng há»c biá»ƒu diá»…n tá»‘t.
- CÃ¡c cÃ¢u phá»©c táº¡p vá»›i phá»§ Ä‘á»‹nh, má»‡nh Ä‘á» Ä‘a nghÄ©a.
- Overfitting do mÃ´ hÃ¬nh phá»©c táº¡p vá»›i dá»¯ liá»‡u háº¡n cháº¿.

#### 4.2 Giáº£i PhÃ¡p
- Sá»­ dá»¥ng pre-trained embeddings Ä‘á»ƒ táº­n dá»¥ng kiáº¿n thá»©c ngÃ´n ngá»¯ cÃ³ sáºµn.
- Fine tuning mÃ´ hÃ¬nh ngá»¯ cáº£nh.
- Káº¿t há»£p nhiá»u Ä‘áº·c trÆ°ng (TF-IDF + embeddings) hoáº·c (TF-IDF + LSTM) Ä‘á»ƒ táº­n dá»¥ng Æ°u Ä‘iá»ƒm cá»§a cáº£ hai.
- Data augmentation Ä‘á»ƒ tÄƒng kÃ­ch thÆ°á»›c táº­p huáº¥n luyá»‡n.
- Regularization techniques nhÆ° Dropout, Early Stopping Ä‘á»ƒ giáº£m overfitting.
- Hyperparameter tuning Ä‘á»ƒ tÃ¬m cáº¥u hÃ¬nh tá»‘i Æ°u.


### 5 HÆ°á»›ng PhÃ¡t Triá»ƒn
- Thá»­ nghiá»‡m vá»›i Transformer models (BERT, DistilBERT).
- Ensemble methods káº¿t há»£p multiple approaches.
- Advanced preprocessing (spelling correction, normalization).
- Hyperparameter optimization vá»›i tools nhÆ° Optuna.

---

## Part 3: Part-of-Speech Tagging vá»›i RNN (lab6_rnn_for_pos_tagging.ipynb)

### 1. Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai
#### Task 1: Táº£i vÃ  Tiá»n xá»­ lÃ½ Dá»¯ liá»‡u
- **Má»¥c Ä‘Ã­ch**: Äá»c dá»¯ liá»‡u tá»« Ä‘á»‹nh dáº¡ng CoNLL-U vÃ  xÃ¢y dá»±ng tá»« Ä‘iá»ƒn.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Äá»c file `.conllu` (train/dev) Ä‘á»ƒ láº¥y danh sÃ¡ch cÃ¡c cÃ¢u, má»—i cÃ¢u gá»“m cÃ¡c cáº·p `(word, upos)`.
  - XÃ¢y dá»±ng `word_to_ix` (Ã¡nh xáº¡ tá»« -> index) vÃ  `tag_to_ix` (Ã¡nh xáº¡ nhÃ£n -> index).
  - ThÃªm cÃ¡c token Ä‘áº·c biá»‡t: `<PAD>` (Ä‘á»‡m), `<UNK>` (tá»« láº¡).
  - **Káº¿t quáº£**:
    - Train sentences: 12,544 cÃ¢u.
    - Dev sentences: 2,001 cÃ¢u.
    - Vocab size: 6,733 tá»« (min_freq=3).
    - Tag size: 18 nhÃ£n (bao gá»“m `<PAD>`).

#### Task 2: Táº¡o PyTorch Dataset vÃ  DataLoader
- **Má»¥c Ä‘Ã­ch**: Chuáº©n bá»‹ dá»¯ liá»‡u dáº¡ng batch Ä‘á»ƒ huáº¥n luyá»‡n mÃ´ hÃ¬nh.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Táº¡o class `POSDataset` káº¿ thá»«a tá»« `torch.utils.data.Dataset`.
  - Viáº¿t hÃ m `collate_fn` sá»­ dá»¥ng `pad_sequence` Ä‘á»ƒ Ä‘á»‡m cÃ¡c cÃ¢u trong batch vá» cÃ¹ng Ä‘á»™ dÃ i.
  - Táº¡o `DataLoader` cho táº­p train (shuffle=True) vÃ  dev (shuffle=False) vá»›i `batch_size=64`.

#### Task 3: XÃ¢y dá»±ng MÃ´ hÃ¬nh RNN
- **Má»¥c Ä‘Ã­ch**: XÃ¢y dá»±ng mÃ´ hÃ¬nh sequence labeling sá»­ dá»¥ng RNN.
- **Kiáº¿n trÃºc mÃ´ hÃ¬nh**:
  1. **Embedding Layer**: Chuyá»ƒn Ä‘á»•i index cá»§a tá»« thÃ nh vector (dim=100).
  2. **RNN Layer**: Bidirectional RNN (hidden_dim=128) Ä‘á»ƒ náº¯m báº¯t ngá»¯ cáº£nh hai chiá»u.
  3. **Dropout**: Giáº£m overfitting (p=0.1).
  4. **Linear Layer**: Ãnh xáº¡ output cá»§a RNN sang khÃ´ng gian nhÃ£n (output_dim=18).
- **Ká»¹ thuáº­t Ä‘áº·c biá»‡t**: Sá»­ dá»¥ng `pack_padded_sequence` vÃ  `pad_packed_sequence` Ä‘á»ƒ RNN bá» qua cÃ¡c token padding, tÄƒng hiá»‡u quáº£ tÃ­nh toÃ¡n.

#### Task 4: Huáº¥n luyá»‡n MÃ´ hÃ¬nh
- **Cáº¥u hÃ¬nh**:
  - Optimizer: Adam (lr=1e-3).
  - Loss function: CrossEntropyLoss (ignore_index=PAD_TAG).
  - Epochs: 20.
- **Quy trÃ¬nh**:
  - Forward pass -> TÃ­nh loss -> Backward pass -> Update weights.
  - Theo dÃµi loss vÃ  accuracy trÃªn táº­p dev sau má»—i epoch.
  - LÆ°u láº¡i tráº¡ng thÃ¡i mÃ´ hÃ¬nh tá»‘t nháº¥t (best dev accuracy).

#### Task 5: ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh
- **Má»¥c Ä‘Ã­ch**: Kiá»ƒm tra hiá»‡u suáº¥t mÃ´ hÃ¬nh trÃªn táº­p dev vÃ  dá»± Ä‘oÃ¡n cÃ¢u má»›i.
- **PhÆ°Æ¡ng phÃ¡p**:
  - TÃ­nh accuracy trÃªn cÃ¡c token thá»±c (bá» qua padding).
  - Viáº¿t hÃ m `predict_sentence` Ä‘á»ƒ gÃ¡n nhÃ£n cho cÃ¢u nháº­p vÃ o báº¥t ká»³.

### 2. HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£
#### 2.1 YÃªu Cáº§u Há»‡ Thá»‘ng
```bash
pip install torch numpy
```

#### 2.2 Cáº¥u TrÃºc ThÆ° Má»¥c
```
Lab06/
â”œâ”€â”€ lab6_rnn_for_pos_tagging.ipynb
â”œâ”€â”€ rnn_for_pos_tagging.md
â””â”€â”€ ../UD_English-EWT/
    â”œâ”€â”€ en_ewt-ud-train.conllu
    â””â”€â”€ en_ewt-ud-dev.conllu
```

#### 2.3 CÃ¡ch Cháº¡y
1. **Má»Ÿ Jupyter Notebook**:
   ```bash
   jupyter notebook lab6_rnn_for_pos_tagging.ipynb
   ```
2. **Cháº¡y tuáº§n tá»± cÃ¡c cell**:
   - Cell 1-2: Import vÃ  setup.
   - Cell 3-5: Task 1 (Data Loading & Vocab).
   - Cell 6-8: Task 2 (Dataset & DataLoader).
   - Cell 9-10: Task 3 (Model Definition).
   - Cell 11-13: Task 4 (Training).
   - Cell 14-15: Task 5 (Evaluation & Demo).

### 3. PhÃ¢n TÃ­ch Káº¿t Quáº£

#### 3.1 QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n Chi Tiáº¿t
DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ chi tiáº¿t cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n qua 20 epochs:

| Epoch | Train Loss | Dev Loss | Dev Accuracy | Nháº­n XÃ©t |
|:-----:|:----------:|:--------:|:------------:|:---------|
| 1     | 67.9458    | 33.3044  | 0.7603       | Khá»Ÿi Ä‘áº§u tá»‘t, mÃ´ hÃ¬nh há»c nhanh cÃ¡c quy luáº­t cÆ¡ báº£n. |
| 2     | 36.7162    | 25.0620  | 0.8155       | Loss giáº£m máº¡nh (~50%), accuracy tÄƒng >5%. |
| 3     | 27.3685    | 20.9647  | 0.8466       | |
| 4     | 21.4835    | 18.4810  | 0.8632       | |
| 5     | 17.7275    | 16.6868  | 0.8767       | Káº¿t thÃºc giai Ä‘oáº¡n há»c nhanh. |
| 6     | 15.0122    | 15.5063  | 0.8876       | |
| 7     | 12.7403    | 14.9340  | 0.8937       | Dev loss báº¯t Ä‘áº§u á»•n Ä‘á»‹nh quanh má»©c 14-15. |
| 8     | 11.3818    | 14.6995  | 0.8957       | |
| 9     | 10.1980    | 14.7442  | 0.8952       | |
| 10    | 9.1291     | 15.0488  | 0.8945       | |
| 11    | 8.2330     | 14.5638  | 0.8999       | Tiá»‡m cáº­n má»©c 90%. |
| 12    | 7.4461     | 14.9955  | 0.8995       | |
| 13    | 6.8169     | 14.9773  | 0.9035       | Äáº¡t Ä‘á»™ chÃ­nh xÃ¡c cao nháº¥t (Best Model). |
| 14    | 6.2771     | 15.7560  | 0.8972       | Dev loss báº¯t Ä‘áº§u tÄƒng -> Dáº¥u hiá»‡u Overfitting. |
| 15    | 5.5244     | 16.7355  | 0.8951       | |
| 16    | 5.0551     | 16.2119  | 0.9021       | |
| 17    | 4.5238     | 16.8760  | 0.9006       | |
| 18    | 3.9932     | 17.4442  | 0.9014       | |
| 19    | 3.6125     | 18.6773  | 0.8960       | |
| 20    | 3.2173     | 18.6980  | 0.9000       | Train loss ráº¥t tháº¥p nhÆ°ng Dev loss cao nháº¥t. |

#### 3.2 Nháº­n XÃ©t

##### 1. Hiá»‡u Suáº¥t Tá»•ng Thá»ƒ
- **Äá»‰nh cao (Peak Performance)**: MÃ´ hÃ¬nh Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c tá»‘t nháº¥t lÃ  **90.35%** táº¡i Epoch 13. ÄÃ¢y lÃ  káº¿t quáº£ ráº¥t kháº£ quan cho má»™t mÃ´ hÃ¬nh RNN Ä‘Æ¡n giáº£n (khÃ´ng dÃ¹ng pre-trained embeddings hay kiáº¿n trÃºc phá»©c táº¡p nhÆ° Transformer).
- **Tá»‘c Ä‘á»™ há»™i tá»¥**: MÃ´ hÃ¬nh há»™i tá»¥ khÃ¡ nhanh. Chá»‰ sau 5 epochs Ä‘áº§u tiÃªn, Ä‘á»™ chÃ­nh xÃ¡c Ä‘Ã£ Ä‘áº¡t 87.67%. CÃ¡c epochs sau Ä‘Ã³ chá»§ yáº¿u tinh chá»‰nh cÃ¡c trÆ°á»ng há»£p khÃ³ (nhÆ° tá»« Ä‘a nghÄ©a, tá»« hiáº¿m).

##### 2. PhÃ¢n TÃ­ch Overfitting
- **Giai Ä‘oáº¡n 1 (Epoch 1-11)**: Cáº£ Train Loss vÃ  Dev Loss Ä‘á»u giáº£m. ÄÃ¢y lÃ  giai Ä‘oáº¡n "Learning" hiá»‡u quáº£ nháº¥t.
- **Giai Ä‘oáº¡n 2 (Epoch 12-13)**: Dev Loss Ä‘i ngang (khoáº£ng 14.9), trong khi Train Loss tiáº¿p tá»¥c giáº£m. ÄÃ¢y lÃ  Ä‘iá»ƒm tá»‘i Æ°u ("Sweet Spot").
- **Giai Ä‘oáº¡n 3 (Epoch 14-20)**:
    - **Train Loss** giáº£m sÃ¢u xuá»‘ng 3.2 (mÃ´ hÃ¬nh há»c thuá»™c lÃ²ng dá»¯ liá»‡u huáº¥n luyá»‡n).
    - **Dev Loss** tÄƒng ngÆ°á»£c láº¡i lÃªn 18.7 (mÃ´ hÃ¬nh máº¥t kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a).
    - **Káº¿t luáº­n**: Viá»‡c huáº¥n luyá»‡n thÃªm sau epoch 13 khÃ´ng mang láº¡i lá»£i Ã­ch vá» Ä‘á»™ chÃ­nh xÃ¡c vÃ  lÃ m giáº£m tÃ­nh tá»•ng quÃ¡t cá»§a mÃ´ hÃ¬nh. CÆ¡ cháº¿ **Early Stopping** nÃªn Ä‘Æ°á»£c kÃ­ch hoáº¡t táº¡i Ä‘Ã¢y.

##### 3. Vai TrÃ² Cá»§a Bidirectional RNN
- Viá»‡c accuracy Ä‘áº¡t >90% chá»©ng tá» kiáº¿n trÃºc 2 chiá»u (Bidirectional) ráº¥t hiá»‡u quáº£.
- Káº¿t quáº£ 90.35% cho tháº¥y mÃ´ hÃ¬nh thá»±c sá»± há»c Ä‘Æ°á»£c cáº¥u trÃºc ngá»¯ phÃ¡p chá»© khÃ´ng chá»‰ nhá»› váº¹t.

#### 3.3 VÃ­ Dá»¥ Dá»± ÄoÃ¡n
CÃ¢u: *"I love NLP ."*
- **Dá»± Ä‘oÃ¡n**: `[('I', 'PRON'), ('love', 'VERB'), ('NLP', 'PROPN'), ('.', 'PUNCT')]`
- **PhÃ¢n tÃ­ch**:
  - "I" -> PRON (Äáº¡i tá»«): ChÃ­nh xÃ¡c.
  - "love" -> VERB (Äá»™ng tá»«): ChÃ­nh xÃ¡c.
  - "NLP" -> PROPN (Danh tá»« riÃªng): ChÃ­nh xÃ¡c.
  - "." -> PUNCT (Dáº¥u cÃ¢u): ChÃ­nh xÃ¡c.

### 4. ThÃ¡ch Thá»©c vÃ  Giáº£i PhÃ¡p
#### 4.1 ThÃ¡ch Thá»©c
- **Tá»« láº¡ (OOV - Out of Vocabulary)**: CÃ¡c tá»« khÃ´ng xuáº¥t hiá»‡n trong táº­p train sáº½ bá»‹ gÃ¡n lÃ  `<UNK>`, lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c.
- **Tá»« Ä‘a nghÄ©a**: Má»™t tá»« cÃ³ thá»ƒ cÃ³ nhiá»u nhÃ£n tÃ¹y ngá»¯ cáº£nh (vd: "book" cÃ³ thá»ƒ lÃ  NOUN hoáº·c VERB).
- **Overfitting**: Sau khoáº£ng 13 epochs, mÃ´ hÃ¬nh báº¯t Ä‘áº§u há»c thuá»™c lÃ²ng táº­p train (loss train giáº£m sÃ¢u nhÆ°ng loss dev tÄƒng).

#### 4.2 Giáº£i PhÃ¡p
- **Early Stopping**: Dá»«ng huáº¥n luyá»‡n khi accuracy trÃªn táº­p dev khÃ´ng cáº£i thiá»‡n sau má»™t sá»‘ epoch nháº¥t Ä‘á»‹nh (trong trÆ°á»ng há»£p nÃ y lÃ  sau epoch 13).
- **Xá»­ lÃ½ OOV**: Sá»­ dá»¥ng token `<UNK>` vÃ  thay tháº¿ cÃ¡c tá»« táº§n suáº¥t tháº¥p báº±ng `<UNK>` khi training Ä‘á»ƒ mÃ´ hÃ¬nh há»c cÃ¡ch xá»­ lÃ½ tá»« láº¡.
- **Ngá»¯ cáº£nh**: Sá»­ dá»¥ng **Bidirectional RNN** Ä‘á»ƒ xem xÃ©t ngá»¯ cáº£nh toÃ n cá»¥c.
- **Padding & Packing**: Sá»­ dá»¥ng `pad_sequence` káº¿t há»£p `pack_padded_sequence` Ä‘á»ƒ xá»­ lÃ½ batch hiá»‡u quáº£ mÃ  khÃ´ng tÃ­nh toÃ¡n trÃªn pháº§n Ä‘á»‡m.

### 5. HÆ°á»›ng PhÃ¡t Triá»ƒn
#### 5.1 Cáº£i Tiáº¿n MÃ´ HÃ¬nh
- **LSTM/GRU**: Thay tháº¿ RNN thÆ°á»ng báº±ng LSTM hoáº·c GRU Ä‘á»ƒ xá»­ lÃ½ phá»¥ thuá»™c xa tá»‘t hÆ¡n (trÃ¡nh vanishing gradient).
- **CRF (Conditional Random Fields)**: ThÃªm lá»›p CRF lÃªn trÃªn RNN Ä‘á»ƒ mÃ´ hÃ¬nh hÃ³a sá»± phá»¥ thuá»™c giá»¯a cÃ¡c nhÃ£n liÃªn tiáº¿p (vd: ADJ thÆ°á»ng Ä‘á»©ng trÆ°á»›c NOUN).
- **Pre-trained Embeddings**: Sá»­ dá»¥ng GloVe hoáº·c Word2Vec thay vÃ¬ há»c embedding tá»« Ä‘áº§u.

#### 5.2 Tá»‘i Æ¯u HÃ³a
- **Hyperparameter Tuning**: Thá»­ nghiá»‡m vá»›i learning rate, hidden dim, sá»‘ layers khÃ¡c nhau.
- **Data Augmentation**: TÄƒng cÆ°á»ng dá»¯ liá»‡u Ä‘á»ƒ mÃ´ hÃ¬nh tá»•ng quÃ¡t hÃ³a tá»‘t hÆ¡n.

---

## Part 4: Named Entity Recognition vá»›i RNN (lab6_rnn_for_ner.ipynb)

### 1. Giáº£i ThÃ­ch CÃ¡c BÆ°á»›c Triá»ƒn Khai

#### Task 1: Táº£i vÃ  Tiá»n xá»­ lÃ½ Dá»¯ liá»‡u
- **Má»¥c Ä‘Ã­ch**: Táº£i bá»™ dá»¯ liá»‡u chuáº©n CoNLL-2003 vÃ  chuáº©n bá»‹ tá»« Ä‘iá»ƒn.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Sá»­ dá»¥ng thÆ° viá»‡n `datasets` cá»§a Hugging Face Ä‘á»ƒ táº£i `conll2003`.
  - TrÃ­ch xuáº¥t cÃ¢u (tokens) vÃ  nhÃ£n (ner_tags).
  - Chuyá»ƒn Ä‘á»•i nhÃ£n tá»« dáº¡ng sá»‘ sang dáº¡ng chuá»—i (vÃ­ dá»¥: `0` -> `O`, `1` -> `B-PER`) Ä‘á»ƒ dá»… kiá»ƒm soÃ¡t.
  - XÃ¢y dá»±ng `word_to_ix` (Ã¡nh xáº¡ tá»« -> index) vÃ  `tag_to_ix` (Ã¡nh xáº¡ nhÃ£n -> index).
  - ThÃªm token Ä‘áº·c biá»‡t: `<PAD>` (Ä‘á»‡m), `<UNK>` (tá»« láº¡).

#### Task 2: Táº¡o PyTorch Dataset vÃ  DataLoader
- **Má»¥c Ä‘Ã­ch**: ÄÃ³ng gÃ³i dá»¯ liá»‡u Ä‘á»ƒ huáº¥n luyá»‡n theo batch.
- **BÆ°á»›c thá»±c hiá»‡n**:
  - Táº¡o class `NERDataset` káº¿ thá»«a `torch.utils.data.Dataset`.
  - Viáº¿t hÃ m `collate_fn` sá»­ dá»¥ng `pad_sequence` Ä‘á»ƒ Ä‘á»‡m cÃ¡c cÃ¢u vÃ  chuá»—i nhÃ£n vá» cÃ¹ng Ä‘á»™ dÃ i trong má»™t batch.
  - Tráº£ vá» thÃªm `lengths` (Ä‘á»™ dÃ i thá»±c cá»§a cÃ¢u) Ä‘á»ƒ sá»­ dá»¥ng cho cÆ¡ cháº¿ `pack_padded_sequence`.

#### Task 3: XÃ¢y dá»±ng MÃ´ hÃ¬nh RNN
- **Má»¥c Ä‘Ã­ch**: XÃ¢y dá»±ng mÃ´ hÃ¬nh sequence labeling máº¡nh máº½ hÆ¡n POS Tagging.
- **Kiáº¿n trÃºc mÃ´ hÃ¬nh**:
  1. **Embedding Layer**: Chuyá»ƒn Ä‘á»•i index tá»« sang vector (dim=100).
  2. **Bi-LSTM Layer**: Sá»­ dá»¥ng **LSTM hai chiá»u** (Bidirectional LSTM) thay vÃ¬ RNN thÆ°á»ng. LSTM giÃºp giáº£i quyáº¿t váº¥n Ä‘á» vanishing gradient tá»‘t hÆ¡n vÃ  náº¯m báº¯t phá»¥ thuá»™c xa.
     - Hidden dim: 256.
     - Bidirectional: True (Output dim = 256 * 2 = 512).
  3. **Dropout**: Tá»· lá»‡ 0.3 Ä‘á»ƒ giáº£m overfitting.
  4. **Linear Layer**: Ãnh xáº¡ output cá»§a LSTM sang sá»‘ lÆ°á»£ng nhÃ£n NER.
- **Ká»¹ thuáº­t**: Sá»­ dá»¥ng `pack_padded_sequence` Ä‘á»ƒ tá»‘i Æ°u hÃ³a tÃ­nh toÃ¡n, bá» qua cÃ¡c token padding.

#### Task 4: Huáº¥n luyá»‡n MÃ´ hÃ¬nh
- **Cáº¥u hÃ¬nh**:
  - Optimizer: Adam (lr=0.001).
  - Loss function: CrossEntropyLoss (ignore_index=PAD_TAG).
  - Epochs: 10.
- **Quy trÃ¬nh**:
  - TÃ­nh toÃ¡n Loss vÃ  Accuracy trÃªn táº­p train sau má»—i epoch.
  - Accuracy Ä‘Æ°á»£c tÃ­nh báº±ng cÃ¡ch so sÃ¡nh nhÃ£n dá»± Ä‘oÃ¡n vÃ  nhÃ£n tháº­t, **bá» qua cÃ¡c vá»‹ trÃ­ padding**.

#### Task 5: ÄÃ¡nh giÃ¡ MÃ´ hÃ¬nh
- **Má»¥c Ä‘Ã­ch**: Kiá»ƒm tra kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a trÃªn táº­p Validation vÃ  Test.
- **PhÆ°Æ¡ng phÃ¡p**:
  - Sá»­ dá»¥ng hÃ m `evaluate` Ä‘á»ƒ tÃ­nh Loss vÃ  Accuracy trÃªn táº­p Val/Test.
  - Äáº£m báº£o khÃ´ng tÃ­nh toÃ¡n gradient (`torch.no_grad()`) Ä‘á»ƒ tiáº¿t kiá»‡m bá»™ nhá»›.
  - Viáº¿t hÃ m `predict_sentence` Ä‘á»ƒ dá»± Ä‘oÃ¡n thá»±c thá»ƒ cho cÃ¢u nháº­p vÃ o báº¥t ká»³.

### 2. HÆ°á»›ng Dáº«n Thá»±c Thi MÃ£

#### 2.1 YÃªu Cáº§u Há»‡ Thá»‘ng
```bash
pip install torch datasets numpy
```

#### 2.2 Cáº¥u TrÃºc ThÆ° Má»¥c
```
Lab06/
â”œâ”€â”€ lab6_rnn_for_ner.ipynb
â”œâ”€â”€ rnn_for_ner.md
```

#### 2.3 CÃ¡ch Cháº¡y
1. **Má»Ÿ Jupyter Notebook**:
   ```bash
   jupyter notebook lab6_rnn_for_ner.ipynb
   ```
2. **Cháº¡y tuáº§n tá»± cÃ¡c cell**:
   - Cell 1-2: Import thÆ° viá»‡n vÃ  setup seed.
   - Cell 3-6: Táº£i dá»¯ liá»‡u CoNLL-2003 vÃ  xÃ¢y dá»±ng vocab.
   - Cell 7-9: Táº¡o Dataset vÃ  DataLoader.
   - Cell 10-11: Äá»‹nh nghÄ©a mÃ´ hÃ¬nh Bi-LSTM.
   - Cell 12-13: Huáº¥n luyá»‡n mÃ´ hÃ¬nh (10 epochs).
   - Cell 14-15: ÄÃ¡nh giÃ¡ trÃªn táº­p Validation vÃ  Test.
   - Cell 16-17: Dá»± Ä‘oÃ¡n cÃ¢u má»›i.

### 3. PhÃ¢n TÃ­ch Káº¿t Quáº£

#### 3.1 QuÃ¡ TrÃ¬nh Huáº¥n Luyá»‡n Chi Tiáº¿t
DÆ°á»›i Ä‘Ã¢y lÃ  káº¿t quáº£ chi tiáº¿t cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n qua 10 epochs:

| Epoch | Train Loss | Train Accuracy | Nháº­n XÃ©t |
|:-----:|:----------:|:--------------:|:---------|
| 1     | 0.819      | 82.22%         | Khá»Ÿi Ä‘áº§u tá»‘t, mÃ´ hÃ¬nh há»c nhanh cÃ¡c quy luáº­t cÆ¡ báº£n. |
| 2     | 0.521      | 86.07%         | Loss giáº£m máº¡nh, accuracy tÄƒng gáº§n 4%. |
| 3     | 0.399      | 88.46%         | |
| 4     | 0.316      | 90.67%         | VÆ°á»£t má»‘c 90% accuracy. |
| 5     | 0.260      | 92.21%         | |
| 6     | 0.219      | 93.36%         | |
| 7     | 0.186      | 94.39%         | |
| 8     | 0.161      | 95.04%         | Äáº¡t má»‘c 95% accuracy. |
| 9     | 0.140      | 95.68%         | |
| 10    | 0.121      | **96.21%**     | Train loss ráº¥t tháº¥p, mÃ´ hÃ¬nh há»c ráº¥t tá»‘t trÃªn táº­p train. |

#### 3.2 Káº¿t Quáº£ ÄÃ¡nh GiÃ¡
- **Validation Accuracy**: **94.45%** (Loss: 0.242)
- **Test Accuracy**: **92.60%** (Loss: 0.349)

#### 3.3 Nháº­n XÃ©t

##### 1. Hiá»‡u Suáº¥t Tá»•ng Thá»ƒ
- **Äá»™ chÃ­nh xÃ¡c cao**: MÃ´ hÃ¬nh Ä‘áº¡t Ä‘á»™ chÃ­nh xÃ¡c **92.60%** trÃªn táº­p Test. ÄÃ¢y lÃ  káº¿t quáº£ ráº¥t áº¥n tÆ°á»£ng cho bÃ i toÃ¡n NER, Ä‘áº·c biá»‡t khi chá»‰ sá»­ dá»¥ng kiáº¿n trÃºc Bi-LSTM cÆ¡ báº£n mÃ  khÃ´ng cÃ³ CRF hay pre-trained embeddings phá»©c táº¡p (nhÆ° BERT).
- **Kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a**: Sá»± chÃªnh lá»‡ch giá»¯a Train Acc (96.21%) vÃ  Test Acc (92.60%) lÃ  khoáº£ng 3.6%. Äiá»u nÃ y cho tháº¥y mÃ´ hÃ¬nh cÃ³ hiá»‡n tÆ°á»£ng overfitting nháº¹ nhÆ°ng váº«n giá»¯ Ä‘Æ°á»£c kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a tá»‘t trÃªn dá»¯ liá»‡u chÆ°a tá»«ng gáº·p.

##### 2. Vai TrÃ² Cá»§a Bi-LSTM
- Viá»‡c sá»­ dá»¥ng **LSTM hai chiá»u** lÃ  yáº¿u tá»‘ then chá»‘t. Trong NER, viá»‡c xÃ¡c Ä‘á»‹nh má»™t tá»« lÃ  thá»±c thá»ƒ hay khÃ´ng phá»¥ thuá»™c ráº¥t nhiá»u vÃ o ngá»¯ cáº£nh cáº£ hai phÃ­a.
- VÃ­ dá»¥: Trong cÃ¢u "Washington is a beautiful city", "Washington" lÃ  Ä‘á»‹a danh (LOC). NhÆ°ng trong "Washington announced a new policy", "Washington" cÃ³ thá»ƒ lÃ  tá»• chá»©c (ORG) hoáº·c ngÆ°á»i (PER). Bi-LSTM giÃºp mÃ´ hÃ¬nh nhÃ¬n tháº¥y tá»« "city" hoáº·c "announced" Ä‘á»ƒ Ä‘Æ°a ra quyáº¿t Ä‘á»‹nh Ä‘Ãºng.

#### 3.4 VÃ­ Dá»¥ Dá»± ÄoÃ¡n
CÃ¢u: *"VNU University is located in Hanoi"*
- **Dá»± Ä‘oÃ¡n**:
  - VNU: **B-ORG** (Tá»• chá»©c)
  - University: **I-ORG** (Tá»• chá»©c)
  - is: **O**
  - located: **O**
  - in: **O**
  - Hanoi: **O** (Dá»± Ä‘oÃ¡n sai, láº½ ra pháº£i lÃ  B-LOC)
- **PhÃ¢n tÃ­ch**:
  - MÃ´ hÃ¬nh nháº­n diá»‡n Ä‘Ãºng cá»¥m "VNU University" lÃ  tá»• chá»©c (ORG).
  - Tuy nhiÃªn, tá»« "Hanoi" bá»‹ dá»± Ä‘oÃ¡n nháº§m thÃ nh "O" (khÃ´ng pháº£i thá»±c thá»ƒ). Äiá»u nÃ y cÃ³ thá»ƒ do tá»« "Hanoi" Ã­t xuáº¥t hiá»‡n trong táº­p train (CoNLL-2003 chá»§ yáº¿u lÃ  dá»¯ liá»‡u tin tá»©c phÆ°Æ¡ng TÃ¢y) hoáº·c do mÃ´ hÃ¬nh chÆ°a Ä‘á»§ máº¡nh Ä‘á»ƒ báº¯t Ä‘Æ°á»£c ngá»¯ cáº£nh nÃ y. ÄÃ¢y lÃ  minh chá»©ng cho thÃ¡ch thá»©c vá» **OOV (Out-of-Vocabulary)** vÃ  **Domain Adaptation**.

### 4. ThÃ¡ch Thá»©c vÃ  Giáº£i PhÃ¡p

#### 4.1 ThÃ¡ch Thá»©c
- **Dá»¯ liá»‡u máº¥t cÃ¢n báº±ng**: NhÃ£n `O` chiáº¿m Ä‘a sá»‘ Ã¡p Ä‘áº£o. Náº¿u khÃ´ng xá»­ lÃ½ tá»‘t, mÃ´ hÃ¬nh sáº½ cÃ³ xu hÆ°á»›ng dá»± Ä‘oÃ¡n má»i thá»© lÃ  `O` Ä‘á»ƒ Ä‘áº¡t accuracy cao (nhÆ°ng F1-score cho thá»±c thá»ƒ sáº½ ráº¥t tháº¥p).
- **Tá»« láº¡ (OOV)**: TÃªn riÃªng (nhÆ° "Hanoi", "VNU") thÆ°á»ng khÃ´ng cÃ³ trong tá»« Ä‘iá»ƒn huáº¥n luyá»‡n, dáº«n Ä‘áº¿n viá»‡c mÃ´ hÃ¬nh pháº£i dá»±a hoÃ n toÃ n vÃ o ngá»¯ cáº£nh hoáº·c gÃ¡n token `<UNK>`.
- **Overfitting**: Vá»›i mÃ´ hÃ¬nh máº¡nh nhÆ° LSTM vÃ  táº­p dá»¯ liá»‡u nhá»/trung bÃ¬nh, mÃ´ hÃ¬nh dá»… há»c thuá»™c lÃ²ng.

#### 4.2 Giáº£i PhÃ¡p
- **Masking**: Sá»­ dá»¥ng `ignore_index` trong Loss function vÃ  masking khi tÃ­nh Accuracy Ä‘á»ƒ loáº¡i bá» áº£nh hÆ°á»Ÿng cá»§a padding, giÃºp Ä‘Ã¡nh giÃ¡ chÃ­nh xÃ¡c hÆ¡n.
- **Dropout**: Sá»­ dá»¥ng Dropout vá»›i tá»· lá»‡ 0.3 Ä‘á»ƒ giáº£m thiá»ƒu overfitting.
- **Bi-LSTM**: Táº­n dá»¥ng ngá»¯ cáº£nh toÃ n cá»¥c Ä‘á»ƒ giáº£m bá»›t sá»± phá»¥ thuá»™c vÃ o viá»‡c tá»« Ä‘Ã³ cÃ³ trong tá»« Ä‘iá»ƒn hay khÃ´ng.

### 5. HÆ°á»›ng PhÃ¡t Triá»ƒn

#### 5.1 Cáº£i Tiáº¿n MÃ´ HÃ¬nh
- **Bi-LSTM-CRF**: ThÃªm lá»›p **CRF (Conditional Random Fields)**. CRF cá»±c ká»³ há»¯u Ã­ch trong NER vÃ¬ nÃ³ há»c Ä‘Æ°á»£c cÃ¡c quy luáº­t chuyá»ƒn Ä‘á»•i nhÃ£n (vÃ­ dá»¥: `I-ORG` khÃ´ng bao giá» Ä‘i sau `B-PER`). Äiá»u nÃ y sáº½ giÃºp sá»­a cÃ¡c lá»—i dá»± Ä‘oÃ¡n vÃ´ lÃ½.
- **Pre-trained Embeddings**: Sá»­ dá»¥ng **GloVe** hoáº·c **FastText** Ä‘á»ƒ khá»Ÿi táº¡o embedding. FastText Ä‘áº·c biá»‡t tá»‘t cho NER vÃ¬ nÃ³ sá»­ dá»¥ng subword information, giÃºp xá»­ lÃ½ tá»‘t hÆ¡n cÃ¡c tá»« OOV vÃ  tÃªn riÃªng.
- **Character-level Embedding**: Káº¿t há»£p thÃªm CNN/LSTM á»Ÿ má»©c kÃ½ tá»± Ä‘á»ƒ mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c Ä‘iá»ƒm hÃ¬nh thÃ¡i (nhÆ° viáº¿t hoa, Ä‘uÃ´i "-tion", "-ing").

#### 5.2 Transformer & LLMs
- Chuyá»ƒn sang sá»­ dá»¥ng **BERT** (Bidirectional Encoder Representations from Transformers). BERT Ä‘Ã£ Ä‘Æ°á»£c pre-train trÃªn lÆ°á»£ng dá»¯ liá»‡u khá»•ng lá»“ vÃ  hiá»ƒu ngá»¯ cáº£nh sÃ¢u sáº¯c hÆ¡n nhiá»u so vá»›i LSTM. Fine-tuning BERT trÃªn CoNLL-2003 thÆ°á»ng cho káº¿t quáº£ SOTA (>93-94% F1).

---
**Káº¿t luáº­n**: BÃ i thá»±c hÃ nh Ä‘Ã£ xÃ¢y dá»±ng thÃ nh cÃ´ng há»‡ thá»‘ng NER sá»­ dá»¥ng Bi-LSTM vá»›i Ä‘á»™ chÃ­nh xÃ¡c kháº£ quan (~92.6% trÃªn táº­p Test). Máº·c dÃ¹ cÃ²n má»™t sá»‘ háº¡n cháº¿ vá»›i cÃ¡c tá»« hiáº¿m (nhÆ° vÃ­ dá»¥ "Hanoi"), nhÆ°ng Ä‘Ã¢y lÃ  ná»n táº£ng vá»¯ng cháº¯c Ä‘á»ƒ phÃ¡t triá»ƒn cÃ¡c há»‡ thá»‘ng trÃ­ch xuáº¥t thÃ´ng tin phá»©c táº¡p hÆ¡n.
