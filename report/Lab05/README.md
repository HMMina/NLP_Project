# Lab 05


## B√°o c√°o v√† Ph√¢n t√≠ch 

### üìã M·ª•c l·ª•c
1. [C√°c b∆∞·ªõc tri·ªÉn khai](#1-c√°c-b∆∞·ªõc-tri·ªÉn-khai)
2. [H∆∞·ªõng d·∫´n ch·∫°y code](#2-h∆∞·ªõng-d·∫´n-ch·∫°y-code)
3. [Ph√¢n t√≠ch k·∫øt qu·∫£](#3-ph√¢n-t√≠ch-k·∫øt-qu·∫£)
4. [Th√°ch th·ª©c v√† Gi·∫£i ph√°p](#4-th√°ch-th·ª©c-v√†-gi·∫£i-ph√°p)



## 1. C√°c b∆∞·ªõc tri·ªÉn khai

### 1.1 Task 1: Tri·ªÉn khai TextClassifier

**B∆∞·ªõc 1: Thi·∫øt k·∫ø ki·∫øn tr√∫c**
- T·∫°o class `TextClassifier` k·∫øt h·ª£p:
  - M·ªôt vectorizer (CountVectorizer ho·∫∑c TfidfVectorizer)
  - M√¥ h√¨nh LogisticRegression t·ª´ scikit-learn
- M·ª•c ƒë√≠ch: Cung c·∫•p interface th·ªëng nh·∫•t cho ph√¢n lo·∫°i vƒÉn b·∫£n

**B∆∞·ªõc 2: Tri·ªÉn khai c√°c ph∆∞∆°ng th·ª©c ch√≠nh**
```python
class TextClassifier:
    def __init__(self, vectorizer: Vectorizer):
        """Kh·ªüi t·∫°o v·ªõi vectorizer tu√¢n theo interface Vectorizer"""
        
    def fit(self, texts: List[str], labels: List[int]) -> None:
        """Hu·∫•n luy·ªán m√¥ h√¨nh tr√™n vƒÉn b·∫£n ƒë√£ g√°n nh√£n"""
        # 1. Vector h√≥a c√°c vƒÉn b·∫£n training b·∫±ng fit_transform
        # 2. Hu·∫•n luy·ªán LogisticRegression tr√™n c√°c vector
        
    def predict(self, texts: List[str]) -> List[int]:
        """D·ª± ƒëo√°n nh√£n cho vƒÉn b·∫£n m·ªõi"""
        # 1. Vector h√≥a vƒÉn b·∫£n b·∫±ng transform (kh√¥ng ph·∫£i fit_transform!)
        # 2. Tr·∫£ v·ªÅ d·ª± ƒëo√°n t·ª´ LogisticRegression
        
    def evaluate(self, y_true: List[int], y_pred: List[int]) -> dict:
        """T√≠nh to√°n c√°c ƒë·ªô ƒëo hi·ªáu su·∫•t"""
        # Tr·∫£ v·ªÅ: accuracy, precision, recall, f1-score
```

**B∆∞·ªõc 3: C·∫•u h√¨nh LogisticRegression**
- Max iterations: 100
- Random state: 42 (ƒë·∫£m b·∫£o t√≠nh t√°i t·∫°o)

**C√°c quy·∫øt ƒë·ªãnh thi·∫øt k·∫ø quan tr·ªçng:**
- S·ª≠ d·ª•ng composition thay v√¨ inheritance (has-a vectorizer, kh√¥ng ph·∫£i is-a vectorizer)
- Tu√¢n theo API fit/predict c·ªßa scikit-learn
- T√°ch bi·ªát concerns: vector h√≥a vs ph√¢n lo·∫°i
- Ho·∫°t ƒë·ªông v·ªõi b·∫•t k·ª≥ implementation Vectorizer n√†o (interface Lab01)

### 1.2 Task 2: Ch·∫°y th·ª≠ nghi·ªám v·ªõi t·∫≠p d·ªØ li·ªáu nh·ªè
```
texts = [
        "This movie is fantastic and I love it!",
        "I hate this film, it's terrible.",
        "The acting was superb, a truly great experience.",
        "What a waste of time, absolutely boring.",
        "Highly recommend this, a masterpiece.",
        "Could not finish watching, so bad.",
    ]
    labels = [1, 0, 1, 0, 1, 0]  # 1 = positive, 0 = negative
```

### 1.3 Task 3: Pipeline v·ªõi PySpark

**B∆∞·ªõc 1: Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng Spark**
```python
spark = SparkSession.builder \
    .appName("SentimentAnalysis") \
    .config("spark.driver.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "8") \
    .getOrCreate()
```

**B∆∞·ªõc 2: Chu·∫©n b·ªã d·ªØ li·ªáu**
- Load sentiments.csv (5,792 vƒÉn b·∫£n)
- Chuy·ªÉn ƒë·ªïi nh√£n sentiment: -1 ‚Üí 0, +1 ‚Üí 1 (ph√¢n lo·∫°i nh·ªã ph√¢n)
- C√¥ng th·ª©c: `label = (sentiment + 1) / 2`

**B∆∞·ªõc 3: X√¢y d·ª±ng ML Pipeline**

T·∫°o pipeline 5 giai ƒëo·∫°n:
```python
Pipeline(stages=[
    Tokenizer(inputCol="text", outputCol="words"),
    StopWordsRemover(inputCol="words", outputCol="filtered_words"),
    HashingTF(inputCol="filtered_words", outputCol="raw_features", numFeatures=10000),
    IDF(inputCol="raw_features", outputCol="features"),
    LogisticRegression(maxIter=100, regParam=0.001)
])
```
**Pipeline g·ªìm c√°c th√†nh ph·∫ßn:**
1. **Tokenizer**: T√°ch vƒÉn b·∫£n th√†nh t·ª´ (x·ª≠ l√Ω kho·∫£ng tr·∫Øng, d·∫•u c√¢u)
2. **StopWordsRemover**: Lo·∫°i b·ªè "the", "is", "and", v.v. (gi·∫£m nhi·ªÖu)
3. **HashingTF**: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng nhanh b·∫±ng hashing trick (kh√¥ng c·∫ßn l∆∞u vocabulary)
4. **IDF**: G√°n tr·ªçng s·ªë cho c√°c t·ª´ theo t·∫ßm quan tr·ªçng
5. **LogisticRegression**: M√¥ h√¨nh ph√¢n lo·∫°i nhanh

**B∆∞·ªõc 4: Chia Train-Test**
- Training: 80% (4,633 m·∫´u)
- Testing: 20% (1,159 m·∫´u)
- Split theo t·ª∑ l·ªá (gi·ªØ nguy√™n ph√¢n ph·ªëi class)

**B∆∞·ªõc 5: ƒê√°nh gi√°**
- S·ª≠ d·ª•ng MulticlassClassificationEvaluator
- C√°c ƒë·ªô ƒëo: accuracy, precision, recall, F1

### 1.4 Task 4: C√°c th√≠ nghi·ªám c·∫£i thi·ªán c√°c th√†nh ph·∫ßn trong m√¥ h√¨nh
**Th√≠ nghi·ªám 1: ƒêi·ªÅu ch·ªânh s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng**
**M·ª•c ti√™u**: T√¨m `numFeatures` t·ªëi ∆∞u cho HashingTF

**Ph∆∞∆°ng ph√°p**:
```python
feature_sizes = [1000, 5000, 10000, 20000]
for numFeatures in feature_sizes:
    # X√¢y d·ª±ng l·∫°i pipeline v·ªõi numFeatures m·ªõi
    # Hu·∫•n luy·ªán v√† ƒë√°nh gi√°
    # Ghi nh·∫≠n accuracy v√† th·ªùi gian training
```

**Ki·ªÉm ƒë·ªãnh gi·∫£ thuy·∫øt l·ª±a ch·ªçn size vector**: 
- Qu√° nh·ªè ‚Üí Va ch·∫°m hash, m·∫•t th√¥ng tin
- Qu√° l·ªõn ‚Üí ƒê·∫∑c tr∆∞ng th∆∞a th·ªõt, training ch·∫≠m h∆°n

**Th√≠ nghi·ªám 2: Word2Vec Embeddings**
**M·ª•c ti√™u**: Thay th·∫ø TF-IDF b·∫±ng semantic embeddings ƒë·∫∑c
**Tri·ªÉn khai**:
```python
word2vec = Word2Vec(
    vectorSize=10000,        # Embeddings 10000 chi·ªÅu
    minCount=5,              # B·ªè qua c√°c t·ª´ hi·∫øm
    inputCol="filtered_words",
    outputCol="features"
)
```

**Th√≠ nghi·ªám 3: So s√°nh c√°c m√¥ h√¨nh ph√¢n lo·∫°i**
**C√°c m√¥ h√¨nh ƒë∆∞·ª£c ki·ªÉm tra**:
1. **Logistic Regression**
   - Bi√™n quy·∫øt ƒë·ªãnh tuy·∫øn t√≠nh
   - Training v√† inference nhanh
   - D·ªÖ gi·∫£i th√≠ch
   - C·∫•u h√¨nh: `maxIter=100, regParam=0.001`

2. **Naive Bayes**
   - M√¥ h√¨nh x√°c su·∫•t
   - Gi·∫£ ƒë·ªãnh c√°c ƒë·∫∑c tr∆∞ng ƒë·ªôc l·∫≠p
   - Training r·∫•t nhanh
   - C·∫•u h√¨nh: `smoothing=1.0`

3. **Gradient-Boosted Trees (GBT)**
   - T·∫≠p h·ª£p c√°c c√¢y quy·∫øt ƒë·ªãnh
   - Bi√™n quy·∫øt ƒë·ªãnh phi tuy·∫øn
   - X·ª≠ l√Ω t∆∞∆°ng t√°c gi·ªØa c√°c ƒë·∫∑c tr∆∞ng
   - C·∫•u h√¨nh: `maxIter=20, maxDepth=7`

**Ti√™u ch√≠ ƒë√°nh gi√°**:
- Accuracy v√† F1-score (hi·ªáu su·∫•t)
- Th·ªùi gian training (hi·ªáu qu·∫£)
- S·ª≠ d·ª•ng b·ªô nh·ªõ (kh·∫£ nƒÉng m·ªü r·ªông)

---

## 2. H∆∞·ªõng d·∫´n ch·∫°y code
### 2.1 Y√™u c·∫ßu ti√™n quy·∫øt
**C√†i ƒë·∫∑t c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt:**
```bash
pip install scikit-learn numpy pandas
pip install pyspark
```

**Thi·∫øt l·∫≠p Dataset:**
ƒê·∫£m b·∫£o `sentiments.csv` t·ªìn t·∫°i t·∫°i `C:\Users\ADMIN\.vscode\NLP_APP\sentiments.csv` v·ªõi ƒë·ªãnh d·∫°ng:
```
text,sentiment
"This is great!",1
"This is bad.",-1
```

### 2.2 Ch·∫°y c√°c th√≠ nghi·ªám
**Di chuy·ªÉn ƒë·∫øn th∆∞ m·ª•c Test:**
```bash
cd C:\Users\ADMIN\.vscode\NLP_APP\Lab05\test
```

#### Test 1: Ph√¢n lo·∫°i vƒÉn b·∫£n c∆° b·∫£n (Task 1 & 2)
**File:** `lab5_test.py`
**M·ª•c ƒë√≠ch**: So s√°nh CountVectorizer vs TfidfVectorizer tr√™n t·∫≠p d·ªØ li·ªáu nh·ªè

**L·ªánh:**
```bash
python lab5_test.py
```

**K·∫øt qu·∫£ th·ª±c thi:**
```
Text Classification: CountVectorizer vs TfidfVectorizer
============================================================

Dataset: 6 samples (3 positive, 3 negative)
Train samples: 4 | Test samples: 2

============================================================
Test 1: Using CountVectorizer
============================================================

============================================================
Results using CountVectorizer
============================================================

Predictions:
  [‚úó] Expected: positive | Predicted: negative
      Text: This movie is fantastic and I love it!
  [‚úó] Expected: negative | Predicted: positive
      Text: Could not finish watching, so bad.

Metrics:
  Accuracy  : 0.0000
  Precision : 0.0000
  Recall    : 0.0000
  F1        : 0.0000

============================================================
Test 2: Using TfidfVectorizer
============================================================

============================================================
Results using TfidfVectorizer
============================================================

Predictions:
  [‚úó] Expected: positive | Predicted: negative
      Text: This movie is fantastic and I love it!
  [‚úì] Expected: negative | Predicted: negative
      Text: Could not finish watching, so bad.

============================================================
Comparison Summary
============================================================
Metric       CountVectorizer    TfidfVectorizer    Winner
------------------------------------------------------------
Accuracy     0.0000             0.5000             TF-IDF
Precision    0.0000             0.0000             Tie
Recall       0.0000             0.0000             Tie
F1           0.0000             0.0000             Tie
```

**K·∫øt lu·∫≠n:**
- Accuracy c·ªßa c·∫£ hai m√¥ h√¨nh th·∫•p do t·∫≠p d·ªØ li·ªáu r·∫•t nh·ªè
- TfidfVectorizer n√™n v∆∞·ª£t tr·ªôi h∆°n CountVectorizer
- C·∫ßn th·ª≠ nghi·ªám tr√™n t·∫≠p d·ªØ li·ªáu l·ªõn h∆°n ƒë·ªÉ c√≥ k·∫øt qu·∫£ √Ω nghƒ©a

#### Test 2: Ch·∫°y th√≠ nghi·ªám Pipeline v·ªõi PySpark (Task 3)
**File:** `lab5_spark_sentiment_analysis.py`
**M·ª•c ƒë√≠ch**: Kh·ªüi t·∫°o Pipeline PySpark v√† ƒë√°nh gi√° m√¥ h√¨nh ph√¢n t√≠ch c·∫£m x√∫c

**L·ªánh:**
```bash
python lab5_spark_sentiment_analysis.py
```

**K·∫øt qu·∫£ th·ª±c thi:**
```
============================================================
PySpark Sentiment Analysis Pipeline
============================================================
WARNING: Using incubator modules: jdk.incubator.vector
25/10/29 15:43:53 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/10/29 15:43:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

============================================================
Step 1: Loading Data
============================================================
Initial rows: 5792

Sample data:
+--------------------------------------------------+---------+
|                                              text|sentiment|
+--------------------------------------------------+---------+
|Kickers on my watchlist XIDE TIT SOQ PNK CPW BP...|        1|
|user: AAP MOVIE. 55% return for the FEA/GEED in...|        1|
|user I'd be afraid to short AMZN - they are loo...|        1|
|                                 MNTA Over 12.00  |        1|
|                                  OI  Over 21.37  |        1|
+--------------------------------------------------+---------+
only showing top 5 rows
Rows after cleaning: 5791
Dropped 1 rows with null values

Label distribution:
+-----+-----+
|label|count|
+-----+-----+
|  0.0| 2106|
|  1.0| 3685|
+-----+-----+


============================================================
Step 2: Building Pipeline
============================================================
Stage 1: Tokenizer (text ‚Üí words)
Stage 2: StopWordsRemover (words ‚Üí filtered_words)
Stage 3: HashingTF (filtered_words ‚Üí raw_features, 10000 features)
Stage 4: IDF (raw_features ‚Üí features)
Stage 5: LogisticRegression (maxIter=100, regParam=0.001)

 Pipeline created with 5 stages

============================================================
Step 3: Training Model
============================================================
Training samples: 4682
Training in progress...
25/10/29 15:44:02 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
Model training completed!

============================================================
Step 4: Evaluating Model
============================================================
Test samples: 1109

 Sample predictions:
+--------------------------------------------------+-----+----------+-----------------------------------------+
|                                              text|label|prediction|                              probability|
+--------------------------------------------------+-----+----------+-----------------------------------------+
|  ISG An update to our Feb 20th video review..i...|  0.0|       1.0| [0.25869560846247547,0.7413043915375246]|
|  The rodeo clown sent BK screaming into the SI...|  0.0|       0.0| [0.999998410875766,1.589124233980499E-6]|
| , ES,SPY, Ground Hog Week, distribution at hig...|  0.0|       1.0| [0.035278463202358945,0.964721536797641]|
|                          ES, S  PAT TWO, update  |  0.0|       0.0|  [0.9971335766063395,0.0028664233936605]|
| PCN doulble top at key fib retracement weekly....|  0.0|       1.0|  [0.3992288932697887,0.6007711067302113]|
| also not very healthy, fell back below DT line...|  0.0|       1.0| [0.11947408164202711,0.8805259183579729]|
| thinking out loud. 50 mva sub 200 mva- done. B...|  1.0|       0.0| [0.99872844966236,0.0012715503376400372]|
|"RT @WSJheard: Can√¢‚Ç¨‚Ñ¢t get your hands on a Nint...|  1.0|       1.0| [0.00861611933751715,0.9913838806624828]|
|#ContrAlert Don't Panic: Wall Street Is Going C...|  0.0|       1.0|[0.006779450855187919,0.9932205491448121]|
|#CoronavirusPandemic As bad as #China's economi...|  0.0|       0.0| [0.9059362679382089,0.09406373206179108]|
+--------------------------------------------------+-----+----------+-----------------------------------------+
only showing top 10 rows

============================================================
Evaluation Metrics
============================================================
Accuracy : 0.7295
Precision: 0.7243
Recall   : 0.7295
F1 Score : 0.7248

============================================================
Prediction Analysis
============================================================
Total predictions: 1109
Correct: 809 (72.95%)
Incorrect: 300 (27.05%)

 Misclassified examples:
+--------------------------------------------------------------------------------+-----+----------+
|                                                                            text|label|prediction|
+--------------------------------------------------------------------------------+-----+----------+
|  ISG An update to our Feb 20th video review..if it closes below 495 much low...|  0.0|       1.0|
|                            , ES,SPY, Ground Hog Week, distribution at highs..  |  0.0|       1.0|
|               PCN doulble top at key fib retracement weekly....time to exit ...|  0.0|       1.0|
| also not very healthy, fell back below DT line after breaking it, SI weak, M...|  0.0|       1.0|
| thinking out loud. 50 mva sub 200 mva- done. Bottoming tails at 61.60 provid...|  1.0|       0.0|
| thinking out loud. 50 mva sub 200 mva- done. Bottoming tails at 61.60 provid...|  1.0|       0.0|
+--------------------------------------------------------------------------------+-----+----------+
+--------------------------------------------------------------------------------+-----+----------+
only showing top 5 rows
only showing top 5 rows

============================================================
Pipeline execution completed successfully!
============================================================

 Spark session stopped.
SUCCESS: The process with PID 23516 (child process of PID 3068) has been terminated.
SUCCESS: The process with PID 3068 (child process of PID 10716) has been terminated.
SUCCESS: The process with PID 10716 (child process of PID 20392) has been terminated.
```
**K·∫øt lu·∫≠n:**
- M√¥ h√¨nh ƒë·∫°t ~73% accuracy tr√™n t·∫≠p test 1,109 m·∫´u
- Precision v√† Recall c√¢n b·∫±ng t·ªët (~72-73%)
- M√¥ h√¨nh c·∫£i thi·ªán h∆°n r·∫•t nhi·ªÅu so v·ªõi t·∫≠p d·ªØ li·ªáu nh·ªè ban ƒë·∫ßu

#### Test 3: C·∫£i thi·ªán c√°c th√†nh ph·∫ßn m√¥ h√¨nh (Task 4)
**File:** `lab5_improvement_test.py`
**M·ª•c ƒë√≠ch**: Th·ª±c hi·ªán c√°c th√≠ nghi·ªám nh·∫±m c·∫£i thi·ªán hi·ªáu su·∫•t m√¥ h√¨nh

**L·ªánh:**
```bash
python lab5_improvement_test.py
```


**K·∫øt qu·∫£ th·ª±c thi:**
```
======================================================================
TASK 4: Model Improvement Experiments
======================================================================
WARNING: Using incubator modules: jdk.incubator.vector
25/10/29 16:39:48 WARN Shell: Did not find winutils.exe: java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://cwiki.apache.org/confluence/display/HADOOP2/WindowsProblems
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
25/10/29 16:39:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable

Loading data from: c:\Users\ADMIN\.vscode\NLP_APP\sentiments.csv
Total samples: 5791
Train: 4682, Test: 1109

======================================================================
EXPERIMENT 1: Feature Dimensionality Reduction
======================================================================
Testing different numFeatures values: 1000, 5000, 10000, 20000

--- Testing with numFeatures = 1000 ---
25/10/29 16:40:05 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS
  Accuracy: 0.7196
  F1 Score: 0.7156
  Training Time: 8.59s

--- Testing with numFeatures = 5000 ---
  Accuracy: 0.7277
  F1 Score: 0.7235
  Training Time: 4.33s

--- Testing with numFeatures = 10000 ---
  Accuracy: 0.7295
  F1 Score: 0.7248
  Training Time: 2.46s

--- Testing with numFeatures = 20000 ---
  Accuracy: 0.7358
  F1 Score: 0.7286
  Training Time: 2.53s

======================================================================
Experiment 1 Summary
======================================================================
NumFeatures     Accuracy     F1 Score     Train Time
----------------------------------------------------------------------
1000            0.7196       0.7156       8.59        s
5000            0.7277       0.7235       4.33        s
10000           0.7295       0.7248       2.46        s
20000           0.7358       0.7286       2.53        s

 Best: numFeatures=20000 with Accuracy=0.7358

======================================================================
EXPERIMENT 2: Word2Vec Embeddings
======================================================================
Using Word2Vec to generate dense word embeddings

Training Word2Vec model...

======================================================================
Experiment 2 Results
======================================================================
Accuracy : 0.7529
Precision: 0.7519
Recall   : 0.7529
F1 Score : 0.7409
Training Time: 39.12s

======================================================================
EXPERIMENT 3: Model Architecture Comparison
======================================================================
Comparing: Logistic Regression, Naive Bayes, GBT Classifier

--- Model 1: Logistic Regression ---
  Accuracy: 0.7295, F1: 0.7248, Time: 1.37s

--- Model 2: Naive Bayes ---
  Accuracy: 0.6844, F1: 0.6842, Time: 0.61s

--- Model 3: Gradient-Boosted Trees ---
  Accuracy: 0.7340, F1: 0.7042, Time: 19.80s

======================================================================
Experiment 3 Summary
======================================================================
Model                     Accuracy     F1 Score     Train Time
----------------------------------------------------------------------
Logistic Regression       0.7295       0.7248       1.37        s
Naive Bayes               0.6844       0.6842       0.61        s
Gradient-Boosted Trees    0.7340       0.7042       19.80       s

 Best Model: Gradient-Boosted Trees with Accuracy=0.7340

======================================================================
FINAL SUMMARY - All Experiments
======================================================================

 Key Findings:
1. Best numFeatures: 20000
2. Word2Vec Accuracy: 0.7529
3. Best Model: Gradient-Boosted Trees
SUCCESS: The process with PID 17620 (child process of PID 14724) has been terminated.
SUCCESS: The process with PID 14724 (child process of PID 24900) has been terminated.
SUCCESS: The process with PID 24900 (child process of PID 7936) has been terminated.
```

**K·∫øt lu·∫≠n:**
- Th√≠ nghi·ªám 1: TƒÉng numFeatures l√™n 20,000 ƒë·∫°t ~73.58% accuracy
- Th√≠ nghi·ªám 2: S·ª≠ d·ª•ng Word2Vec embeddings ƒë·∫°t ~75.29% accuracy, t·ªët h∆°n TF-IDF
- Th√≠ nghi·ªám 3: Gradient-Boosted Trees ƒë·∫°t ~73.40% accuracy, nh∆∞ng th·ªùi gian training l√¢u h∆°n nhi·ªÅu so v·ªõi Logistic Regression

---

## 3. Ph√¢n t√≠ch k·∫øt qu·∫£
### 3.1 Test 1
**C·∫•u h√¨nh m√¥ h√¨nh:**
- **Vectorizer**: TfidfVectorizer v√† CountVectorizer
- **Classifier**: Logistic Regression (maxIter=100, regParam=0.001)
- **Dataset**: 6 m·∫´u (3 positive, 3 negative)

**Ph√¢n t√≠ch:**
- TF-IDF t·ªët h∆°n CountVectorizer: ƒê·∫°t 50% accuracy so v·ªõi 0% c·ªßa CountVectorizer
- Tuy nhi√™n, c·∫£ hai ƒë·ªÅu c√≥ hi·ªáu su·∫•t th·∫•p do t·∫≠p d·ªØ li·ªáu qu√° nh·ªè (6 m·∫´u)
- C·∫ßn nhi·ªÅu d·ªØ li·ªáu h∆°n ƒë·ªÉ m√¥ h√¨nh h·ªçc t·ªët h∆°n

### 3.2 Test 2
**C·∫•u h√¨nh Pipeline:**
- Tokenizer: T√°ch vƒÉn b·∫£n th√†nh t·ª´ (x·ª≠ l√Ω kho·∫£ng tr·∫Øng, d·∫•u c√¢u)
- StopWordsRemover: Lo·∫°i b·ªè "the", "is", "and", v.v. (gi·∫£m nhi·ªÖu)
- HashingTF: Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng nhanh b·∫±ng hashing trick (numFeatures=10,000)
- IDF: G√°n tr·ªçng s·ªë cho c√°c t·ª´ theo t·∫ßm quan tr·ªçng
- LogisticRegression : M√¥ h√¨nh ph√¢n lo·∫°i (maxIter=100, regParam=0.001)
**Ph√¢n t√≠ch:**
- M√¥ h√¨nh ƒë·∫°t ~73% accuracy tr√™n t·∫≠p test 1,109 m·∫´u 
- Precision v√† Recall c√¢n b·∫±ng t·ªët (~72-73%)
- M√¥ h√¨nh c·∫£i thi·ªán h∆°n r·∫•t nhi·ªÅu so v·ªõi t·∫≠p d·ªØ li·ªáu nh·ªè ban ƒë·∫ßu
- Pipeline kh√° hi·ªáu qu·∫£ trong vi·ªác x·ª≠ l√Ω vƒÉn b·∫£n v√† ph√¢n lo·∫°i
- C√°c th√†nh ph·∫ßn nh∆∞ StopWordsRemover v√† IDF ƒë√≥ng vai tr√≤ quan tr·ªçng trong vi·ªác n√¢ng cao hi·ªáu su·∫•t
- C√≥ th·ªÉ c·∫£i thi·ªán th√™m b·∫±ng c√°ch ƒëi·ªÅu ch·ªânh c√°c th√†nh ph·∫ßn trong pipeline (v√≠ d·ª•: th·ª≠ nghi·ªám v·ªõi c√°c tham s·ªë kh√°c nhau cho HashingTF ho·∫∑c LogisticRegression, hay th·ª≠ nghi·ªám thay th·∫ø c√°c th√†nh ph·∫ßn trong pipeline b·∫±ng c√°c m√¥ h√¨nh kh√°c)

### 3.3 Test 3
**Th√≠ nghi·ªám 1: ƒêi·ªÅu ch·ªânh s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng**
C·∫•u h√¨nh Pipeline gi·ªëng Test 2, thay ƒë·ªïi numFeatures c·ªßa HashingTF

**Ph√¢n t√≠ch:**
- TƒÉng numFeatures t·ª´ 1,000 l√™n 20,000 c·∫£i thi·ªán accuracy t·ª´ ~71.96% l√™n ~73.58%
- Do t·∫≠p d·ªØ li·ªáu l·ªõn c√≥ nhi·ªÅu t·ª´ kh√°c nhau, s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng cao gi√∫p gi·∫£m va ch·∫°m hash v√† gi·ªØ l·∫°i nhi·ªÅu th√¥ng tin h∆°n
- Tuy nhi√™n, l·ª£i √≠ch gi·∫£m d·∫ßn: TƒÉng t·ª´ 10,000 l√™n 20,000 ch·ªâ c·∫£i thi·ªán accuracy nh·∫π (~0.63%)
- C·∫ßn c√¢n nh·∫Øc gi·ªØa hi·ªáu su·∫•t v√† chi ph√≠ t√≠nh to√°n ƒë·ªÉ l·ª±a ch·ªçn numFeatures ph√π h·ª£p v·ªõi d·ªØ li·ªáu

**Th√≠ nghi·ªám 2: Word2Vec Embeddings**

Thay th·∫ø HashingTF + IDF b·∫±ng Word2Vec ƒë·ªÉ t·∫°o embeddings ƒë·∫∑c

**Ph√¢n t√≠ch:**
- M√¥ h√¨nh ƒë·∫°t ~75.29% accuracy, v∆∞·ª£t tr·ªôi so v·ªõi TF-IDF (~73.58%)
- Word2Vec m·∫∑c d√π ch·ªâ s·ª≠ d·ª•ng vectorSize=10,000 nh∆∞ng ƒë·∫°t hi·ªáu su·∫•t t·ªët t∆∞∆°ng ƒë∆∞∆°ng v·ªõi TF-IDF v·ªõi numFeatures=20,000
- ƒêi·ªÅu n√†y cho th·∫•y embeddings ƒë·∫∑c c√≥ th·ªÉ n·∫Øm b·∫Øt ng·ªØ nghƒ©a t·ªët h∆°n
- Tuy nhi√™n, th·ªùi gian training l√¢u h∆°n ƒë√°ng k·ªÉ (~39.12s so v·ªõi ~2.53s c·ªßa TF-IDF v·ªõi numFeatures=20,000)
- C·∫ßn c√¢n nh·∫Øc gi·ªØa hi·ªáu su·∫•t v√† th·ªùi gian hu·∫•n luy·ªán khi l·ª±a ch·ªçn ph∆∞∆°ng ph√°p tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng

**Th√≠ nghi·ªám 3: So s√°nh c√°c m√¥ h√¨nh ph√¢n lo·∫°i**

So s√°nh Logistic Regression, Naive Bayes v√† Gradient-Boosted Trees (GBT)

**Ph√¢n t√≠ch:**
- Logistic Regression ƒë·∫°t ~72.95% accuracy v·ªõi th·ªùi gian training nhanh (~1.37s)
- Naive Bayes ƒë·∫°t ~68.44% accuracy, th·∫•p h∆°n ƒë√°ng k·ªÉ so v·ªõi hai m√¥ h√¨nh c√≤n l·∫°i, nh∆∞ng th·ªùi gian training r·∫•t nhanh (~0.61s)
- GBT ƒë·∫°t ~73.40% accuracy, t∆∞∆°ng ƒë∆∞∆°ng v·ªõi Logistic Regression, nh∆∞ng th·ªùi gian training l√¢u h∆°n nhi·ªÅu (~19.80s)
-  Logistic Regression c√¢n b·∫±ng t·ªët gi·ªØa hi·ªáu su·∫•t v√† th·ªùi gian hu·∫•n luy·ªán, l√† l·ª±a ch·ªçn ph√π h·ª£p cho b·ªô d·ªØ li·ªáu n√†y, trong khi GBT c√≥ th·ªÉ ph√π h·ª£p h∆°n n·∫øu ∆∞u ti√™n hi·ªáu su·∫•t h∆°n th·ªùi gian
- Naive Bayes c√≥ th·ªÉ ph√π h·ª£p h∆°n v·ªõi c√°c lo·∫°i d·ªØ li·ªáu ho·∫∑c b√†i to√°n kh√°c

### 3.4 K·∫øt lu·∫≠n t·ªïng th·ªÉ
- TF-IDF v·ªõi numFeatures cao (20,000) v√† Word2Vec embeddings ƒë·ªÅu mang l·∫°i hi·ªáu su·∫•t t·ªët ƒë·ªëi v·ªõi b·ªô d·ªØ li·ªáu c√≥ k√≠ch th∆∞·ªõc l·ªõn
- Logistic Regression l√† m√¥ h√¨nh ph√¢n lo·∫°i ph√π h·ª£p cho b·ªô d·ªØ li·ªáu n√†y, c√¢n b·∫±ng t·ªët gi·ªØa hi·ªáu su·∫•t v√† th·ªùi gian hu·∫•n luy·ªán, trong khi GBT c√≥ th·ªÉ ƒë∆∞·ª£c xem x√©t n·∫øu ∆∞u ti√™n hi·ªáu su·∫•t h∆°n th·ªùi gian
- Vi·ªác ƒëi·ªÅu ch·ªânh c√°c th√†nh ph·∫ßn trong pipeline (s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng, ph∆∞∆°ng ph√°p tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng, m√¥ h√¨nh ph√¢n lo·∫°i) ƒë·ªÅu ·∫£nh h∆∞·ªüng ƒë√°ng k·ªÉ ƒë·∫øn hi·ªáu su·∫•t cu·ªëi c√πng
- C·∫ßn c√¢n nh·∫Øc k·ªπ l∆∞·ª°ng gi·ªØa hi·ªáu su·∫•t, th·ªùi gian hu·∫•n luy·ªán v√† chi ph√≠ t√≠nh to√°n khi thi·∫øt k·∫ø h·ªá th·ªëng

---

## 4. Th√°ch th·ª©c v√† Gi·∫£i ph√°p
### Th√°ch th·ª©c 1: L·ªói import module
**V·∫•n ƒë·ªÅ:**
```python
ModuleNotFoundError: No module named 'Lab01'
```
**Nguy√™n nh√¢n g·ªëc:**
- C√°c script test Lab05 kh√¥ng t√¨m th·∫•y modules Lab01
- Python path kh√¥ng bao g·ªìm workspace root
**ƒêi·ªÅu tra:**
- Test scripts trong `Lab05/test/` c·∫ßn ƒëi l√™n 2 c·∫•p, kh√¥ng ph·∫£i 3
**Gi·∫£i ph√°p:**
```python
# Tr∆∞·ªõc (sai):
workspace_root = os.path.join(__file__, '..', '..', '..')

# Sau (ƒë√∫ng):
workspace_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
sys.path.insert(0, workspace_root)
```
**B√†i h·ªçc:**
- Lu√¥n s·ª≠ d·ª•ng `os.path.abspath()` cho ƒë∆∞·ªùng d·∫´n tin c·∫≠y
- Test thi·∫øt l·∫≠p `sys.path` b·∫±ng c√°ch print `workspace_root`
- C√¢n nh·∫Øc s·ª≠ d·ª•ng bi·∫øn m√¥i tr∆∞·ªùng `PYTHONPATH` cho production

### Th√°ch th·ª©c 2: V·∫•n ƒë·ªÅ import Lab01
**V·∫•n ƒë·ªÅ:**
```python
from src.core.interfaces import Vectorizer  # Th·∫•t b·∫°i
```
**Nguy√™n nh√¢n g·ªëc:**
- `count_vectorizer.py` c·ªßa Lab01 s·ª≠ d·ª•ng import tuy·ªát ƒë·ªëi `from src.core.interfaces`
- Ch·ªâ ho·∫°t ƒë·ªông n·∫øu Lab01 trong `sys.path` nh∆∞ m·ªôt package
- G√¢y ra v·∫•n ƒë·ªÅ ph·ª• thu·ªôc v√≤ng
**Gi·∫£i ph√°p:**
```python
# ƒê√£ thay ƒë·ªïi trong Lab01/src/representations/count_vectorizer.py:
from core.interfaces import Vectorizer  # Import t∆∞∆°ng ƒë·ªëi
```
**B√†i h·ªçc:**
- ∆Øu ti√™n relative imports trong m·ªôt package
- Absolute imports ch·ªâ d√†nh cho dependencies b√™n ngo√†i

### Th√°ch th·ª©c 3: V·∫•n ƒë·ªÅ b·ªô nh·ªõ PySpark
**V·∫•n ƒë·ªÅ (Ti·ªÅm nƒÉng):**
```
Java.lang.OutOfMemoryError: Java heap space
```
**Nguy√™n nh√¢n g·ªëc:**
- B·ªô nh·ªõ driver m·∫∑c ƒë·ªãnh c·ªßa PySpark: 1GB
- Sentiment dataset + ML pipeline v∆∞·ª£t m·∫∑c ƒë·ªãnh
- ƒê·∫∑c bi·ªát v·ªõi numFeatures l·ªõn (20K+)
**Gi·∫£i ph√°p ph√≤ng ng·ª´a:**
```python
spark = SparkSession.builder \
    .config("spark.driver.memory", "4g") \
    .config("spark.executor.memory", "4g") \
    .config("spark.sql.shuffle.partitions", "8") \
    .getOrCreate()
```
**B√†i h·ªçc:**
- Lu√¥n c·∫•u h√¨nh b·ªô nh·ªõ Spark cho ML workloads
- B·∫Øt ƒë·∫ßu v·ªõi 4GB driver, tƒÉng n·∫øu c·∫ßn
- S·ª≠ d·ª•ng Spark UI ƒë·ªÉ debug v·∫•n ƒë·ªÅ b·ªô nh·ªõ

### C·∫£i ti·∫øn t∆∞∆°ng lai

**C√°c n√¢ng c·∫•p ti·ªÅm nƒÉng:**
1. **Deep Learning**: S·ª≠ d·ª•ng LSTM ho·∫∑c BERT ƒë·ªÉ hi·ªÉu ng·ªØ c·∫£nh t·ªët h∆°n
2. **Ph∆∞∆°ng ph√°p Ensemble**: K·∫øt h·ª£p nhi·ªÅu m√¥ h√¨nh v·ªõi voting
3. **Active Learning**: G√°n nh√£n l·∫∑p ƒëi l·∫∑p l·∫°i c√°c v√≠ d·ª• kh√¥ng ch·∫Øc ch·∫Øn


---
## T√≥m t·∫Øt
**Implementation ho√†n th√†nh:** 100%
- Task 1: TextClassifier v·ªõi fit/predict/evaluate
- Task 2: TfidfVectorizer v·ªõi c√¥ng th·ª©c ƒë√∫ng
- Task 3: Pipeline ph√¢n t√≠ch c·∫£m x√∫c PySpark
- Task 4: Ba th√≠ nghi·ªám c·∫£i thi·ªán m√¥ h√¨nh

**B√†i h·ªçc ch√≠nh:**
1. TF-IDF > CountVectorizer cho ph√¢n lo·∫°i vƒÉn b·∫£n
2. Word2Vec n·∫Øm b·∫Øt ng·ªØ nghƒ©a t·ªët h∆°n TF-IDF
3. S·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng quan tr·ªçng nh∆∞ng c√≥ l·ª£i √≠ch gi·∫£m d·∫ßn
4. C√≥ th·ªÉ l·ª±a ch·ªçn nhi·ªÅu lo·∫°i m√¥ h√¨nh ph√¢n lo·∫°i t√πy theo ƒë·ªô ph√π h·ª£p v·ªõi d·ªØ li·ªáu

